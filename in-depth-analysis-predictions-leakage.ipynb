{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id='toc'></a>","metadata":{}},{"cell_type":"markdown","source":"# Medical Appointment No Shows - What is going on?\n\nOne of the limitations I've faced breaking into machine learning domain, which is actually not a limitation rather a blessing, is the overwhelming vast amount of knowledge and resources available leaving me puzzled where to start and what topics to cover.\n\nSo I've decided to create this notebook with that struggle in mind; displaying a structured work flow of an imbalanced binary classification problem but with a twist! There are a lot of notebooks that shows how to do things right, but few that discusses how things can actually go wrong, spotting errors and their impact on final results with examples.\n\nWe'll cover many topics that are important to be aware of going through a classification problem, I came across these topics during my efforts to improve on my first classification attempt going through different research papers, books, blogs and other notebooks; I wanted to make this notebook as a collective reference for such topics and at the same time provide links to external sources that I've found to be very informative and well presented.\n\nI have a particular interest in this dataset, I've chosen it to be my first EDA project in python language then to be my first hands on applied machine learning project and now I'm providing updates to my first classification attempt using new knowledge acquired. Each section in this notebook was created at different time interval; I'm keeping them intact for the joy of reflecting back on my previous work and measuring progress.\n\nThe three main sections below are sorted chronologically:\n- Exploratory Data Analysis (EDA)\n- Predictions (1st attempt)\n- Updates\n\n## Table of Contents\n\n- About the dataset and basic preprocessing\n<ul>\n<li><a href=\"#intro\">Introduction</a></li>\n<li><a href=\"#wrangling\">Data Wrangling</a></li>\n</ul>  \n\n- Exploratory Data Analysis (EDA)\n<ul>\n<li><a href=\"#eda\">Exploratory Data Analysis (EDA)</a></li>\n<li><a href=\"#conclusions\">EDA - Conclusion and limitations</a></li>\n</ul>\n\n- Predictions (1st attempt)\n<ul>\n<li><a href=\"#predictions\">Appointment predictions</a></li>\n<li><a href=\"#predconc\">Predictions conclusion - Who's To Blame?</a></li>\n</ul>\n\n- Updates\n<ul>\n<li><a href=\"#analysis\">In-depth analysis and feature engineering</a></li>\n    <ul>\n    <li><a href=\"#panalysis\">Analysing patient behavior</a></li>\n    <li><a href=\"#fengn\">Numeric feature engineering</a></li>\n    <li><a href=\"#bline\">Establishing baseline and analysing results</a></li>\n    <li><a href=\"#fengc\">Categorical feature engineering</a></li>\n    <li><a href=\"#pm\">Panic Attack!</a></li>\n    </ul>\n<li><a href=\"#modeling\">Modeling</a></li>\n    <ul>\n    <li><a href=\"#fs\">Feature selection</a></li>\n    <li><a href=\"#rts\">Resampling training set</a></li>\n    <li><a href=\"#ovrft\">Overfitting</a></li>\n    <li><a href=\"#thrm\">Threshold moving</a></li>\n    <li><a href=\"#kfcv\">K-fold cross validation</a></li>\n    <li><a href=\"#vdb\">Visualizing decision boundary</a></li>\n    <li><a href=\"#mfnc\">More features and a new classifier</a></li>\n    <li><a href=\"#upr\">Updated prediction results: RandomForest and CatBoost</a></li>\n    <li><a href=\"#pm2\">Heart skips a beat!</a></li>\n    <li><a href=\"#rvapp\">Revised Approach</a></li>\n    </ul>\n</ul>\n\n- Wrap-up\n<ul>\n<li><a href=\"#references\">Summary of updates and additional references</a></li>\n</ul>","metadata":{}},{"cell_type":"code","source":"# pip install pyod","metadata":{"execution":{"iopub.status.busy":"2022-05-19T15:59:37.099317Z","iopub.execute_input":"2022-05-19T15:59:37.099690Z","iopub.status.idle":"2022-05-19T15:59:37.103409Z","shell.execute_reply.started":"2022-05-19T15:59:37.099662Z","shell.execute_reply":"2022-05-19T15:59:37.102363Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Load libraries\n\nimport pandas as pd\nimport numpy as np\nimport datetime as dt\nfrom time import time\nfrom collections import Counter\nfrom functools import reduce\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nfrom plotly import subplots\nimport squarify\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression, LassoCV\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.feature_selection import SelectKBest, SelectPercentile, chi2, f_classif, mutual_info_classif, SelectFromModel\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, RepeatedStratifiedKFold, cross_val_score, cross_val_predict\nfrom sklearn.metrics import precision_recall_curve, roc_curve, auc, classification_report, confusion_matrix, f1_score\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import MDS,TSNE\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer, RobustScaler, MaxAbsScaler, LabelEncoder\nimport category_encoders as ce\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom pyod.models.pca import PCA as PCA_pyod\nimport catboost as cb\nimport catboost.utils as utils\n\n# # setting pandas display options\npd.set_option('display.max_columns', None)\n# pd.set_option('display.max_rows', None)\n# pd.set_option('display.width', 2000)\n# pd.set_option('display.float_format', '{:20,.2f}'.format)\n# pd.set_option('display.max_colwidth', None)\n\n# # reset display options\n# pd.reset_option('display.max_rows')\n# pd.reset_option('display.max_columns')\n# pd.reset_option('display.width')\n# pd.reset_option('display.float_format')\n# pd.reset_option('display.max_colwidth')\n\n%matplotlib inline\n# from plotly.offline import init_notebook_mode, iplot\n# init_notebook_mode(connected=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T15:59:39.744698Z","iopub.execute_input":"2022-05-19T15:59:39.745239Z","iopub.status.idle":"2022-05-19T15:59:39.762951Z","shell.execute_reply.started":"2022-05-19T15:59:39.745185Z","shell.execute_reply":"2022-05-19T15:59:39.762011Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"<a id='intro'></a>\n## Introduction\n\nThe dataset includes medical appointment status as well as some patient details of Brazilian families; I'd be interested in investigating the reason why some patients do not show up to their scheduled appointments and whether there are opportunities for improvement in appointment administration that would result in a higher attendance frequency.\n\nThe dataset consists of 14 variables as follows:\n\n1. PatientId : Unique identification of a patient\n2. AppointmentID : Unique identification of each appointment\n3. Gender: Male or Female.\n4. ScheduledDay: The day of registering the appointment.\n5. AppointmentDay: The day of actual appointment.\n6. Age: How old is the patient.\n7. Neighbourhood: Where the appointment takes place.\n8. Scholarship: Whether the patient is enrolled in [Bolsa_Família](https://en.wikipedia.org/wiki/Bolsa_Fam%C3%ADlia), which is a social welfare program of the Government of Brazil\n9. Hipertension: Hypertension, also known as high blood pressure. Part of patient's medical history.\n10. Diabetes: Part of patient's medical history.\n11. Alcoholism: drinking of alcohol that results in significant mental or physical health problems. Part of patient's medical history.\n12. Handcap: handicap, part of patient's medical history.\n13. SMS_received: frequent reminders of scheduled appointment.\n14. No-show: whether the patient made the actual appointment or not. 'Yes/True' means the patient did not make the appointment.\n\nSeveral questions will be addressed:\n\n1. Factors driving higher attendance frequency? is it age, gender, medical history or enrollment in Bolsa Família program?\n\n2. Does time has an impact on attendance frequency?\n\n3. Are there certain neighborhoods experiencing higher attendance frequency than others? Why?\n\n4. Is appointment scheduling administered properly?\n\t\nMain dependent variable is appointment status 'No-Show’; rest will be the independent ones.\n","metadata":{}},{"cell_type":"markdown","source":"<a id='wrangling'></a>\n## Data Wrangling","metadata":{}},{"cell_type":"code","source":"# load data\nmaster_df = pd.read_csv('../input/noshowappointments/KaggleV2-May-2016.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:08:58.373662Z","iopub.execute_input":"2022-05-19T16:08:58.375471Z","iopub.status.idle":"2022-05-19T16:08:58.692373Z","shell.execute_reply.started":"2022-05-19T16:08:58.375388Z","shell.execute_reply":"2022-05-19T16:08:58.691515Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"# inspect\nmaster_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:01.070820Z","iopub.execute_input":"2022-05-19T16:09:01.071250Z","iopub.status.idle":"2022-05-19T16:09:01.091822Z","shell.execute_reply.started":"2022-05-19T16:09:01.071212Z","shell.execute_reply":"2022-05-19T16:09:01.091138Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"master_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:02.682403Z","iopub.execute_input":"2022-05-19T16:09:02.682941Z","iopub.status.idle":"2022-05-19T16:09:02.759104Z","shell.execute_reply.started":"2022-05-19T16:09:02.682890Z","shell.execute_reply":"2022-05-19T16:09:02.757824Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"master_df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:06.078184Z","iopub.execute_input":"2022-05-19T16:09:06.078861Z","iopub.status.idle":"2022-05-19T16:09:06.148304Z","shell.execute_reply.started":"2022-05-19T16:09:06.078790Z","shell.execute_reply":"2022-05-19T16:09:06.147063Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"# Inspect unusual age records that are < 0\n\nmaster_df[master_df.Age < 0]","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:07.611952Z","iopub.execute_input":"2022-05-19T16:09:07.612566Z","iopub.status.idle":"2022-05-19T16:09:07.632549Z","shell.execute_reply.started":"2022-05-19T16:09:07.612510Z","shell.execute_reply":"2022-05-19T16:09:07.631410Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"# Inspect unusual Handcap records that are > 1\n\nmaster_df[master_df.Handcap > 1].sample(3, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:09.165249Z","iopub.execute_input":"2022-05-19T16:09:09.165608Z","iopub.status.idle":"2022-05-19T16:09:09.188855Z","shell.execute_reply.started":"2022-05-19T16:09:09.165576Z","shell.execute_reply":"2022-05-19T16:09:09.187925Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"# Check for duplicate records\n\nmaster_df.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:10.662732Z","iopub.execute_input":"2022-05-19T16:09:10.663131Z","iopub.status.idle":"2022-05-19T16:09:10.801961Z","shell.execute_reply.started":"2022-05-19T16:09:10.663096Z","shell.execute_reply":"2022-05-19T16:09:10.800984Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"# Check whether each patient has multiple records\n\nmaster_df.PatientId.nunique(), master_df.AppointmentID.nunique()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:12.181835Z","iopub.execute_input":"2022-05-19T16:09:12.182204Z","iopub.status.idle":"2022-05-19T16:09:12.203092Z","shell.execute_reply.started":"2022-05-19T16:09:12.182172Z","shell.execute_reply":"2022-05-19T16:09:12.201956Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"# Inspect multiple records of single patient\n\nmaster_df[master_df.PatientId.duplicated(keep=False)].sort_values(by='PatientId')","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:13.723266Z","iopub.execute_input":"2022-05-19T16:09:13.723617Z","iopub.status.idle":"2022-05-19T16:09:13.788131Z","shell.execute_reply.started":"2022-05-19T16:09:13.723586Z","shell.execute_reply":"2022-05-19T16:09:13.786882Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"markdown","source":"### Data Cleaning\n\n- Convert ID columns to string, no arithmetic operations will be performed on them\n- Convert Scheduled and Appointment columns to date time, then calculate elapsed time from scheduling day till appointment day\n- Convert No-show to 0 and 1 for convenience\n- Make all header lower case and rename columns for better representation\n- Drop records with age < 0, assuming that 0 represent new born\n- Adjust Handcap values that are > 1 to be 1, assuming that these are just typos\n- Unify age variable of patients with multiple records","metadata":{}},{"cell_type":"code","source":"# Make copy of master df\n\nmaster_df_clean = master_df.copy()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:15.245875Z","iopub.execute_input":"2022-05-19T16:09:15.246264Z","iopub.status.idle":"2022-05-19T16:09:15.276866Z","shell.execute_reply.started":"2022-05-19T16:09:15.246227Z","shell.execute_reply":"2022-05-19T16:09:15.275680Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"# Convert ID columns to string, no arithmetic operations will be performed on them\n\nmaster_df_clean.PatientId, master_df_clean.AppointmentID = master_df_clean.PatientId.astype(str), \\\nmaster_df_clean.AppointmentID.astype(str)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:16.774764Z","iopub.execute_input":"2022-05-19T16:09:16.775443Z","iopub.status.idle":"2022-05-19T16:09:17.062713Z","shell.execute_reply.started":"2022-05-19T16:09:16.775396Z","shell.execute_reply":"2022-05-19T16:09:17.061734Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"# Calculate elapsed time from scheduling day till actual appointment day\n\n# Convert to datetime\nmaster_df_clean.ScheduledDay = master_df_clean.ScheduledDay.apply(np.datetime64)\nmaster_df_clean.AppointmentDay = master_df_clean.AppointmentDay.apply(np.datetime64)\n\n# Extract date only\nmaster_df_clean['scheduledDay_x'] = master_df_clean['ScheduledDay'].dt.date\nmaster_df_clean.scheduledDay_x = master_df_clean.scheduledDay_x.apply(np.datetime64)\n\n# Calculate Elapsed days\nmaster_df_clean['elapsed_days_sch'] = pd.to_timedelta((master_df_clean.AppointmentDay - master_df_clean.scheduledDay_x)).dt.days\n\n# drop new scheduled day column as I need full timestamp data\nmaster_df_clean.drop(columns = ['scheduledDay_x'], inplace =True)\n\n# Inspect\nmaster_df_clean.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:18.336893Z","iopub.execute_input":"2022-05-19T16:09:18.337251Z","iopub.status.idle":"2022-05-19T16:09:19.579593Z","shell.execute_reply.started":"2022-05-19T16:09:18.337212Z","shell.execute_reply":"2022-05-19T16:09:19.578566Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"# Elapsed days distribution\nmaster_df_clean.elapsed_days_sch.plot(kind = 'hist', bins = 20);","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:19.825935Z","iopub.execute_input":"2022-05-19T16:09:19.826326Z","iopub.status.idle":"2022-05-19T16:09:20.055352Z","shell.execute_reply.started":"2022-05-19T16:09:19.826287Z","shell.execute_reply":"2022-05-19T16:09:20.054243Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"# records having -ve values\nmaster_df_clean[master_df_clean.elapsed_days_sch < 0]","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:21.364562Z","iopub.execute_input":"2022-05-19T16:09:21.364958Z","iopub.status.idle":"2022-05-19T16:09:21.387089Z","shell.execute_reply.started":"2022-05-19T16:09:21.364924Z","shell.execute_reply":"2022-05-19T16:09:21.386381Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"# drop -ve values as they are probably erroneous\n\nmaster_df_clean = master_df_clean[~master_df_clean.elapsed_days_sch < 0].copy()\n\nmaster_df_clean.reset_index(drop= True, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:22.836017Z","iopub.execute_input":"2022-05-19T16:09:22.836636Z","iopub.status.idle":"2022-05-19T16:09:22.861624Z","shell.execute_reply.started":"2022-05-19T16:09:22.836599Z","shell.execute_reply":"2022-05-19T16:09:22.860828Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"master_df_clean.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:24.334742Z","iopub.execute_input":"2022-05-19T16:09:24.335157Z","iopub.status.idle":"2022-05-19T16:09:24.340934Z","shell.execute_reply.started":"2022-05-19T16:09:24.335122Z","shell.execute_reply":"2022-05-19T16:09:24.340059Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"# convert No-show to 0 and 1 for convenience\n\nmaster_df_clean['No-show'].replace(to_replace=['No', 'Yes'], value=[0, 1], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:25.823127Z","iopub.execute_input":"2022-05-19T16:09:25.824078Z","iopub.status.idle":"2022-05-19T16:09:25.901242Z","shell.execute_reply.started":"2022-05-19T16:09:25.824023Z","shell.execute_reply":"2022-05-19T16:09:25.900413Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"# Make all header lower case and rename columns for better representation\n\nmaster_df_clean.columns = master_df_clean.columns.str.lower()\n\nmaster_df_clean.rename(columns={'patientid':'patient_id', 'appointmentid':'appointment_id', \n                                'scheduledday': 'scheduling_day', 'appointmentday': 'appointment_day', \n                                'scholarship':'enrolled_in_Bolsa_Família', 'hipertension': 'hypertension', \n                                'handcap':'handicap', 'no-show':'no_show'}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:27.338924Z","iopub.execute_input":"2022-05-19T16:09:27.339267Z","iopub.status.idle":"2022-05-19T16:09:27.345976Z","shell.execute_reply.started":"2022-05-19T16:09:27.339238Z","shell.execute_reply":"2022-05-19T16:09:27.345215Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"# Drop records with age < 0\n\nmaster_df_clean = master_df_clean[master_df_clean.age >= 0]\n\nmaster_df_clean.reset_index(drop= True, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:28.797823Z","iopub.execute_input":"2022-05-19T16:09:28.798487Z","iopub.status.idle":"2022-05-19T16:09:28.824570Z","shell.execute_reply.started":"2022-05-19T16:09:28.798450Z","shell.execute_reply":"2022-05-19T16:09:28.823704Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"code","source":"# Adjust Handicap values that are > 1 to be 1 as it is not clear whether these entries are errors or representation\n# of various stages of Handicap\n\nmaster_df_clean.loc[master_df_clean.handicap > 1 , 'handicap'] = 1","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:30.282019Z","iopub.execute_input":"2022-05-19T16:09:30.282378Z","iopub.status.idle":"2022-05-19T16:09:30.290624Z","shell.execute_reply.started":"2022-05-19T16:09:30.282347Z","shell.execute_reply":"2022-05-19T16:09:30.289868Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"# Unify age variable of patients with multiple records, I'll use the first recorded age as lead.  \n# this will be done through creating a temp sorted dataframe for duplicate patient ids,\n# identify gaps in recorded ages for single patient record,\n# filter on each unique id then create a dict with IDs being keys and first recorded age as values\n# finally map dict keys to main dataframe patient IDs\n\nduplicate_id = master_df_clean[master_df_clean.patient_id.duplicated(keep=False)].sort_values(by='patient_id')\n\nduplicate_id['first'] = duplicate_id['age']\n\nduplicate_id['last'] = duplicate_id['age'].shift(-1)\n\nduplicate_id['gap'] = duplicate_id['age'].shift(-1) - duplicate_id['age']\n\nduplicate_id_fltrd = duplicate_id[(duplicate_id.first != duplicate_id.last) & (duplicate_id.gap == 1)]\n\nduplicate_id_dict = duplicate_id_fltrd.set_index('patient_id').to_dict()['age']\n\nmaster_df_clean.loc[master_df_clean.patient_id.isin(duplicate_id_fltrd.patient_id), 'age'] = \\\nmaster_df_clean.patient_id.map(duplicate_id_dict)\n\n# inspect\n\nmaster_df_clean[master_df_clean.patient_id.isin(duplicate_id_fltrd.patient_id)].sort_values(by='patient_id', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:31.852972Z","iopub.execute_input":"2022-05-19T16:09:31.853784Z","iopub.status.idle":"2022-05-19T16:09:32.153691Z","shell.execute_reply.started":"2022-05-19T16:09:31.853703Z","shell.execute_reply":"2022-05-19T16:09:32.152693Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"markdown","source":"<li><a href=\"#toc\">Table of contents</a></li>","metadata":{}},{"cell_type":"markdown","source":"<a id='eda'></a>\n## Exploratory Data Analysis\n\n\n### Factors driving higher attendance frequency? is it age, gender, medical history or enrollment in Bolsa Família program?","metadata":{}},{"cell_type":"code","source":"# Grouping multiple records of unique patients into single records and computing average, min, max of some appointment details\n# this is to avoid double counting of same patient records while analysing independent variables \n# and also to provide better representation of appointment details to facilitate comparability\n\n# First, group records and make calculations\nunique_no_show_df = master_df_clean.groupby('patient_id')[['no_show']].agg(['count', 'mean'])\n\nunique_no_show_df.columns = unique_no_show_df.columns.droplevel()\n\nunique_elapsed_days_sch_df = master_df_clean.groupby('patient_id')[['elapsed_days_sch']].agg(['mean', 'max', 'min'])\n\nunique_elapsed_days_sch_df.columns = unique_elapsed_days_sch_df.columns.droplevel()\n\nunique_sms_received_df = master_df_clean.groupby('patient_id')[['sms_received']].agg(['mean'])\n\nunique_sms_received_df.columns = unique_sms_received_df.columns.droplevel()\n\ndfs = [unique_no_show_df, unique_elapsed_days_sch_df, unique_sms_received_df]\n\ngrouped_df = reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True, how='outer'), dfs) \\\n.rename(columns = {'count':'appointment_count', 'mean_x': 'no_show_mean', 'mean_y': 'elapsed_days_sch_mean', \n                   'max':'elapsed_days_sch_max', 'min':'elapsed_days_sch_min', 'mean':'sms_received_mean'})\n\nmaster_df_clean.index = master_df_clean.patient_id\n\n# Then merge to obtain other variables, \n# keep first occuence only as these variables are supposed to be fixed for a single patient\n\ngrouped_df = pd.merge(grouped_df, master_df_clean.drop_duplicates(subset=['patient_id'], keep='first'), \n                      left_index=True, right_index=True, how='left')\n\nmaster_df_clean.reset_index(drop=True, inplace=True)\n\ngrouped_df.reset_index(drop=True, inplace=True)\n\ncolumns_list = grouped_df.columns.tolist()\n\ncolumns_sort = columns_list[6:18]+columns_list[0:6]\n\ngrouped_df = grouped_df[columns_sort]","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:33.349203Z","iopub.execute_input":"2022-05-19T16:09:33.349858Z","iopub.status.idle":"2022-05-19T16:09:34.126591Z","shell.execute_reply.started":"2022-05-19T16:09:33.349797Z","shell.execute_reply":"2022-05-19T16:09:34.125733Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"# duplicate intervals to bypass 'include_lowest' bug as suggested --> (https://github.com/pandas-dev/pandas/issues/23164)\n# age groups categorization follows this link,\n# https://brasilemsintese.ibge.gov.br/populacao/distribuicao-da-populacao-por-grandes-grupos-de-idade.html\n# I split the age group of 15-64 into two groups for better representation\n\nbins = pd.IntervalIndex.from_tuples([(-0.001,0), (0, 14), (14.999,15), (15, 24), (24.999,25), (25, 64), (64.999,65), \n                                     (65,122)])\nlabels = ['Children','Children','Youth', 'Youth', 'Adults','Adults','Seniors','Seniors']\n\ngrouped_df['age_group'] = pd.cut(grouped_df.age, bins, include_lowest =True).map(dict(zip(bins, labels)))","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:34.820503Z","iopub.execute_input":"2022-05-19T16:09:34.821063Z","iopub.status.idle":"2022-05-19T16:09:34.952804Z","shell.execute_reply.started":"2022-05-19T16:09:34.821011Z","shell.execute_reply":"2022-05-19T16:09:34.951458Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"# calculate attendance frequency per each age group as a percentage of total group\n\nage_groups = grouped_df.age_group.value_counts()\n\nlow_atnd_freq = grouped_df[grouped_df.no_show_mean > 0].age_group.value_counts()\n\nhgh_atnd_freq = grouped_df[grouped_df.no_show_mean == 0].age_group.value_counts()\n\nout_low = low_atnd_freq / age_groups\n\nout_low.sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:35.965690Z","iopub.execute_input":"2022-05-19T16:09:35.966077Z","iopub.status.idle":"2022-05-19T16:09:36.034729Z","shell.execute_reply.started":"2022-05-19T16:09:35.966035Z","shell.execute_reply":"2022-05-19T16:09:36.033722Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"out_hgh = hgh_atnd_freq / age_groups\n\nout_hgh.sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:36.595604Z","iopub.execute_input":"2022-05-19T16:09:36.596001Z","iopub.status.idle":"2022-05-19T16:09:36.605728Z","shell.execute_reply.started":"2022-05-19T16:09:36.595966Z","shell.execute_reply":"2022-05-19T16:09:36.604603Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"# Plot the results\n\nfig, axes = plt.subplots(ncols=2, figsize=(8, 4))\nfig.tight_layout()\nax1, ax2 = axes\n\n# setup plot labels, box sizes and colors\n\nlabels_low = [i + '\\n' + str((round(v,2))) for i, v in out_low.items()]\nsizes_low = out_low.values\ncolors_low = [plt.cm.Spectral(i/float(len(labels_low))) for i in range(len(labels_low))]\n\nlabels_hgh = [i + '\\n' + str((round(v,2))) for i, v in out_hgh.items()]\nsizes_hgh = out_hgh.values\ncolors_hgh = [plt.cm.Spectral(i/float(len(labels_hgh))) for i in range(len(labels_hgh))]\n\n# Draw Plot\nsquarify.plot(sizes=sizes_low, label=labels_low, color=colors_low, alpha=1, bar_kwargs=dict(linewidth=2, edgecolor=\"#222222\"), text_kwargs=dict(fontsize=11), ax=ax1);\n\nsquarify.plot(sizes=sizes_hgh, label=labels_hgh, color=colors_hgh, alpha=1, bar_kwargs=dict(linewidth=2, edgecolor=\"#222222\"), text_kwargs=dict(fontsize=11), ax=ax2);\n\n# Decorate\nax1.set_title('Age group classification of patients' + '\\n' + 'with low attendance frequency', fontsize=15, y=1.05, c='orange')\nax2.set_title('Age group classification of patients' + '\\n' + 'with high attendance frequency', fontsize=15, y=1.05, c='green')\nax1.axis('off'); ax2.axis('off');","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:37.020110Z","iopub.execute_input":"2022-05-19T16:09:37.020612Z","iopub.status.idle":"2022-05-19T16:09:37.293687Z","shell.execute_reply.started":"2022-05-19T16:09:37.020581Z","shell.execute_reply":"2022-05-19T16:09:37.292345Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"# Create summary table for patient's gender and medical condition \n# to apply value count on the whole dataframe instead of one variable at a time\n# Same concept of splitting population as before\n\nsummary_high_show_df = grouped_df[grouped_df.no_show_mean == 0].copy()\n\nsummary_low_show_df =  grouped_df[grouped_df.no_show_mean > 0].copy()\n\nsummary_high_show_df.drop(columns={'patient_id','appointment_id','scheduling_day','appointment_day',\n                                   'age','neighbourhood', 'appointment_count', 'no_show_mean',\n                                   'elapsed_days_sch_mean', 'elapsed_days_sch_max', 'elapsed_days_sch_min', \n                                   'elapsed_days_sch_min', 'age_group', 'sms_received_mean'}, inplace = True)\n\nsummary_low_show_df.drop(columns={'patient_id','appointment_id','scheduling_day','appointment_day',\n                                  'age','neighbourhood', 'appointment_count', 'no_show_mean',\n                                  'elapsed_days_sch_mean', 'elapsed_days_sch_max', 'elapsed_days_sch_min', \n                                  'elapsed_days_sch_min', 'age_group', 'sms_received_mean'}, inplace = True)\n\nsummary_high_show_df = summary_high_show_df.apply(pd.Series.value_counts)\n\nsummary_low_show_df = summary_low_show_df.apply(pd.Series.value_counts)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:37.438986Z","iopub.execute_input":"2022-05-19T16:09:37.439416Z","iopub.status.idle":"2022-05-19T16:09:37.504663Z","shell.execute_reply.started":"2022-05-19T16:09:37.439380Z","shell.execute_reply":"2022-05-19T16:09:37.503740Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"# Calculate relative weight of each condition and plotting\n\nsummary_high_show_df.transform(lambda x: round(x / x.sum(),2)).plot(kind='pie', \n                                                                    subplots = True, legend = False, figsize = (20, 10), \n                                                                    fontsize = 15, \n                                                                    autopct = \\\n                                                                    lambda p: '{:.0f}%'.format(round(p)) if p > 0 else '', \n                                                                    labels = None);\n\nplt.gcf().suptitle('Characteristics of patients attending medical appointments', fontsize=20, y=.68, color = 'brown')\n\nplt.legend([\"False\", \"True\", 'Female', 'Male'], bbox_to_anchor=(1.05, 1), loc = 4)\n\nsummary_low_show_df.transform(lambda x: round(x / x.sum(),2)).plot(kind='pie', \n                                                                   subplots = True, legend = False, figsize = (20, 10),\n                                                                   fontsize = 15, \n                                                                   autopct = \\\n                                                                   lambda p: '{:.0f}%'.format(round(p)) if p > 0 else '', \n                                                                   labels = None);\n\nplt.gcf().suptitle(\"Characteristics of patients skipping medical appointments\", fontsize=20, y=.68, color = 'brown');","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:37.842653Z","iopub.execute_input":"2022-05-19T16:09:37.843036Z","iopub.status.idle":"2022-05-19T16:09:38.982817Z","shell.execute_reply.started":"2022-05-19T16:09:37.843005Z","shell.execute_reply":"2022-05-19T16:09:38.981676Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"markdown","source":"Seniors takes the lead being highest attendance frequency age group, while youth leads the low attendance one. It makes sense as health deteriorates with age.\n\nThe dataset is dominated by female presence; it seems that women are receiving extra medical care than men.\n\nPatients' characteristics do not significantly vary between those who attend or skip their medical appointment. However, enrolment in Bolsa Familia program, hypertension and diabetes medical conditions are the highest among patients who attend.","metadata":{}},{"cell_type":"markdown","source":"### Does time has an impact on attendance frequency?","metadata":{}},{"cell_type":"code","source":"# Resample and inspect\n\ngrouped_df[grouped_df.no_show_mean > 0].groupby(['scheduling_day'])['no_show_mean'].count().resample('1m').sum()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:38.984875Z","iopub.execute_input":"2022-05-19T16:09:38.985298Z","iopub.status.idle":"2022-05-19T16:09:39.012970Z","shell.execute_reply.started":"2022-05-19T16:09:38.985255Z","shell.execute_reply":"2022-05-19T16:09:39.012199Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"grouped_df[grouped_df.no_show_mean > 0].groupby(['appointment_day'])['no_show_mean'].count().resample('1m').sum()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:39.014200Z","iopub.execute_input":"2022-05-19T16:09:39.014585Z","iopub.status.idle":"2022-05-19T16:09:39.034942Z","shell.execute_reply.started":"2022-05-19T16:09:39.014557Z","shell.execute_reply":"2022-05-19T16:09:39.033944Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"grouped_df[grouped_df.no_show_mean == 0].groupby(['scheduling_day'])['no_show_mean'].count().resample('1m').sum()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:39.048129Z","iopub.execute_input":"2022-05-19T16:09:39.048517Z","iopub.status.idle":"2022-05-19T16:09:39.088689Z","shell.execute_reply.started":"2022-05-19T16:09:39.048482Z","shell.execute_reply":"2022-05-19T16:09:39.087615Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"code","source":"grouped_df[grouped_df.no_show_mean == 0].groupby(['appointment_day'])['no_show_mean'].count().resample('1m').sum()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:39.456784Z","iopub.execute_input":"2022-05-19T16:09:39.457379Z","iopub.status.idle":"2022-05-19T16:09:39.479634Z","shell.execute_reply.started":"2022-05-19T16:09:39.457343Z","shell.execute_reply":"2022-05-19T16:09:39.478701Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"# Summary of appointments by weekday\n\nmaster_df_clean['day'] = master_df_clean.appointment_day.dt.day_name()\n\nmaster_df_clean.groupby('day')['no_show'].count()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:39.861172Z","iopub.execute_input":"2022-05-19T16:09:39.861550Z","iopub.status.idle":"2022-05-19T16:09:39.928222Z","shell.execute_reply.started":"2022-05-19T16:09:39.861514Z","shell.execute_reply":"2022-05-19T16:09:39.927159Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"code","source":"# attendance frequency by weekdays\n\nmaster_df_clean.groupby('day')['no_show'].value_counts(normalize=True).unstack().plot(\n    kind = 'bar', stacked = True, ylabel = 'Frequency', color = ['green', 'orange']);\n\n# Decoration9\n\nplt.title('Attendance frequency by weekday', fontsize = 17, c = 'brown', y = 1.06)\n\nplt.legend([\"Attendance\", \"Absence\"], bbox_to_anchor = (1.05, 1), loc = 'upper left')\n\nplt.gca().spines[\"top\"].set_alpha(0.0); plt.gca().spines[\"right\"].set_alpha(0.0)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:40.275411Z","iopub.execute_input":"2022-05-19T16:09:40.276010Z","iopub.status.idle":"2022-05-19T16:09:40.510712Z","shell.execute_reply.started":"2022-05-19T16:09:40.275957Z","shell.execute_reply":"2022-05-19T16:09:40.509941Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"# Check whether decreas in elapsed time between shceduling and actual appointment date impact attendace ferquency\n# 16 being average elapsed timed of the dataset\n\nround(grouped_df[(grouped_df.elapsed_days_sch_mean >= 16) & (grouped_df.no_show_mean == 0)].age_group.value_counts(normalize=True),2)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:40.681603Z","iopub.execute_input":"2022-05-19T16:09:40.681999Z","iopub.status.idle":"2022-05-19T16:09:40.699766Z","shell.execute_reply.started":"2022-05-19T16:09:40.681962Z","shell.execute_reply":"2022-05-19T16:09:40.698668Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"code","source":"round(grouped_df[(grouped_df.elapsed_days_sch_mean < 16) & (grouped_df.no_show_mean == 0)].age_group.value_counts(normalize=True),2)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:41.086866Z","iopub.execute_input":"2022-05-19T16:09:41.087368Z","iopub.status.idle":"2022-05-19T16:09:41.114980Z","shell.execute_reply.started":"2022-05-19T16:09:41.087335Z","shell.execute_reply":"2022-05-19T16:09:41.113876Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"markdown","source":"May appears to be squeezed compared to other months as most appointments were scheduled to take place during this month. It might be because this period follows the heavy festive months where people are busy celebrating according to [this](https://www.lonelyplanet.com/brazil/narratives/planning/month-by-month), but still this can be considered as an improvement opportunity for appointment administration to spread appointments across months.\n\nThere is no preference for specific weekday over the other when it comes to attendance frequency, despite that Thursday/Wednesday experienced improved attendance frequency and Friday/Saturday are the lowest, it’s not significantly different than rest of weekdays. \n\nNo appointments where scheduled to take place on Sunday which is understandable being a weekend day but given that attendance frequency on Saturday, which is also a weekend day, is comparable to the rest of the weekdays it implies that patients are willing to attend appointments on weekends. This can be also considered as an improvement opportunity for appointment administration to spread appointments across weekdays regardless of weekends. \n\nOn the other hand, decrease in elapsed time between scheduling and actual appointment date does not result in a significant decrease in missed appointments. Which is a bit surprising actually and warrants further investigation of underlying appointment scheduling data.","metadata":{}},{"cell_type":"markdown","source":"### Are there certain neighborhoods experiencing higher attendance frequency than others? Why?","metadata":{}},{"cell_type":"code","source":"# Check appointment distribution among Neighborhoods\n\nround(master_df_clean.neighbourhood.value_counts(normalize=True) * 100, 2)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:41.645284Z","iopub.execute_input":"2022-05-19T16:09:41.645677Z","iopub.status.idle":"2022-05-19T16:09:41.682455Z","shell.execute_reply.started":"2022-05-19T16:09:41.645633Z","shell.execute_reply":"2022-05-19T16:09:41.681291Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"# Create a summary table for attendance frequency per each neighborhood\n# to facilitate comparison among each and identify which neighborhood experienced most successful appointments\n\nappointments_by_neighbourhoods = grouped_df.groupby(['neighbourhood'])['appointment_count'].sum()\n\nattendance_by_neighbourhood = grouped_df[grouped_df.no_show_mean == 0]. \\\ngroupby(['neighbourhood'])['appointment_count'].sum()\n\nfrequency_of_attendance= attendance_by_neighbourhood / appointments_by_neighbourhoods\n\nfrequency_of_attendance = frequency_of_attendance.to_frame().rename(columns={'appointment_count':'frequency_of_attendance'})\n\nappointments_by_neighbourhoods = appointments_by_neighbourhoods.to_frame(). \\\nrename(columns={'appointment_count':'total_appointments'})\n\nneighbourhoods_summary = pd.merge(frequency_of_attendance, appointments_by_neighbourhoods, \n                                  left_index=True, right_index=True, how='left')\n\ntemp_df = master_df_clean.groupby('neighbourhood')[['elapsed_days_sch', 'sms_received']].agg(['mean'])\n\ntemp_df.columns = temp_df.columns.droplevel(level=1)\n\nneighbourhoods_summary = pd.merge(neighbourhoods_summary, temp_df, left_index=True, right_index=True, how='left'). \\\nrename(columns={'elapsed_days_sch':'elapsed_days_sch_mean', 'sms_received':'reminder_frequency'})\n\nneighbourhoods_summary.reset_index(inplace=True)\n\nneighbourhoods_summary.rename(columns={'index':'neighbourhood'}, inplace=True)\n\nneighbourhoods_summary.elapsed_days_sch_mean = neighbourhoods_summary.elapsed_days_sch_mean.astype(int)\n\nneighbourhoods_summary = round(neighbourhoods_summary,2)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:42.078019Z","iopub.execute_input":"2022-05-19T16:09:42.078723Z","iopub.status.idle":"2022-05-19T16:09:42.166311Z","shell.execute_reply.started":"2022-05-19T16:09:42.078668Z","shell.execute_reply":"2022-05-19T16:09:42.165357Z"},"trusted":true},"execution_count":130,"outputs":[]},{"cell_type":"code","source":"# Stats of top 10 neighborhoods with highest attendance frequency\n\nneighbourhoods_summary.sort_values(by='frequency_of_attendance', ascending =False).head(10).describe()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:42.464451Z","iopub.execute_input":"2022-05-19T16:09:42.464958Z","iopub.status.idle":"2022-05-19T16:09:42.489466Z","shell.execute_reply.started":"2022-05-19T16:09:42.464918Z","shell.execute_reply":"2022-05-19T16:09:42.488445Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"code","source":"# Stats of top 10 neighborhoods with lowest attendance frequency\n\nneighbourhoods_summary.sort_values(by='frequency_of_attendance').head(10).describe()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:42.867306Z","iopub.execute_input":"2022-05-19T16:09:42.867693Z","iopub.status.idle":"2022-05-19T16:09:42.896236Z","shell.execute_reply.started":"2022-05-19T16:09:42.867656Z","shell.execute_reply":"2022-05-19T16:09:42.895164Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"# details of top 10 neighborhoods with highest attendance frequency\n\nneighbourhoods_summary.sort_values(by='frequency_of_attendance', ascending =False).head(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:43.272431Z","iopub.execute_input":"2022-05-19T16:09:43.272813Z","iopub.status.idle":"2022-05-19T16:09:43.288286Z","shell.execute_reply.started":"2022-05-19T16:09:43.272774Z","shell.execute_reply":"2022-05-19T16:09:43.287501Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"# details of top 10 neighborhoods with lowest attendance frequency\n\nneighbourhoods_summary.sort_values(by='frequency_of_attendance').head(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:43.681247Z","iopub.execute_input":"2022-05-19T16:09:43.681754Z","iopub.status.idle":"2022-05-19T16:09:43.697641Z","shell.execute_reply.started":"2022-05-19T16:09:43.681710Z","shell.execute_reply":"2022-05-19T16:09:43.696471Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"code","source":"# will compare age and gender composition among top 2 neighborhood of each highest and lowest groups as \n# other variables are approximatly similar and will be further investigated in appointment adminstration analysis section \n\nprint('ILHA DO BOI gender and age composition :', '\\n','\\n',\n      round(grouped_df[grouped_df.neighbourhood == 'ILHA DO BOI'].gender.value_counts(normalize=True),2),'\\n','\\n',\n      round(grouped_df[grouped_df.neighbourhood == 'ILHA DO BOI'].age_group.value_counts(normalize=True),2),'\\n','\\n',\n      '-'* 20,'\\n','\\n',\n      'AEROPORTO gender and age composition :', '\\n','\\n',\n      round(grouped_df[grouped_df.neighbourhood == 'AEROPORTO'].gender.value_counts(normalize=True),2),'\\n','\\n',\n      round(grouped_df[grouped_df.neighbourhood == 'AEROPORTO'].age_group.value_counts(normalize=True),2)\n     )","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:44.087233Z","iopub.execute_input":"2022-05-19T16:09:44.087943Z","iopub.status.idle":"2022-05-19T16:09:44.145412Z","shell.execute_reply.started":"2022-05-19T16:09:44.087890Z","shell.execute_reply":"2022-05-19T16:09:44.144351Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"code","source":"print('ILHA DO FRADE gender and age composition :', '\\n','\\n',\n      round(grouped_df[grouped_df.neighbourhood == 'ILHA DO FRADE'].gender.value_counts(normalize=True),2),'\\n','\\n',\n      round(grouped_df[grouped_df.neighbourhood == 'ILHA DO FRADE'].age_group.value_counts(normalize=True),2),'\\n','\\n',\n      '-'* 20,'\\n','\\n',\n      'JESUS DE NAZARETH gender and age composition :', '\\n','\\n',\n      round(grouped_df[grouped_df.neighbourhood == 'SANTOS DUMONT'].gender.value_counts(normalize=True),2),'\\n','\\n',\n      round(grouped_df[grouped_df.neighbourhood == 'SANTOS DUMONT'].age_group.value_counts(normalize=True),2)\n     )","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:44.493408Z","iopub.execute_input":"2022-05-19T16:09:44.494027Z","iopub.status.idle":"2022-05-19T16:09:44.554446Z","shell.execute_reply.started":"2022-05-19T16:09:44.493991Z","shell.execute_reply":"2022-05-19T16:09:44.553471Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"# Check if a neighborhood didn’t have any successful appointment\n\nneighbourhoods_summary[neighbourhoods_summary.frequency_of_attendance.isnull()]","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:44.900714Z","iopub.execute_input":"2022-05-19T16:09:44.901099Z","iopub.status.idle":"2022-05-19T16:09:44.915126Z","shell.execute_reply.started":"2022-05-19T16:09:44.901068Z","shell.execute_reply":"2022-05-19T16:09:44.913931Z"},"trusted":true},"execution_count":137,"outputs":[]},{"cell_type":"code","source":"# Check details of this neighborhood\n\nmaster_df_clean[master_df_clean.neighbourhood == 'ILHAS OCEÂNICAS DE TRINDADE']","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:45.318754Z","iopub.execute_input":"2022-05-19T16:09:45.319344Z","iopub.status.idle":"2022-05-19T16:09:45.355911Z","shell.execute_reply.started":"2022-05-19T16:09:45.319288Z","shell.execute_reply":"2022-05-19T16:09:45.354899Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"markdown","source":"Comparing summary statistics of top 10 neighborhoods having highest and lowest attendance frequency indicates that neither shorter elapsed time nor higher frequent reminders have a significant impact on attendance frequency.\n\nBy examining the demographic composition of top neighborhoods in from each attendance category (highest/lowest) we can conclude that neighborhoods with demographic composition of less senior patients had less attendance frequency. This confirms my earlier observation that older patients have high attendance frequency.\n\nHowever, ILHAS OCEÂNICAS DE TRINDADE neighborhood did not experience any successful appointments contrary to my expectation given the composition of gender and age group. Maybe because elapsed time was 28 days and no frequent reminders were sent to patients.\n\nThis will be further investigated in appointment administration section.\n\nIt warrants additional data to conclude why exactly some Neighborhoods experience higher attendance frequency than others (nature of visits i.e. is it for severe illness 'cancer for example' or just customary, distance between patient and appoint location, commute times, venue, etc...)","metadata":{}},{"cell_type":"markdown","source":"### Is appointment scheduling administered properly?","metadata":{}},{"cell_type":"code","source":"# inspect patients with multiple appointments as it will give better overview on scheduling practices\n\ngrouped_df[grouped_df.appointment_count > 1].sort_values(by='appointment_count', ascending=False).head(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:45.722586Z","iopub.execute_input":"2022-05-19T16:09:45.723133Z","iopub.status.idle":"2022-05-19T16:09:45.766103Z","shell.execute_reply.started":"2022-05-19T16:09:45.723090Z","shell.execute_reply":"2022-05-19T16:09:45.765078Z"},"trusted":true},"execution_count":139,"outputs":[]},{"cell_type":"code","source":"# inspect records of patients with several appointments\n\nmaster_df_clean[(master_df_clean.patient_id == '1484143378533.0')].sort_values(by='scheduling_day') \\\n.drop(columns={'enrolled_in_Bolsa_Família', 'hypertension', 'diabetes', 'alcoholism', 'handicap'})","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:46.136537Z","iopub.execute_input":"2022-05-19T16:09:46.137075Z","iopub.status.idle":"2022-05-19T16:09:46.191515Z","shell.execute_reply.started":"2022-05-19T16:09:46.137031Z","shell.execute_reply":"2022-05-19T16:09:46.190481Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"code","source":"master_df_clean[(master_df_clean.patient_id == '17798942295934.0')].sort_values(by='appointment_day') \\\n.drop(columns={'enrolled_in_Bolsa_Família', 'hypertension', 'diabetes', 'alcoholism', 'handicap'})","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:46.537917Z","iopub.execute_input":"2022-05-19T16:09:46.538550Z","iopub.status.idle":"2022-05-19T16:09:46.587774Z","shell.execute_reply.started":"2022-05-19T16:09:46.538497Z","shell.execute_reply":"2022-05-19T16:09:46.586383Z"},"trusted":true},"execution_count":141,"outputs":[]},{"cell_type":"code","source":"# Calculate attendace frequency of patients having multiple appointments in the same day\n\ntemp_df = master_df_clean[master_df_clean.patient_id.duplicated(keep=False)].sort_values(by='patient_id')\n\ntemp_df['gap'] = temp_df['appointment_day'].shift(-1) - temp_df['appointment_day']\n\ntemp_df['gap'] = temp_df['gap'].dt.days\n\ntemp_df_fltrd = temp_df[(temp_df.no_show == 1) & (temp_df.gap == 0)]\n\nround(temp_df_fltrd.shape[0] / temp_df[temp_df.no_show == 1].shape[0],2)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:46.957931Z","iopub.execute_input":"2022-05-19T16:09:46.958290Z","iopub.status.idle":"2022-05-19T16:09:47.182674Z","shell.execute_reply.started":"2022-05-19T16:09:46.958257Z","shell.execute_reply":"2022-05-19T16:09:47.181557Z"},"trusted":true},"execution_count":142,"outputs":[]},{"cell_type":"code","source":"# Heatmap for appointment stats\n\nappointment_stats = master_df_clean.groupby(['appointment_id'])[['no_show', 'sms_received', 'elapsed_days_sch']].mean()\n\n# Plot\n\nplt.figure(figsize=(10,6))\n\nsns.heatmap(appointment_stats.corr(), xticklabels = appointment_stats.corr().columns, \n            yticklabels = appointment_stats.corr().columns, cmap = 'BuPu', center = 0, annot = True, linewidths = .01)\n\n# Decorations\n\nplt.title('Appointment\\'s Stats Heat Map', fontsize = 20, color = 'purple' , y = 1.06)\n\nplt.xticks(fontsize = 13)\n\nplt.yticks(rotation = 0, fontsize = 13, va = \"center\");","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:47.365361Z","iopub.execute_input":"2022-05-19T16:09:47.365727Z","iopub.status.idle":"2022-05-19T16:09:47.935555Z","shell.execute_reply.started":"2022-05-19T16:09:47.365695Z","shell.execute_reply":"2022-05-19T16:09:47.934503Z"},"trusted":true},"execution_count":143,"outputs":[]},{"cell_type":"markdown","source":"I'll comment on appointment administration activities then will discuss my concerns on the dataset itself\n\n##### Analysis of appointment administration activities\n\n8% of missed appointments were scheduled in the same day, this is noted for patients having multiple appointments. Another development opportunity for appointment administration is to steer away from scheduling multiple appointments in the same day; unless there is a need to.\n\nFriday is a common day for missed appointments as noted from the detailed records inspected above and those of ILHAS OCEÂNICAS DE TRINDADE neighborhood. This is also confirmed by earlier analysis of attendance by weekday as Friday was one of the lowest.\n\nFrequent reminders are associated with the length of elapsed days as noted from +ve correlation between sms received and elapsed time. However, correlation coefficient indicate weak linear relationship which can be observed in the appointment records of the patients above. Take patient # 1484143378533 as an example, we can observe that reminders are not sent systematically for similar elapsed time period same as for patient # 17798942295934 but with less frequency. Switch sorting of each table to reproduce these observation. \n\nThis can be an area for improvement in appointment administration by following a systematic approach in sending reminders to patients and/or increasing the frequency or methods of reminders. However, given the weak correlation that is almost nonlinear between the frequency of reminders and attendance; associated costs of increasing or changing reminder methods need to be closely examined. \n\nWeak +ve correlation that is almost nonlinear between elapsed time and attendance frequency need to be investigated further, as a strong +ve linear relationship is expected unless there are limited sources of medical services providers.\n\n\n##### concerns on data integrity\n\nActually, appointment administration and related data needs a lot further investigation than any other independent variable in this dataset to confirm data integrity, as several red flags were noted as follows:\n\n1. Insignificant impact of shorter appointment lead times on attendance frequency\n2. Time stamps and appointment ID discrepancies, discussed further below.\n\nUpon inspecting the appointment record of the 2 patients above, having around 80% attendance frequency and a total of 50 appointments, I noticed that:\n\n1. Several appointments were scheduled in the same day within a very short time, sometimes less than a minute!\n\n2. Appointment ID sequence gap is significant compared to the short scheduling time.\n\nTo elaborate more, let’s take the record of patient # 1484143378533 as an example. We can see that there are 5 appointments scheduled on the 2nd of May’16 with the following time stamps sorted in an ascending order ‘18:56:40’, ‘18:57:31, ‘18:59:07’, ‘19:00:08’,’ 19:01:32’. Each appoint has a unique ID as follows ‘5649276’, ‘5649283’,’ 5649297’,’ 5649306,’ 5649316’. Such data sparks a lot of concerns:\n\n- How is it possible to have such quick calls/registrations?\n\n- How come gaps in appointment ID are significant within such short time span? Are all Neighborhoods linked to the same registration system?\n\n- What’s the point of having such pattern of registration, does they all relate to each other? Should some of them be cancelled?\n\n- Is No show record for these particular appointments correct?\n\n- IF it’s not a registration problem/error rather a data gathering one, does this affect the accuracy of other independent variables?\n\nUnfortunately, answers to these questions can’t be extracted from the current dataset. But I’m Curious!","metadata":{}},{"cell_type":"markdown","source":"<a id='conclusions'></a>","metadata":{}},{"cell_type":"markdown","source":"## Conclusion\n\n1. Senior age group have higher attendance frequency than others\n\t\n2. Youth age group most likely will skip their medical appointments\n\t\n3. Enrolment in Bolsa Familia program, hypertension and diabetes medical conditions are common characteristics of patients with higher attendance frequency\n\t\n4. Seniors and adults age group shows a slight improvement in attendance frequency with shorter lead times.\n\n5. No preference for specific weekday over the other when it comes to attendance frequency.\n\n\n## Appointment administration improvement opportunities \n\n1. Spread appointments across months and weekdays regardless of weekends.\n\n2. Avoid scheduling multiple appointments in the same day for a single patient. Unless there is a need to.\n\n3. Follow a systematic approach when sending reminders to patients and consider increasing the frequency and/or methods of reminders while closely monitoring the associated costs of doing so.\n\t\n\n## Limitations\n\t\n- How was the appointment scheduled is not clear.\n\t\n- Sufficient details about appointments are not provided (Type, employee ID, physician, venue, etc…).\n\t\n- Medical history of children has some unusual entries like alcoholism. I did not disregard this.\n\t\n- Count and timing of sms reminders are not provided\n\t\n- I assumed age of 0 relates to newborns and not data entry error\n\n- Appointment administration and related data anomalies can lead to different interpretations if proved to be valid, actually it casts doubts on the integrity of the whole dataset.","metadata":{}},{"cell_type":"markdown","source":"<li><a href=\"#toc\">Table of contents</a></li>","metadata":{}},{"cell_type":"markdown","source":"<a id='predictions'></a>","metadata":{}},{"cell_type":"markdown","source":"## Appointment predictions\n\n#### modeling workflow:\n\n- Data preprocessing including:\n  - Removing data points having identical features but different class labels, since I can't conclude on the reason for such instances from the given data set.\n  - Outlier detection and removal using PyOD.\n- Checking for multicollinearity.\n- Feature selection using different techniques.\n- Testing several models with different feature sets.\n- Hyperparameter tuning for best performing model(s).\n- Testing multiple sampling techniques to address class imbalances.\n- Concluding on best performing model and features.\n\ncheck out this [GOLD MINE](https://machinelearningmastery.com/), the blog has been a great help guiding this modeling.","metadata":{}},{"cell_type":"code","source":"master_df_clean_ml = master_df_clean.copy()\n\nmaster_df_clean_ml.age = master_df_clean_ml.age.astype('int64')\n\nmaster_df_clean_ml.no_show = master_df_clean_ml.no_show.replace({0:1, 1:0})\n\nmaster_df_clean_ml.rename(columns = {'no_show': 'attend'}, inplace = True)\n\ngender = pd.get_dummies(master_df_clean_ml.gender, drop_first=True)\n\nmaster_df_clean_ml = pd.concat([master_df_clean_ml, gender], axis = 1)\n\nmaster_df_clean_ml.drop(columns=['appointment_id', 'gender', 'neighbourhood', \n                                 'scheduling_day', 'day'], axis =1 , inplace = True)\n\nmaster_df_clean_ml.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:47.937443Z","iopub.execute_input":"2022-05-19T16:09:47.937884Z","iopub.status.idle":"2022-05-19T16:09:48.037028Z","shell.execute_reply.started":"2022-05-19T16:09:47.937817Z","shell.execute_reply":"2022-05-19T16:09:48.036003Z"},"trusted":true},"execution_count":144,"outputs":[]},{"cell_type":"code","source":"master_df_clean_ml.drop(columns = ['patient_id', 'appointment_day'], inplace = True)\n\ncolumns_list = master_df_clean_ml.columns.tolist()\n\ncolumns_sort = columns_list[0:7]+columns_list[8:]+columns_list[7:8]\n\nmaster_df_clean_ml = master_df_clean_ml[columns_sort]\n\nmaster_df_clean_ml.columns","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:48.187650Z","iopub.execute_input":"2022-05-19T16:09:48.188033Z","iopub.status.idle":"2022-05-19T16:09:48.205923Z","shell.execute_reply.started":"2022-05-19T16:09:48.187996Z","shell.execute_reply":"2022-05-19T16:09:48.204590Z"},"trusted":true},"execution_count":145,"outputs":[]},{"cell_type":"code","source":"master_df_clean_ml.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:48.600484Z","iopub.execute_input":"2022-05-19T16:09:48.600899Z","iopub.status.idle":"2022-05-19T16:09:48.609049Z","shell.execute_reply.started":"2022-05-19T16:09:48.600859Z","shell.execute_reply":"2022-05-19T16:09:48.607945Z"},"trusted":true},"execution_count":146,"outputs":[]},{"cell_type":"code","source":"# removing identical features having different classes, since I cannot conclude why such instance happen from the available\n# data. Such instances will degrade model performance\n\n# Mask for identical feats with different class labels\n\ndup = master_df_clean_ml[(master_df_clean_ml.duplicated(subset = master_df_clean_ml.columns[0:9], keep = False))].\\\nsort_values(by = ['age', 'enrolled_in_Bolsa_Família', 'hypertension', 'diabetes',\n       'alcoholism', 'handicap', 'sms_received', 'elapsed_days_sch', 'M'], ascending =True)\n\n# Creating a matrix to capture rows to be deleted, all duplicated rows with different class labels will \n# have a unique pattern, it's better to inspect the data outside of Jupyter Notebook to better identify the pattern.\n\ndup['unique'] = dup['attend'].shift(-1) - dup['attend']\n\ndup['next'] = dup['elapsed_days_sch'].shift(-1) - dup['elapsed_days_sch']\n\ndup['previous'] = dup['elapsed_days_sch'].shift() - dup['elapsed_days_sch']\n\ndup['prev_unique'] = dup['attend'].shift() - dup['attend']\n\ndup.to_csv('dup.csv')\n\ndup.tail(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:49.003314Z","iopub.execute_input":"2022-05-19T16:09:49.003686Z","iopub.status.idle":"2022-05-19T16:09:49.778368Z","shell.execute_reply.started":"2022-05-19T16:09:49.003655Z","shell.execute_reply":"2022-05-19T16:09:49.777170Z"},"trusted":true},"execution_count":147,"outputs":[]},{"cell_type":"markdown","source":"Indexes 24127 and 74219 are an example of duplicated exact features having different class labels. Here we have 3 instances, index 86776 as well, of which 2 are having different class labels; I'll be removing instances similar to these 2 from the data as I can't conclude on the exact reason for such difference and it will degrade model performance.","metadata":{}},{"cell_type":"code","source":"# Capturing all rows to be deleted\n\ndup = dup[(((dup.attend == 0) & (dup.unique == 1) & (dup.next == 0) & (dup.previous == 0) & (dup.prev_unique == 1)) |\n           ((dup.attend == 1) & (dup.unique == -1) & (dup.next == 0)) |\n           ((dup.attend == 0) & (dup.previous == 0) & (dup.prev_unique == 1)) | \n           ((dup.attend == 1) & (dup.unique == -1) & (dup.previous == 0)) |\n           ((dup.attend == 0) & (dup.unique == 1) & (dup.next == 0)) |\n           ((dup.attend == 0) & (dup.unique == 0) & (dup.next == 0) & (dup.previous == 0) & (dup.prev_unique == 1)) |\n           ((dup.attend == 0) & (dup.unique == 0) & (dup.next == 0) & (dup.previous == 0) & (dup.prev_unique == 0)) |\n           ((dup.attend == 0) & (dup.unique == 1) & (dup.next == 0) & (dup.previous == 0) & (dup.prev_unique == 0)))]\n\n# Removing captured rows\n\nmaster_df_clean_ml = master_df_clean_ml[~master_df_clean_ml.index.isin(dup.index)]\n\n# Separate features and labels\n\nfeatures = master_df_clean_ml.drop(columns = ['attend'])\nlabels = master_df_clean_ml['attend']\n\n# Rescaling\n\nfeatures_resc = StandardScaler().fit_transform(features)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:26:25.626392Z","iopub.execute_input":"2022-05-19T16:26:25.626813Z","iopub.status.idle":"2022-05-19T16:26:25.692468Z","shell.execute_reply.started":"2022-05-19T16:26:25.626775Z","shell.execute_reply":"2022-05-19T16:26:25.691258Z"},"trusted":true},"execution_count":284,"outputs":[]},{"cell_type":"code","source":"dup.shape, master_df_clean_ml.shape, features.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:26:45.972986Z","iopub.execute_input":"2022-05-19T16:26:45.973376Z","iopub.status.idle":"2022-05-19T16:26:45.979134Z","shell.execute_reply.started":"2022-05-19T16:26:45.973342Z","shell.execute_reply":"2022-05-19T16:26:45.978397Z"},"trusted":true},"execution_count":285,"outputs":[]},{"cell_type":"code","source":"# Outlier detection and removal, setting n_component to 1 as I want the analysis to be based on the PC explaining the \n# highest variance in the dataset\n\npca_pyod = PCA_pyod(n_components = 1).fit(features_resc)\n\nfeatures_pyod = features.copy()\n\n# identifying instances labeled as outliers for further analysis\n\nfeatures_pyod['outlier_score'] = pca_pyod.decision_scores_\n\nfeatures_pyod['outlier_labels'] = pca_pyod.labels_\n\n# analysing inliers\npyod_inliers = features_pyod[features_pyod.outlier_labels == 0]\n\npyod_inliers.describe()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:50.220348Z","iopub.execute_input":"2022-05-19T16:09:50.220708Z","iopub.status.idle":"2022-05-19T16:09:50.550493Z","shell.execute_reply.started":"2022-05-19T16:09:50.220676Z","shell.execute_reply":"2022-05-19T16:09:50.549337Z"},"trusted":true},"execution_count":150,"outputs":[]},{"cell_type":"code","source":"features.shape, pyod_inliers.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:50.615146Z","iopub.execute_input":"2022-05-19T16:09:50.615500Z","iopub.status.idle":"2022-05-19T16:09:50.622961Z","shell.execute_reply.started":"2022-05-19T16:09:50.615469Z","shell.execute_reply":"2022-05-19T16:09:50.621873Z"},"trusted":true},"execution_count":151,"outputs":[]},{"cell_type":"code","source":"features.alcoholism.value_counts()[1], features.handicap.value_counts()[1]","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:51.036302Z","iopub.execute_input":"2022-05-19T16:09:51.036812Z","iopub.status.idle":"2022-05-19T16:09:51.047991Z","shell.execute_reply.started":"2022-05-19T16:09:51.036779Z","shell.execute_reply":"2022-05-19T16:09:51.046933Z"},"trusted":true},"execution_count":152,"outputs":[]},{"cell_type":"markdown","source":"A total of 8,201 instances were identified as outliers, of which 5,093 related to alcoholism and handicap which were removed completely. I'll agree with such cleanup since these two features did not have material effect on attendance frequency as noted in my earlier EDA.\n\nI'll explore further what constitute the rest of the outliers identified apart from alcoholism and handicap features.","metadata":{}},{"cell_type":"code","source":"# analysing outliers\n\npyod_outliers = features_pyod[features_pyod.outlier_labels == 1]\n\npyod_outliers.sort_values(by = 'outlier_score').tail()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:51.444995Z","iopub.execute_input":"2022-05-19T16:09:51.445352Z","iopub.status.idle":"2022-05-19T16:09:51.467372Z","shell.execute_reply.started":"2022-05-19T16:09:51.445320Z","shell.execute_reply":"2022-05-19T16:09:51.466327Z"},"trusted":true},"execution_count":153,"outputs":[]},{"cell_type":"code","source":"# inspect records related to the ones having highest outlier scores\n\npyod_outliers[(pyod_outliers.age == 73)].sort_values(by = 'elapsed_days_sch', ascending=False)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-19T16:09:51.852591Z","iopub.execute_input":"2022-05-19T16:09:51.853116Z","iopub.status.idle":"2022-05-19T16:09:51.888420Z","shell.execute_reply.started":"2022-05-19T16:09:51.853082Z","shell.execute_reply":"2022-05-19T16:09:51.887327Z"},"trusted":true},"execution_count":154,"outputs":[]},{"cell_type":"code","source":"features[features.age == 73].plot(kind='hist', y = 'elapsed_days_sch', bins = 30);","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:52.255012Z","iopub.execute_input":"2022-05-19T16:09:52.255364Z","iopub.status.idle":"2022-05-19T16:09:52.536597Z","shell.execute_reply.started":"2022-05-19T16:09:52.255334Z","shell.execute_reply":"2022-05-19T16:09:52.535577Z"},"trusted":true},"execution_count":155,"outputs":[]},{"cell_type":"markdown","source":"Cleanup is done basically on instances having abnormal elapsed days along with being handicapped/alcoholism.","metadata":{}},{"cell_type":"code","source":"features_pyod = features_pyod[features_pyod.outlier_labels == 0]\n\nlabels_pyod = labels[labels.index.isin(features_pyod.index)]\n\nfeatures_pyod.drop(columns=['outlier_score', 'outlier_labels'], inplace = True)\n\nfeatures_pyod.describe()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:52.667957Z","iopub.execute_input":"2022-05-19T16:09:52.668317Z","iopub.status.idle":"2022-05-19T16:09:52.737132Z","shell.execute_reply.started":"2022-05-19T16:09:52.668287Z","shell.execute_reply":"2022-05-19T16:09:52.735979Z"},"trusted":true},"execution_count":156,"outputs":[]},{"cell_type":"code","source":"# Dropping unnecessary columns after outlier removal\n\nfeatures_pyod.drop(columns=['alcoholism', 'handicap'], inplace = True)\n\n# Rescaling\n\nfeatures_resc_pyod = StandardScaler().fit_transform(features_pyod)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:53.077367Z","iopub.execute_input":"2022-05-19T16:09:53.077926Z","iopub.status.idle":"2022-05-19T16:09:53.102280Z","shell.execute_reply.started":"2022-05-19T16:09:53.077890Z","shell.execute_reply":"2022-05-19T16:09:53.101204Z"},"trusted":true},"execution_count":157,"outputs":[]},{"cell_type":"code","source":"features_pyod.shape, features_resc_pyod.shape, labels_pyod.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:53.515300Z","iopub.execute_input":"2022-05-19T16:09:53.515901Z","iopub.status.idle":"2022-05-19T16:09:53.522124Z","shell.execute_reply.started":"2022-05-19T16:09:53.515864Z","shell.execute_reply":"2022-05-19T16:09:53.521252Z"},"trusted":true},"execution_count":158,"outputs":[]},{"cell_type":"code","source":"# Checking for severe multicollinearity \n\n# In regression, \"multicollinearity\" refers to predictors that are correlated with other predictors (variable/feature).  \n# Multicollinearity occurs when the model includes multiple factors that are correlated not just to the response variable,\n# but also to each other.\n\n# A VIF of 1 (the minimum possible VIF) means the tested predictor is not correlated with the other predictors. \n# The rule of thumb is that VIF shouldn’t be higher than 10.\n\nvif_data = pd.DataFrame()\nvif_data['features'] = features.columns\nvif_data['vif'] = [variance_inflation_factor(features.values, i) for i in range(len(features.columns))]\n\nvif_data","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:53.927786Z","iopub.execute_input":"2022-05-19T16:09:53.928309Z","iopub.status.idle":"2022-05-19T16:09:54.581127Z","shell.execute_reply.started":"2022-05-19T16:09:53.928277Z","shell.execute_reply":"2022-05-19T16:09:54.580007Z"},"trusted":true},"execution_count":159,"outputs":[]},{"cell_type":"code","source":"# Feature selection chi2\n\n# chi-square test measures dependence between stochastic variables (whose values depend on outcomes of a random phenomenon), \n# so using this function “weeds out” the features that are, the most likely to be independent of class and therefore \n# irrelevant for classification.\n\nfeatures_best = SelectKBest(chi2, k = 3)\nfeatures_best.fit_transform(features, labels)\ncols = features_best.get_support(indices=True)\nfeatures_best_df_chi2 = features.iloc[:,cols]\n\n\n# # printing scores, it’s better to visualize though\n# for i in range(len(features_best.scores_)):\n#     print('%s: %f' % (features.columns[i], features_best.scores_[i]))\n    \n# plot the scores\nplt.bar([features.columns[i] for i in range(len(features_best.scores_))], features_best.scores_, );\nplt.xticks(rotation = 90)\nplt.gca().spines[\"top\"].set_alpha(0.0); plt.gca().spines[\"right\"].set_alpha(0.0);","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:54.583029Z","iopub.execute_input":"2022-05-19T16:09:54.583683Z","iopub.status.idle":"2022-05-19T16:09:54.862449Z","shell.execute_reply.started":"2022-05-19T16:09:54.583635Z","shell.execute_reply":"2022-05-19T16:09:54.861264Z"},"trusted":true},"execution_count":160,"outputs":[]},{"cell_type":"code","source":"# Feature selection f_classif\n\nfeatures_best = SelectKBest(k = 3)\nfeatures_best.fit_transform(features, labels)\ncols = features_best.get_support(indices=True)\nfeatures_best_df_fcla = features.iloc[:,cols]\n\n# # for i in range(len(features_best.scores_)):\n# #     print('%s: %f' % (features.columns[i], features_best.scores_[i]))\n    \n# plot the scores\nplt.bar([features.columns[i] for i in range(len(features_best.scores_))], features_best.scores_);\nplt.xticks(rotation = 90)\nplt.gca().spines[\"top\"].set_alpha(0.0); plt.gca().spines[\"right\"].set_alpha(0.0);","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:54.864175Z","iopub.execute_input":"2022-05-19T16:09:54.864599Z","iopub.status.idle":"2022-05-19T16:09:55.098385Z","shell.execute_reply.started":"2022-05-19T16:09:54.864560Z","shell.execute_reply":"2022-05-19T16:09:55.097382Z"},"trusted":true},"execution_count":161,"outputs":[]},{"cell_type":"code","source":"# Feature selection Mutual\n\nfeatures_best = SelectKBest(mutual_info_classif, k = 3)\nfeatures_best.fit_transform(features, labels)\ncols = features_best.get_support(indices=True)\nfeatures_best_df_mutual = features.iloc[:,cols]\n\n# for i in range(len(features_best.scores_)):\n#     print('%s: %f' % (features.columns[i], features_best.scores_[i]))\n\n# plot the scores\nplt.bar([features.columns[i] for i in range(len(features_best.scores_))], features_best.scores_);\nplt.xticks(rotation = 90)\nplt.gca().spines[\"top\"].set_alpha(0.0); plt.gca().spines[\"right\"].set_alpha(0.0);","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:55.147683Z","iopub.execute_input":"2022-05-19T16:09:55.148049Z","iopub.status.idle":"2022-05-19T16:10:04.382397Z","shell.execute_reply.started":"2022-05-19T16:09:55.148016Z","shell.execute_reply":"2022-05-19T16:10:04.381348Z"},"trusted":true},"execution_count":162,"outputs":[]},{"cell_type":"code","source":"# Feature selection Lasso\n\nlasso = LassoCV().fit(features_resc, labels)\nimportance = np.abs(lasso.coef_)\nfeature_names = np.array(features.columns)\n\nplt.bar(height=importance, x=feature_names)\nplt.title(\"Feature importances via coefficients\")\nplt.xticks(rotation = 90)\nplt.gca().spines[\"top\"].set_alpha(0.0); plt.gca().spines[\"right\"].set_alpha(0.0);","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:27:04.751785Z","iopub.execute_input":"2022-05-19T16:27:04.752336Z","iopub.status.idle":"2022-05-19T16:27:05.784356Z","shell.execute_reply.started":"2022-05-19T16:27:04.752288Z","shell.execute_reply":"2022-05-19T16:27:05.783393Z"},"trusted":true},"execution_count":286,"outputs":[]},{"cell_type":"markdown","source":"As noted earlier in the EDA section, current features are not informative of output class.","metadata":{}},{"cell_type":"code","source":"# Select only top 3 features from lasso\n\nthreshold = np.sort(importance)[-3]\n\ntic = time()\n\nsfm = SelectFromModel(lasso, threshold=threshold).fit(features_resc, labels)\n\ntoc = time()\n\nprint(f\"Done in {toc - tic:.3f}s\", '\\n' ,\"Features selected by SelectFromModel: \"\n      f\"{feature_names[sfm.get_support()]}\")\n\nfeatures_best_df_sfm = features[feature_names[sfm.get_support()]]","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:27:16.344946Z","iopub.execute_input":"2022-05-19T16:27:16.345513Z","iopub.status.idle":"2022-05-19T16:27:17.104636Z","shell.execute_reply.started":"2022-05-19T16:27:16.345465Z","shell.execute_reply":"2022-05-19T16:27:17.103539Z"},"trusted":true},"execution_count":287,"outputs":[]},{"cell_type":"code","source":"# Building a dataframe for all unique best selected features across all methods\n\ncols = [features_best_df_chi2.columns, features_best_df_fcla, features_best_df_mutual.columns, features_best_df_sfm]\n\nfeats = {x for l in cols for x in l}\n\nfeatures_top = features[feats]\n\nfeatures_top","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:27:27.441758Z","iopub.execute_input":"2022-05-19T16:27:27.442402Z","iopub.status.idle":"2022-05-19T16:27:27.458632Z","shell.execute_reply.started":"2022-05-19T16:27:27.442364Z","shell.execute_reply":"2022-05-19T16:27:27.457627Z"},"trusted":true},"execution_count":288,"outputs":[]},{"cell_type":"code","source":"# Dataframe of top common feature(s) across all methods\n\nfeatures_ulti = features_top[['elapsed_days_sch']]","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:27:38.054552Z","iopub.execute_input":"2022-05-19T16:27:38.054967Z","iopub.status.idle":"2022-05-19T16:27:38.061522Z","shell.execute_reply.started":"2022-05-19T16:27:38.054931Z","shell.execute_reply":"2022-05-19T16:27:38.060296Z"},"trusted":true},"execution_count":289,"outputs":[]},{"cell_type":"code","source":"features.shape, features_pyod.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:27:45.859397Z","iopub.execute_input":"2022-05-19T16:27:45.859917Z","iopub.status.idle":"2022-05-19T16:27:45.866125Z","shell.execute_reply.started":"2022-05-19T16:27:45.859884Z","shell.execute_reply":"2022-05-19T16:27:45.864971Z"},"trusted":true},"execution_count":290,"outputs":[]},{"cell_type":"code","source":"labels.value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:28:07.322972Z","iopub.execute_input":"2022-05-19T16:28:07.323524Z","iopub.status.idle":"2022-05-19T16:28:07.331811Z","shell.execute_reply.started":"2022-05-19T16:28:07.323488Z","shell.execute_reply":"2022-05-19T16:28:07.331031Z"},"trusted":true},"execution_count":291,"outputs":[]},{"cell_type":"markdown","source":"I'll evaluate models using repeated stratified k-fold cross-validation, using different feature sets.\n\nThe k-fold cross-validation procedure provides a good general estimate of model performance that is not too optimistically biased, compared to a single train-test split. k=10 means each fold will contain about x/10 examples. Where x represents n_samples. So using the features set, each fold will have 8,204 examples.\n\nStratified means that each fold will contain the same mixture of examples by class, that is about 92% to 8% attendance and no-show cases respectively if we are using the features set.\n\nRepeated means that the evaluation process will be performed multiple times to help avoid fluke results and better capture the variance of the chosen model. I'll use 3 repeats. This means a single model will be fit and evaluated 10 * 3 or 30 times and the mean and standard deviation of these runs will be reported.","metadata":{}},{"cell_type":"code","source":"# All models\n\nmodels = [('DC', DummyClassifier(strategy= 'stratified')), ('LR', LogisticRegression(class_weight='balanced')), \n          ('NB', GaussianNB()), ('DT', tree.DecisionTreeClassifier(class_weight='balanced'))]\n\nfeats = [('features', features), ('features_rescaled', features_resc), ('Chi2', features_best_df_chi2), \n         ('f_classif', features_best_df_fcla), ('mutual_calssif', features_best_df_mutual), \n         ('lasso', features_best_df_sfm), ('top_features', features_top), ('ultimate_features', features_ulti)]\n\nresults = []\n\nnames = []\n\nmetric = 'roc_auc'\n\nfor name, model in models:\n    for feat_name, feat in feats:\n        cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n        scores = cross_val_score(model, feat, labels, scoring=metric, cv=cv, n_jobs=-1)\n        names.append(name)\n        results.append(scores)\n#         print('%s : \\n %s ---> Mean ROC_AUC ---> %.2f (Std: %f)' % (name, feat_name, scores.mean(), scores.std()))\n\nplt.figure(figsize=(10,5))\nplt.boxplot(results, labels=names, showmeans=True);\nplt.gca().spines[\"top\"].set_alpha(0.0); plt.gca().spines[\"right\"].set_alpha(0.0);","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:28:20.202648Z","iopub.execute_input":"2022-05-19T16:28:20.203157Z","iopub.status.idle":"2022-05-19T16:29:18.263407Z","shell.execute_reply.started":"2022-05-19T16:28:20.203124Z","shell.execute_reply":"2022-05-19T16:29:18.262581Z"},"trusted":true},"execution_count":292,"outputs":[]},{"cell_type":"markdown","source":"DT - Decision Tree is highest performing model using the 6th feature set which is selected using Lasso.","metadata":{}},{"cell_type":"code","source":"feats = [('features_pyod', features_pyod), ('features_rescaled_pyod', features_resc_pyod)]\n\nresults = []\n\nnames = []\n\nfor name, model in models:\n    for feat_name, feat in feats:\n        cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n        scores = cross_val_score(model, feat, labels_pyod, scoring=metric, cv=cv, n_jobs=-1)\n        names.append(name)\n        results.append(scores)\n#         print('%s : \\n %s ---> Mean ROC_AUC ---> %.2f (Std: %f)' % (name, feat_name, scores.mean(), scores.std()))\n\nplt.figure(figsize=(10,5))\nplt.boxplot(results, labels=names, showmeans=True);\nplt.gca().spines[\"top\"].set_alpha(0.0); plt.gca().spines[\"right\"].set_alpha(0.0);","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:29:18.265320Z","iopub.execute_input":"2022-05-19T16:29:18.265944Z","iopub.status.idle":"2022-05-19T16:29:34.062689Z","shell.execute_reply.started":"2022-05-19T16:29:18.265892Z","shell.execute_reply":"2022-05-19T16:29:34.061550Z"},"trusted":true},"execution_count":293,"outputs":[]},{"cell_type":"markdown","source":"LR - Logistic Regression is highest performing model using features trimmed by PyOD. I'll be comparing between these two models.","metadata":{}},{"cell_type":"markdown","source":"### Precision: \nIs the ratio of correctly predicted positive observations (TP) to the total predicted positive observations (TP/TP+FP). Taking attendance calss as an example, precision address the following question: of all patients that the classifier labeled as attended, how many actually attended? in other terms we can say that our model predicts X% of the time, a patient will attend.\n\n### Recall (Sensitivity): \nIs the ratio of correctly predicted positive observations to all observations related to the class in question. Taking attendance class as an example, Recall address the following question: of all the patients that truly attended, how many did the classifier labeled? (TP/TP+FN)\n\n### F1 score: \nIs the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. F1 is usually more useful than accuracy, especially in an uneven class distribution. Accuracy works best if false positives and false negatives have similar values. If the value of false positives and false negatives are very different, it’s better to look at both Precision and Recall. (2*(Recall * Precision) / (Recall + Precision))\n\n### Confusion Matrix:\n\nConfusion matrix does a pretty good job clearing any 'confusion' that the above terms might have caused to you. The below table shows how the results of predictions vs actuals are displayed in scikit-learn's confusion matrix, we can shuffle how results are displayed using the 'label' parameter. Note that not all confusion matrixes do display results in same setup, so know your matrix before judging a classifier.\n\nColumns represent predictions, rows are actuals.\n\n                   Predictions\n    ---------------------------------------\n    |        No         |        Yes      |\n    |------------------ |-----------------|\n    | 'True Negative'   | 'False Positive'|\n    | 'False Negative'  | 'True Positive' |\n    \n\nLet's take first confusion matrix we have below (logistic regression cross validation) as an example and link the matrix to the metric discussed above and shown in the classification report. For 'not_attend' class label:\n\n- Precision: (TP/TP+FP) = (3619 / 3619 + 14437) = .20\n- Recall: (TP/TP+FN) = (3619 / 3619 + 1131) or (3619 / 4750*) = .76\n- F1-score: (2*(Recall * Precision) / (Recall + Precision)) = (2*(.20 * .76) / (.20 + .76)) = .316 (.32 rounded)\n\n(*) can be found in 'support' column. The column 'support' displays how many object of class x were in the test set, it is not a representation of the total number of samples for this specific class unless cross validation is used.\n\nGiven the current setup, 'not_attend' class is displayed as true negative/false negative in the matrix, how does this fits into the precision/recall formula? Positive and negative in this case are generic names for the predicted classes, you can invert what you are \"trying to predict\" and thereby get two scores as displayed in the classification report; actually the classification report auto calculate these metrics regardless of your prediction goals.\n\n### ROC_AUC:\n\nI'll use ROC_AUC as a general performance measure across different models, however, final decision on which model performs best is not dependent on the ROC_AUC threshold.\n\nI'd consider a classifier to be working properly if it minimize False Positives (patients predicted to show-up but actually did not) even at the expense of high False Negatives (patients predicted to NOT show-up but actually did show-up), below is a clarification for such decision:\n\n- Normally, all patients are expected to show-up to their medical appointments.\n- Appointment setting teams need to focus efforts and resources on those who have a higher tendency of actually not showing up.\n\nSo if a classifier did predict a show-up, which is normally expected, but it turns out to be a no show; then we've excluded this case (patient) from team's scope for setting up a thorough appointment reminder process. This is not desirable.\n\nThus, I'll be giving priority for recall on the minority classes (no-shows) while minimizing false positives.\n\nMore on ROC_AUC [here](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)","metadata":{}},{"cell_type":"markdown","source":"<a id='logcross'></a>","metadata":{}},{"cell_type":"code","source":"clf = LogisticRegression(solver='liblinear', class_weight='balanced')\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\nscores = cross_val_score(clf, features_pyod, labels_pyod, scoring=metric, cv=cv, n_jobs=-1)\npredictions = cross_val_predict(clf, features_pyod, labels_pyod, cv = 10, n_jobs = -1)\n\nprint(classification_report(labels_pyod, predictions, target_names=['not_attend', 'attend']), '-'*40, '\\n',\n      confusion_matrix(labels_pyod, predictions), '\\n', '-'*40, '\\n',\n      'Mean ROC_AUC: %f (Std: %f)' % (scores.mean(), scores.std()))","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:29:42.930626Z","iopub.execute_input":"2022-05-19T16:29:42.931025Z","iopub.status.idle":"2022-05-19T16:29:46.382407Z","shell.execute_reply.started":"2022-05-19T16:29:42.930989Z","shell.execute_reply":"2022-05-19T16:29:46.381123Z"},"trusted":true},"execution_count":294,"outputs":[]},{"cell_type":"markdown","source":"<li><a href=\"#treecross\">DT cross_val</a></li>","metadata":{}},{"cell_type":"markdown","source":"<a id='logroc'></a>","metadata":{}},{"cell_type":"code","source":"labels_pyod.value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:29:56.303827Z","iopub.execute_input":"2022-05-19T16:29:56.304245Z","iopub.status.idle":"2022-05-19T16:29:56.313970Z","shell.execute_reply.started":"2022-05-19T16:29:56.304209Z","shell.execute_reply":"2022-05-19T16:29:56.313048Z"},"trusted":true},"execution_count":295,"outputs":[]},{"cell_type":"code","source":"features_train, features_test, labels_train, labels_test = train_test_split(features_pyod, labels_pyod, \n                                                                            test_size=0.2, random_state=42)\n\nclf = LogisticRegression(C = 10, class_weight = 'balanced', max_iter = 300)\n\nt0 = time()\nclf.fit(features_train, labels_train)\nprint(\"training time:\", round(time()-t0, 3), \"s\")\n\nt0 = time()\npred = clf.predict(features_test)\nprint(\"prediction time:\", round(time()-t0, 3), \"s\")\n\nprint(classification_report(labels_test, pred, target_names=['not_attend', 'attend']), '-'*40, '\\n',\n      confusion_matrix(labels_test, pred), '\\n', '-'*40)\n\nfpr, tpr, threshold = roc_curve(labels_test, pred)\n\nroc_auc = auc(fpr, tpr)\n\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1], [0,1], 'r--')\nplt.xlim([0,1])\nplt.ylim([0,1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate');","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:30:01.162834Z","iopub.execute_input":"2022-05-19T16:30:01.163588Z","iopub.status.idle":"2022-05-19T16:30:02.420796Z","shell.execute_reply.started":"2022-05-19T16:30:01.163530Z","shell.execute_reply":"2022-05-19T16:30:02.419605Z"},"trusted":true},"execution_count":296,"outputs":[]},{"cell_type":"markdown","source":"<li><a href=\"#treeroc\">DT ROC_AUC</a></li>","metadata":{}},{"cell_type":"markdown","source":"<a id='treecross'></a>","metadata":{}},{"cell_type":"code","source":"clf = tree.DecisionTreeClassifier(class_weight='balanced')\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nscores = cross_val_score(clf, features_best_df_sfm, labels, scoring=metric, cv=cv, n_jobs=-1)\npredictions = cross_val_predict(clf, features_best_df_sfm, labels, cv = 10, n_jobs = -1)\n\nprint(classification_report(labels, predictions, target_names=['not_attend', 'attend']), '-'*40, '\\n',\n      confusion_matrix(labels, predictions), '\\n', '-'*40, '\\n',\n      'Mean ROC_AUC: %f (Std: %f)' % (scores.mean(), scores.std()))","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:30:17.468790Z","iopub.execute_input":"2022-05-19T16:30:17.469186Z","iopub.status.idle":"2022-05-19T16:30:19.028411Z","shell.execute_reply.started":"2022-05-19T16:30:17.469154Z","shell.execute_reply":"2022-05-19T16:30:19.027356Z"},"trusted":true},"execution_count":297,"outputs":[]},{"cell_type":"code","source":"master_df_clean_ml.shape, features_best_df_sfm.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:30:25.393518Z","iopub.execute_input":"2022-05-19T16:30:25.393951Z","iopub.status.idle":"2022-05-19T16:30:25.400263Z","shell.execute_reply.started":"2022-05-19T16:30:25.393914Z","shell.execute_reply":"2022-05-19T16:30:25.399434Z"},"trusted":true},"execution_count":298,"outputs":[]},{"cell_type":"code","source":"# Current status of SMS reminders distribution\n\nmaster_df_clean_ml.groupby(['sms_received', 'attend']).size()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:30:30.104499Z","iopub.execute_input":"2022-05-19T16:30:30.105105Z","iopub.status.idle":"2022-05-19T16:30:30.120047Z","shell.execute_reply.started":"2022-05-19T16:30:30.105062Z","shell.execute_reply":"2022-05-19T16:30:30.118605Z"},"trusted":true},"execution_count":299,"outputs":[]},{"cell_type":"markdown","source":"If we look at SMS reminder distribution that is currently prevailing before applying the model, we can note that a total of 23,269 SMS were sent to patients of which 13% (2,921) did not attend, while 87% (20,348) of reminders were sent to patient who actually attended.\n\nIf our model was deployed the focus would shift to patients who are predicted to not attend with a total of 28,033 ('No' predictions). Thus, SMS reminders would have targeted 20% (5,554) of patients who did not actually attend their appointments while maintaining even lower reminder rate of around 80% (22,479) for patients who did.\n\nMoreover, this 20% (5,554) targeted by our model do represent 88% of all patients who actually did not attend their appointment (6,300) vs. only 46% before deploying the model.","metadata":{}},{"cell_type":"markdown","source":"<li><a href=\"#logcross\">LR cross_val</a></li>","metadata":{}},{"cell_type":"markdown","source":"<a id='treeroc'></a>","metadata":{}},{"cell_type":"code","source":"features_train, features_test, labels_train, labels_test = train_test_split(features_best_df_sfm, labels, \n                                                                            test_size=0.2, random_state=42)\n\nclf = tree.DecisionTreeClassifier(class_weight='balanced', criterion='entropy', max_depth=100, random_state=42)\n\nt0 = time()\nclf.fit(features_train, labels_train)\nprint(\"training time:\", round(time()-t0, 3), \"s\")\n\nt0 = time()\npred = clf.predict(features_test)\nprint(\"prediction time:\", round(time()-t0, 3), \"s\")\n\nprint(classification_report(labels_test, pred, target_names=['not_attend', 'attend']), '-'*40, '\\n',\n      confusion_matrix(labels_test, pred), '\\n', '-'*40)\n\nfpr, tpr, threshold = roc_curve(labels_test, pred)\n\nroc_auc = auc(fpr, tpr)\n\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1], [0,1], 'r--')\nplt.xlim([0,1])\nplt.ylim([0,1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate');","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:30:45.601282Z","iopub.execute_input":"2022-05-19T16:30:45.601818Z","iopub.status.idle":"2022-05-19T16:30:45.906967Z","shell.execute_reply.started":"2022-05-19T16:30:45.601782Z","shell.execute_reply":"2022-05-19T16:30:45.905588Z"},"trusted":true},"execution_count":300,"outputs":[]},{"cell_type":"markdown","source":"<li><a href=\"#logroc\">LR ROC_AUC</a></li>","metadata":{}},{"cell_type":"code","source":"clf = tree.DecisionTreeClassifier()\n\nover = SMOTE(random_state=42)\n\nfeat_resampled, lab_resampled = over.fit_resample(features_train, labels_train)\n\nt0 = time()\nclf.fit(feat_resampled, lab_resampled)\nprint(\"training time:\", round(time()-t0, 3), \"s\")\n\nt0 = time()\npred = clf.predict(features_test)\nprint(\"prediction time:\", round(time()-t0, 3), \"s\")\n\nprint(classification_report(labels_test, pred, target_names=['not_attend', 'attend']), '-'*40, '\\n',\n      confusion_matrix(labels_test, pred), '\\n', '-'*40)\n\nfpr, tpr, threshold = roc_curve(labels_test, pred)\nroc_auc = auc(fpr, tpr)\n\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1], [0,1], 'r--')\nplt.xlim([0,1])\nplt.ylim([0,1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate');","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:30:51.007928Z","iopub.execute_input":"2022-05-19T16:30:51.008344Z","iopub.status.idle":"2022-05-19T16:30:51.350865Z","shell.execute_reply.started":"2022-05-19T16:30:51.008307Z","shell.execute_reply":"2022-05-19T16:30:51.350029Z"},"trusted":true},"execution_count":301,"outputs":[]},{"cell_type":"code","source":"under = RandomUnderSampler(random_state=42)\n\nfeat_resampled, lab_resampled = under.fit_resample(features_train, labels_train)\n\nt0 = time()\nclf.fit(feat_resampled, lab_resampled)\nprint(\"training time:\", round(time()-t0, 3), \"s\")\n\nt0 = time()\npred = clf.predict(features_test)\nprint(\"prediction time:\", round(time()-t0, 3), \"s\")\n\nprint(classification_report(labels_test, pred, target_names=['not_attend', 'attend']), '-'*40, '\\n',\n      confusion_matrix(labels_test, pred), '\\n', '-'*40)\n\nfpr, tpr, threshold = roc_curve(labels_test, pred)\nroc_auc = auc(fpr, tpr)\n\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1], [0,1], 'r--')\nplt.xlim([0,1])\nplt.ylim([0,1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate');","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:31:07.105481Z","iopub.execute_input":"2022-05-19T16:31:07.106036Z","iopub.status.idle":"2022-05-19T16:31:07.393672Z","shell.execute_reply.started":"2022-05-19T16:31:07.106004Z","shell.execute_reply":"2022-05-19T16:31:07.392409Z"},"trusted":true},"execution_count":302,"outputs":[]},{"cell_type":"markdown","source":"Different sampling techniques did not provide material difference in model performance.","metadata":{}},{"cell_type":"markdown","source":"<a id='predconc'></a>","metadata":{}},{"cell_type":"markdown","source":"### Predictions conclusion -  Who's To Blame?\n\nWell, since I have a lot of unanswered questions highlighted in the conclusion section of the EDA and the fact that available features are not informative; I can't really decide who is to blame.\n\nAs noted earlier while analysing the records of patients having multiple appointments, where several discrepancies were noted in appointment setting and follow-up via SMS reminders, there is clearly a room for improvement in appointment administration procedures.\n\nDecision Tree classifier has the best performance minimizing false positives while maintaining high recall and AUC_ROC score over minority class (patients who do not attend). \n\nWhile this model might not be pretty given high false negative rate, it does a decent job channeling efforts towards patients who are most likely to miss their medical appointments.","metadata":{}},{"cell_type":"markdown","source":"<li><a href=\"#treecross\">DT cross_val</a></li>","metadata":{}},{"cell_type":"markdown","source":"<li><a href=\"#toc\">Table of contents</a></li>","metadata":{}},{"cell_type":"markdown","source":"<a id='analysis'></a>","metadata":{}},{"cell_type":"markdown","source":"## In-depth analysis and Feature Engineering\n\nTowards the end of EDA section we've inspected the records of multiple patients and noted some discrepancies in the timestamps of scheduling dates, I believe that this warrants deeper analysis as it can provide additional insights on reasons for 'no show' that may lead to better prediction results.\n\nWe will first split the dataset into patients who have several records of medical appointments, and those who don't. Then we will analyse a selected sample of patients with multiple appointment records and identify patterns that may be contributing to an appointment being a 'no show'. Finally we’ll engineer some features that capture these patterns and test whether they can be exploited by a classifier to improve predictions. This is actually a challenging task mainly because of the following:\n\n- Absence of sufficient details for each appointment, some of these missing details would greatly enhance our understanding of the underlying cause of 'no show' such as:\n    - Type of appointment (initial, follow-up or substitute) \n    - Medical specialties (Dermatology, Diagnostic radiology, Ophthalmology, etc.), \n    - Venue/location (medical service provider), \n    - Time of appointment (hour of the day), \n    - How is scheduling done (walk-in, call, online, etc.). \n- Class imbalance and overlap issue\n\nGiven the current data we are basically analyzing patients' 'no show' behavior based mainly on features related to timing of appointments, neighborhood and some basic knowledge of patients' characteristics; while in fact a lot of other factors/features can actually contribute to 'no show' behavior that have much more discriminatory power than what is already available.\n\nAnother complication is the imbalanced nature of the dataset, which means that +ve examples 'no show' are underrepresented and this pose a challenge as predictions tends to be skewed towards the dominant class (-ve 'show')\n\nOne important and very critical component of a machine learning project is organizing the workflow, below is an example of a generalized workflow structure:\n\n- Problem definition\n- Exploratory data analysis, this must include an in-depth look at samples of data not just plots and summaries\n- Preprocessing\n- Selecting an evaluation metric\n- Establishing a baseline\n- Modeling and evaluation\n\nWhatever the structure followed there are some key concepts that need to be considered during preprocessing of data for modeling:\n\n- Split the data into train/test sets early on\n- Perform preprocessing on both sets separately, or just ensure that training data is independent of testing/validation data\n- Cross validate\n\nNote: I'll be highlighting problematic preprocessing steps along the way and demonstrate their effect on prediction results, if any.\n\nSo, based on EDA section we can redefine the problem statement for this dataset as *'imbalanced binary calcification problem having class overlapping issue and poor discriminatory features'*\n\nOur selected evaluation metrics will be *F measure* and *Precision Recall Area Under the Curve (PR AUC)* as these two metrics are best suited for the imbalanced nature of the dataset as we will see later on.","metadata":{}},{"cell_type":"code","source":"# Copy for fresh start\nml_df = master_df_clean.sort_values(['patient_id', 'appointment_day', 'scheduling_day']).drop('day', axis = 1).copy()\n\n# Convert categorical variables to numeric\nml_df['M'] = pd.get_dummies(ml_df.gender, drop_first=True)\n\nml_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:55:50.765154Z","iopub.execute_input":"2022-05-19T17:55:50.765533Z","iopub.status.idle":"2022-05-19T17:55:51.094147Z","shell.execute_reply.started":"2022-05-19T17:55:50.765501Z","shell.execute_reply":"2022-05-19T17:55:51.093049Z"},"trusted":true},"execution_count":527,"outputs":[]},{"cell_type":"code","source":"# Class distribution\nml_df.no_show.value_counts(normalize=True).round(2)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:55:56.142137Z","iopub.execute_input":"2022-05-19T17:55:56.142653Z","iopub.status.idle":"2022-05-19T17:55:56.153604Z","shell.execute_reply.started":"2022-05-19T17:55:56.142620Z","shell.execute_reply":"2022-05-19T17:55:56.152585Z"},"trusted":true},"execution_count":528,"outputs":[]},{"cell_type":"code","source":"# Split into two datasets one for patients having several appointment records and another for ones having single appointment\n\ndf_first = ml_df[~ml_df.patient_id.duplicated(keep = False)].copy()\ndf_hist = ml_df[ml_df.patient_id.duplicated(keep = False)].copy()\n\n# Sample of patient having multiple appointments\ndf_hist[df_hist.patient_id == '95294158822869.0']","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:56:00.195040Z","iopub.execute_input":"2022-05-19T17:56:00.195441Z","iopub.status.idle":"2022-05-19T17:56:00.308669Z","shell.execute_reply.started":"2022-05-19T17:56:00.195395Z","shell.execute_reply":"2022-05-19T17:56:00.307622Z"},"trusted":true},"execution_count":529,"outputs":[]},{"cell_type":"markdown","source":"Several appointments seem to be identical/duplicate, check appointment_id # 5598835, 5598836, 5675614 and 5675615. \nWill remove such appointments but keep +ve samples 'no show' only; having samples with almost identical features but different class labels does affect prediction results negatively such samples are referred to as 'Tomek Links'.","metadata":{}},{"cell_type":"code","source":"# Dropping duplicate/identical appointments\n# We will Keep +ve samples of duplicated appointments that have different class label and\n# Only one of the other duplicated samples\n\n# Selecting columns to identify duplicates\nmask = df_hist[['patient_id','gender','scheduling_day','appointment_day','age', 'neighbourhood','no_show']].copy() \n\n# Mark all +ve samples\nmask['flag'] = mask.no_show.apply(lambda x: 1 if x ==1 else 0)\n\n# Total +ve samples per group, used to capture identical appointments with different class labels\nmask['flag_sum'] = mask.groupby(['patient_id', 'scheduling_day']).flag.transform('sum')","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:56:03.272541Z","iopub.execute_input":"2022-05-19T17:56:03.272959Z","iopub.status.idle":"2022-05-19T17:56:03.428041Z","shell.execute_reply.started":"2022-05-19T17:56:03.272918Z","shell.execute_reply":"2022-05-19T17:56:03.426968Z"},"trusted":true},"execution_count":530,"outputs":[]},{"cell_type":"code","source":"# Sample of patient records\nmask[mask.patient_id == '95294158822869.0']","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:56:06.872423Z","iopub.execute_input":"2022-05-19T17:56:06.872804Z","iopub.status.idle":"2022-05-19T17:56:06.917907Z","shell.execute_reply.started":"2022-05-19T17:56:06.872769Z","shell.execute_reply":"2022-05-19T17:56:06.916836Z"},"trusted":true},"execution_count":531,"outputs":[]},{"cell_type":"markdown","source":"Above are sample of a patient records that are duplicated based on scheduling dates, I'm ignoring other features when filtering duplicates and considering these samples to be erroneous. \n\n'flag' captures +ve samples and 'flag_sum' captures sum of +ve samples per grouped scheduling day, so if a patient had two appointments with different labels in a single scheduling slot (example index 50411 and 50415) the flag sum would be equal to 1 and this will make it easy to spot and drop the -ve sample.","metadata":{}},{"cell_type":"code","source":"# -ve samples of duplicated appointments that have different class labels\n# flag_sum >= 1 captures +ve samples per group and flag = 0 captures the -ve samples for which a +ve sample exists per the \n# same group\nidx_1 = mask[(mask.flag_sum >=1) & (mask.flag ==0)].index\n\n# duplicate observations to be dropped\nidx_2 = mask[mask.duplicated()].index\n\n# combined List of indices to be dropped\ncom_idx = idx_1.append(idx_2)\n\n# applying filter\ndf_hist = df_hist[~df_hist.index.isin(com_idx)]\n\n# Move single appointments to the other dataframe, as some patients had only two observations of which one was dropped \n# (the -ve one) so it is now considered a single appointment, we will use all these samples later on.\n_ = df_hist[~df_hist.patient_id.duplicated(keep=False)]\n\ndf_first.append(_)\n\n# Final filtered dataframe\ndf_hist = df_hist[df_hist.patient_id.duplicated(keep = False)].copy()\n\n# reset index\ndf_hist.reset_index(inplace=True, drop=True)\ndf_first.reset_index(inplace=True, drop=True)\n\n# check filter\ndf_hist[df_hist.patient_id == '95294158822869.0']","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:56:10.774675Z","iopub.execute_input":"2022-05-19T17:56:10.775063Z","iopub.status.idle":"2022-05-19T17:56:10.939069Z","shell.execute_reply.started":"2022-05-19T17:56:10.775030Z","shell.execute_reply":"2022-05-19T17:56:10.938049Z"},"trusted":true},"execution_count":532,"outputs":[]},{"cell_type":"markdown","source":"Filter is applied correctly as only +ve sample (index # 66815 above vs 50415 in mask) along with one sample per each group is shown in his records. Index has been reset so don't be confused by different index reference.","metadata":{}},{"cell_type":"code","source":"# class distribution of new dataframe\ndf_hist.no_show.value_counts(normalize=True).round(2)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-19T17:56:13.891654Z","iopub.execute_input":"2022-05-19T17:56:13.892078Z","iopub.status.idle":"2022-05-19T17:56:13.904268Z","shell.execute_reply.started":"2022-05-19T17:56:13.892044Z","shell.execute_reply":"2022-05-19T17:56:13.903043Z"},"trusted":true},"execution_count":533,"outputs":[]},{"cell_type":"markdown","source":"Class distribution is slightly different than main dataframe.","metadata":{}},{"cell_type":"markdown","source":"<a id='panalysis'></a>","metadata":{}},{"cell_type":"markdown","source":"### Analysing patient behavior\n\nWe now look at historical records of some patients and try to identify patterns contributing to 'no show' behavior then capture these patterns in new features.\n\nWe’ve given care to select samples of patients with different characteristics (age, medical condition, neighborhood, etc.) and at the same time highlight problems previously discussed being mainly poor discriminatory features and conflicting patient behavior.","metadata":{}},{"cell_type":"code","source":"df_hist[df_hist.patient_id == '22714516796615.0']","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:56:17.544372Z","iopub.execute_input":"2022-05-19T17:56:17.544731Z","iopub.status.idle":"2022-05-19T17:56:17.582574Z","shell.execute_reply.started":"2022-05-19T17:56:17.544700Z","shell.execute_reply":"2022-05-19T17:56:17.581481Z"},"trusted":true},"execution_count":534,"outputs":[]},{"cell_type":"markdown","source":"First thing to note is that current features are not informative, we can see that almost all features are somewhat similar for appointments having different class labels so they do not provide solid explanation for variation in patient behavior.\n\nThis patient had 5 appointments, only first and last one was missed. We can infer from his behavior the following:\n\n- 1st appointment was missed and by looking up the scheduling and appointment date, the day of the week was Thursday and Monday respectively.\n- On that same day (first appointment day) he had scheduled another appointment (the 2nd appointment), we may think of it as a substitute appointment for the previous missed one and he attended it. Note that day of the week for this appointment scheduling and appointment date was Monday and Wednesday respectively.\n- We may think of 3rd appointment as either:\n    - An independent appointment\n    - A follow up, given time difference (only 1 day) between previous appointment and scheduling date.\n- 4th appointment can be considered as a follow-up for the third appointment considering that it is scheduled on the same day as the previous appointment, and that previous appointment was attended\n- 5th and last appointment can be thought of similarly to the 3rd appointment which means that there are different interpretations of what we might think of what that appointment was; what is important to note is that this appointment was also a ‘no show’ and day of the week for both scheduling and appointment date was Thursday and Monday; same as with first appointment that was also a ‘no show’! Even scheduling time of both appointments are somewhat similar.  \n\nSo to summarize this subset of data, the patient had total of 5 appointments, attended 3 and missed 2. What's common between these two missed appointments is that scheduling day of week was on Thursday and appointment day of week was on Monday, scheduling time of both appointments also appears to be similar regardless of the scheduling day. \n\nWhile such observations could be confined only to this particular patient and does not repeat for others (in terms of specific dates and time), what is obvious is that timing may have an impact on ‘no show’ behavior; we've seen this earlier in the last section of EDA when we looked at records of some patients.","metadata":{}},{"cell_type":"code","source":"df_hist[df_hist.patient_id == '11163631268489.0']","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-19T17:56:21.311621Z","iopub.execute_input":"2022-05-19T17:56:21.312021Z","iopub.status.idle":"2022-05-19T17:56:21.349926Z","shell.execute_reply.started":"2022-05-19T17:56:21.311990Z","shell.execute_reply":"2022-05-19T17:56:21.348772Z"},"trusted":true},"execution_count":535,"outputs":[]},{"cell_type":"markdown","source":"- There are a total of 2 missed appointments for this patient, each were scheduled on the same day of previously attended appointment so we will consider them as a follow-up appointments as follows:\n\n    - app_id # 5690657 & 5690667 scheduled on same day of app_id 5669705, lets refer to them as c_1\n    - app_id # 5719037 & 5719053 scheduled on same day of app_id 5690657, lets refer to them as c_2\n\n- c_1’s first follow-up appointment (app # 5690657) was attended and the 2nd (app # 5690667) was not. Possible explanation is that the 2nd is separated by 1 day only from the 1st (assuming they both relate) so the patient was reluctant to go to that appointment or it may be a double booking.\n\n- c_2’s first follow-up (app 5719037) was not attended and the 2nd (app 5719053) was. Possible explanation is that the 2nd was scheduled on a day having two appointments and the patient preferred to attend both on a single day instead of multiple visits in a short time span (also, assuming a relationship between both appointments exists)\n\nWe can create couple of features to capture differences in days between current and previous appointment and to mark multiple follow-up appointments that are scheduled on different days.\n\nAnyway, we can still note the effect of time on 'no show' behavior, one of the common criteria between this patient and the previous one is the 'no show' on scheduling day '2016-05-12' other common criteria is that both are of the same age. \n\nElapsed days between scheduling and appointment days don’t seem to affect this patient's 'no show' behavior much as in fact he did attend couple of appointments with relatively high gap between both dates (last two appointments).\n\nAdditionally, both c_1 and c_2 appointments are scheduled in a very short time (almost one minute between each) and its not clear whether one should cancel the other or they are really independent from one another but given the difference in appointment dates we are going to consider each as an independent appointment.","metadata":{}},{"cell_type":"code","source":"df_hist[df_hist.patient_id == '1122443646527.0']","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:56:25.027636Z","iopub.execute_input":"2022-05-19T17:56:25.028050Z","iopub.status.idle":"2022-05-19T17:56:25.063258Z","shell.execute_reply.started":"2022-05-19T17:56:25.028013Z","shell.execute_reply":"2022-05-19T17:56:25.062386Z"},"trusted":true},"execution_count":536,"outputs":[]},{"cell_type":"markdown","source":"3 appointments were not attended of which two were scheduled in a very short time span of less than a minute difference, this is now a repeating pattern as we noted the same for the previous patient.","metadata":{}},{"cell_type":"code","source":"df_hist[df_hist.patient_id == '114656291287.0']","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:56:29.145832Z","iopub.execute_input":"2022-05-19T17:56:29.146242Z","iopub.status.idle":"2022-05-19T17:56:29.186214Z","shell.execute_reply.started":"2022-05-19T17:56:29.146204Z","shell.execute_reply":"2022-05-19T17:56:29.185440Z"},"trusted":true},"execution_count":537,"outputs":[]},{"cell_type":"markdown","source":"This patient has 0 absence frequency and his appointments include almost all examples from the patterns noted in previous patients. This makes sense as up till now we are looking at records of patients individually and it is expected that one patient behavior be different from another.","metadata":{}},{"cell_type":"code","source":"df_hist[df_hist.patient_id == '112397157856688.0']","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:56:33.282544Z","iopub.execute_input":"2022-05-19T17:56:33.282914Z","iopub.status.idle":"2022-05-19T17:56:33.318815Z","shell.execute_reply.started":"2022-05-19T17:56:33.282882Z","shell.execute_reply":"2022-05-19T17:56:33.317759Z"},"trusted":true},"execution_count":538,"outputs":[]},{"cell_type":"markdown","source":"Child (age 9) only one missed appointment (app # 5685266) for which a substitute was created (app # 5715126), possible 'no show' reason is absence of reminders 'sms received'.","metadata":{}},{"cell_type":"markdown","source":"Conclusion:\n\nWe saw that there is no unified pattern of 'no show' among patients; rather each subset of patients has its own behavior. This is understandable given the nature of the problem and medical/demographic differences between patients. What we need to think of next is how we should build our classifier, I'll assume that the current data is representative of the underlying population and appointment administration criteria (which is not known to us); this means that if 'no show' frequency for a given feature or a categorical level (example: 'MONTE BELO' of categorical feature neighborhood) is high then we expect it to hold true on new unseen data given the same conditions.\n\nWhile we still can't extract a meaningful explanation or a common pattern that clearly discriminate a 'no show' appointment among all patients, we were able to note repeating patterns among some patients. We can think of these patterns as a hint to the underlying **real** cause of 'no show'; maybe these appointments are of a common medical specialty or relate to a specific venue/physician.\n\nIn the light of what we know so far, we can think of creating some additional features such as:\n\n- Extracting date and time details for each appointment\n- Calculate number of missed appointments per a given scheduling slot (date) collectively and on neighborhood level\n- Assign a type to each appointment either follow-up / substitute\n- Calculate difference between current and previous appointment scheduling hour\n- Identify appointments scheduled on the same day\n- Identify appointments held on the same day\n- Target Encode neighborhood instead of discarding completely\n- Combine neighborhood with other numeric features as a new categorical feature.\n- Use Time stamps as a categorical feature! Think of it as a substitute for the missing details of each appointment (venue, type, medical specialization, physician, etc...), assuming that there is a relationship between time and these details.\n\nActually there are several ways to approach this particular problem, we can cluster the data and build separate model for each cluster. One drawback for this approach is that we might end up with several clusters and thus several independent models, or maybe treat it as a time series classification problem using dynamic time wrapping and K-nearest neighbor classifier but we do not have enough time data and segmentation of time data will be challenging; another approach is to make use of categorical features and combination of categorical/numeric features along with target encoding.\n\nWe will follow this last approach of categorical/numeric features and target encoding using RandomForest and CatBoost classifier, but first let’s extract these features.","metadata":{}},{"cell_type":"markdown","source":"<li><a href=\"#toc\">Table of contents</a></li>","metadata":{}},{"cell_type":"markdown","source":"<a id='fengn'></a>","metadata":{}},{"cell_type":"markdown","source":"### Feature Engineering\n\nAt this point we should start splitting the data into train/test set and then preprocess each individually to avoid data leakage. In a nutshell, leakage occurs when:\n\n- Test data is known (leaked) to the classifier during training; common example is scaling data using a scalar that is fitted on the entire dataset rather than being fitted on the training data set only. In that scenario the classifier will be trained on data that captures the distribution of values included in the testing set, so the testing set in that case lost its 'unseen' nature and became known to the classifier beforehand.\n\n- Target variable is captured by the features used in training. Example is target encoding that is done wrong; this will be discussed in details later.\n\nI'll not do the split now as I want to demonstrate the effect of leakage on prediction results; moreover, some of the preprocessing we will be doing does not pose leakage threat. However, I'll be highlighting preprocessing steps that can lead to leakage as we perform them.","metadata":{}},{"cell_type":"markdown","source":"#### Numeric Features","metadata":{}},{"cell_type":"code","source":"# Characteristics of scheduling day\ndf_hist['sch_day'] =  pd.to_datetime(df_hist.scheduling_day).dt.day\ndf_hist['sch_month'] =  pd.to_datetime(df_hist.scheduling_day).dt.month\ndf_hist['sch_hour'] =  pd.to_datetime(df_hist.scheduling_day).dt.hour\ndf_hist['day_of_week_sch'] =  pd.to_datetime(df_hist.scheduling_day).dt.dayofweek\ndf_hist['day_of_year_sch'] =  pd.to_datetime(df_hist.scheduling_day).dt.dayofyear\n\n# Characteristics of appointment day\ndf_hist['app_day'] =  pd.to_datetime(df_hist.appointment_day).dt.day\ndf_hist['app_month'] =  pd.to_datetime(df_hist.appointment_day).dt.month\ndf_hist['day_of_week_app'] =  pd.to_datetime(df_hist.appointment_day).dt.dayofweek\ndf_hist['day_of_year_app'] =  pd.to_datetime(df_hist.appointment_day).dt.dayofyear\n\n# Weekday or Weekend\n# Week starts on Monday: Monday is o - Sunday is 6, Sat and Sun are weekends in Brazil\ndf_hist['sch_on_weekend'] =  df_hist.day_of_week_sch.apply(lambda x: 1 if x > 4 else 0)\ndf_hist['app_on_weekend'] =  df_hist.day_of_week_app.apply(lambda x: 1 if x > 4 else 0)\n\ndf_hist.head(1)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:56:37.621510Z","iopub.execute_input":"2022-05-19T17:56:37.621873Z","iopub.status.idle":"2022-05-19T17:56:37.939181Z","shell.execute_reply.started":"2022-05-19T17:56:37.621827Z","shell.execute_reply":"2022-05-19T17:56:37.938246Z"},"trusted":true},"execution_count":539,"outputs":[]},{"cell_type":"code","source":"# aggregating features in order to profile the patient behavior\n\n# cumulative previous appointments count\ndf_hist['previous_app'] = df_hist.groupby(['patient_id']).cumcount()\n\n# cumulative missed appointments and absence frequency\ndf_hist['previous_missed_app'] = df_hist.groupby(['patient_id']).no_show.shift().fillna(0)\ndf_hist['previous_missed_app'] = df_hist.groupby(['patient_id']).previous_missed_app.cumsum()\ndf_hist['absence_frequency'] = (df_hist.previous_missed_app / df_hist.previous_app).round(4).fillna(0)\n\n# same day appointment, multiple appointments on same day\ndf_hist['days_prev_app'] = df_hist.sort_values(['appointment_day', 'scheduling_day']).groupby(['patient_id']).appointment_day.diff().dt.days.fillna(0)\ndf_hist['days_fol_app'] = df_hist.sort_values(['patient_id', 'appointment_day']).groupby(['patient_id']).appointment_day.diff(-1).dt.days.fillna(0)\ndf_hist['same_day_app'] = df_hist.duplicated(['patient_id','appointment_day'], keep=False).astype('int')\n\n# identify whether patient attended the previous appointment\ndf_hist['attended_previous_app'] = df_hist.groupby(['patient_id']).no_show.shift().replace({0:1, np.nan:0, 1:0}).astype('int')\n\n# Time span between scheduling day\ndf_hist['norm_day'] = df_hist.scheduling_day.dt.normalize()\n\ndf_hist['schd_days_same'] = df_hist.sort_values(['patient_id', 'scheduling_day']).groupby(['patient_id']).norm_day.diff().dt.days\\\n.apply(lambda x: 1 if x ==0 else 0).fillna(0)\n\ndf_hist['schd_days_same_'] = df_hist.sort_values(['patient_id', 'scheduling_day']).groupby(['patient_id']).norm_day.diff().shift(-1).dt.days\\\n.apply(lambda x: 1 if x ==0 else 0).fillna(0)\n\ndf_hist['schd_days_same'] = np.where((df_hist.schd_days_same == 0) & (df_hist.schd_days_same_ == 1),1,df_hist.schd_days_same)\n\n# time span between scheduling hours, measured in seconds\ndf_hist['same_day_count'] = df_hist[(df_hist.patient_id.duplicated(keep=False)) & (df_hist.schd_days_same ==1)]\\\n.sort_values(['patient_id', 'scheduling_day']).groupby(['patient_id', 'sch_day', 'sch_hour']).scheduling_day.transform('count')\n\ndata = df_hist[(df_hist.patient_id.duplicated(keep=False)) & (df_hist.schd_days_same ==1) & (df_hist.same_day_count >1)]\\\n.sort_values(['patient_id', 'scheduling_day'])\n\ndf_hist['schd_seconds_diff'] = 0\n\ndf_hist.loc[data.index, 'schd_seconds_diff'] = df_hist.sort_values(['patient_id', 'scheduling_day'])\\\n.groupby(['patient_id', 'sch_day', 'sch_hour']).scheduling_day.transform('diff').dt.seconds.bfill()\n\ndf_hist.schd_seconds_diff.fillna(0, inplace=True)\n\n# Marking appointments scheduled within 10 min on the same day\ndf_hist['tight_schedule'] = np.where((df_hist.schd_seconds_diff > 0) & (df_hist.same_day_count > 1) & (df_hist.schd_seconds_diff <= 600), 1,0)\n\ndf_hist.head(1)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:56:41.418744Z","iopub.execute_input":"2022-05-19T17:56:41.419121Z","iopub.status.idle":"2022-05-19T17:57:25.640708Z","shell.execute_reply.started":"2022-05-19T17:56:41.419088Z","shell.execute_reply":"2022-05-19T17:57:25.639684Z"},"trusted":true},"execution_count":540,"outputs":[]},{"cell_type":"code","source":"# Sample of appointments\ndf_hist[df_hist.patient_id == '111124532532143.0']","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:57:25.642291Z","iopub.execute_input":"2022-05-19T17:57:25.642595Z","iopub.status.idle":"2022-05-19T17:57:25.698828Z","shell.execute_reply.started":"2022-05-19T17:57:25.642565Z","shell.execute_reply":"2022-05-19T17:57:25.698020Z"},"trusted":true},"execution_count":541,"outputs":[]},{"cell_type":"markdown","source":"Leakage alert:\n- 'absence_frequency' feature is calculated using 'no_show' column which is the target variable we are trying to predict, however, we are excluding current appointment from calculation as we do when target encoding a categorical feature using leave one out approach, this approach can still lead to overfitting.\n\nAs we can see in the sample of records above the feature captures the historical behavior of that patient excluding his last appointment.","metadata":{}},{"cell_type":"code","source":"# Tagging appointments either being a follow-up / substitute, any better idea than the below approach?\n\n# First capture scheduling days that are same as appointment days (regardless of chronological sequence, \n# no separation yet between follow-up or substitute)\nmark = df_hist.groupby('patient_id').apply(lambda x: x.scheduling_day.dt.date.isin(x.appointment_day.dt.date).astype('int'))\n\ndf_hist['mark'] = mark.reset_index().drop(columns=['patient_id', 'level_1'])\n\n# filter out same day scheduling and appointments (appointments scheduled and attended on same date) as we want to focus\n# on current scheduling and previous appointments\ndf_hist.mark = np.where((df_hist.mark == 1) & (df_hist.elapsed_days_sch == 0),0, df_hist.mark)\n\n# capture scheduling days that are same as previous appointment days \n# we compare current scheduling to immediate previous one not all as in earlier mark\nmark_2= df_hist.groupby('patient_id').apply(lambda x: x.scheduling_day.dt.date == x.appointment_day.shift().dt.date).astype('int')\n\ndf_hist['mark_2'] = mark_2.reset_index().drop(columns=['patient_id', 'level_1'])\n\n# substitute appoints are the ones where current scheduling is same as immediate previous missed appoint day \ndf_hist['substitute_app'] = np.where((df_hist.mark == 1) & (df_hist.mark_2 == 1) & (df_hist.attended_previous_app == 0),1, 0)\n\n# follow-ups are rest of marked days\ndf_hist['follow_up_app'] = np.where((df_hist.mark == 1) & (df_hist.substitute_app == 0),1, 0)\n\n# multiple follow-up appointments scheduled at same time\ndf_hist['mult_fol_app'] = df_hist[df_hist.follow_up_app == 1].groupby(['patient_id', 'norm_day']).follow_up_app.transform('count')\\\n.apply(lambda x: 1 if x >1 else 0)\n\ndf_hist.mult_fol_app.fillna(0, inplace=True)\n\ndf_hist.head(1)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:57:25.700353Z","iopub.execute_input":"2022-05-19T17:57:25.700808Z","iopub.status.idle":"2022-05-19T18:04:19.312711Z","shell.execute_reply.started":"2022-05-19T17:57:25.700776Z","shell.execute_reply":"2022-05-19T18:04:19.311658Z"},"trusted":true},"execution_count":542,"outputs":[]},{"cell_type":"code","source":"# average elapsed days per neighborhood\ndf_hist['avg_elaps_day_neighbourhood'] = df_hist.groupby('neighbourhood').elapsed_days_sch.transform('mean').round()\n\n# difference between elapsed day and average per neighborhood\ndf_hist['elday_navg_diff'] = df_hist.elapsed_days_sch - df_hist.avg_elaps_day_neighbourhood\n\ndf_hist.head(1)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:04:19.314487Z","iopub.execute_input":"2022-05-19T18:04:19.314795Z","iopub.status.idle":"2022-05-19T18:04:19.359120Z","shell.execute_reply.started":"2022-05-19T18:04:19.314762Z","shell.execute_reply":"2022-05-19T18:04:19.358128Z"},"trusted":true},"execution_count":543,"outputs":[]},{"cell_type":"code","source":"# historical missed appointments per scheduling hour of the day and per scheduling hour per neighborhood\n# to capture if a specific scheduling slot experience higher no_show frequency than another\n\ndf_hist['msd_app_sch_slot_nb'] = df_hist.sort_values(['appointment_day','scheduling_day']).groupby(['neighbourhood', 'sch_day', 'sch_month', 'sch_hour'])\\\n.no_show.shift().fillna(0)\n\ndf_hist['msd_app_sch_slot_nb'] = df_hist.sort_values(['appointment_day','scheduling_day']).groupby(['neighbourhood', 'sch_day', 'sch_month', 'sch_hour'])\\\n.msd_app_sch_slot_nb.cumsum().fillna(0)\n\n# historical missed appointments per scheduling hour of the day\ndf_hist['msd_app_sch_slot'] = df_hist.sort_values(['appointment_day', 'scheduling_day']).groupby(['sch_day', 'sch_month', 'sch_hour'])\\\n.no_show.shift().fillna(0)\n\ndf_hist['msd_app_sch_slot'] = df_hist.sort_values(['appointment_day', 'scheduling_day']).groupby(['sch_day', 'sch_month', 'sch_hour'])\\\n.msd_app_sch_slot.cumsum().fillna(0)\n\n# drop temp columns\ndf_hist.drop(columns = ['norm_day', 'schd_days_same_', 'mark', 'mark_2', 'same_day_count'], inplace=True)\n\n# Sort columns\nno_show_idx = df_hist.columns.get_loc(\"no_show\")\n\ncolumns_list = df_hist.columns.tolist()\n\ncolumns_sort = columns_list[0:no_show_idx] + columns_list[no_show_idx+1:] + columns_list[no_show_idx : no_show_idx+1]\n\ndf_hist = df_hist[columns_sort]\n\ndf_hist.head(1)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:04:19.360716Z","iopub.execute_input":"2022-05-19T18:04:19.361135Z","iopub.status.idle":"2022-05-19T18:04:19.742174Z","shell.execute_reply.started":"2022-05-19T18:04:19.361100Z","shell.execute_reply":"2022-05-19T18:04:19.741125Z"},"trusted":true},"execution_count":544,"outputs":[]},{"cell_type":"markdown","source":"Leakage alert:\n- both features(msd_app_sch_slot' and 'msd_app_sch_slot_nb') are created using same method as 'absence_frequency' discussed earlier.","metadata":{}},{"cell_type":"code","source":"# Remove long elapsed days that belong to -ve class\ndf_hist = df_hist[~df_hist.index.isin(df_hist[(df_hist.elapsed_days_sch >90) & (df_hist.no_show ==0)].index)]\n\ndf_hist.reset_index(inplace=True, drop=True)\n\ndf_hist","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-19T18:04:19.743458Z","iopub.execute_input":"2022-05-19T18:04:19.743738Z","iopub.status.idle":"2022-05-19T18:04:19.823686Z","shell.execute_reply.started":"2022-05-19T18:04:19.743711Z","shell.execute_reply":"2022-05-19T18:04:19.822643Z"},"trusted":true},"execution_count":545,"outputs":[]},{"cell_type":"markdown","source":"<li><a href=\"#toc\">Table of contents</a></li>","metadata":{}},{"cell_type":"markdown","source":"<a id='bline'></a>","metadata":{}},{"cell_type":"markdown","source":"#### Establishing baseline and analysing results\n\nNow that we've extracted some features, it’s time to establish a baseline before going any further. Baseline is usually established before feature engineering but I want to have a measure of performance improvement before and after categorical feature extraction so we will establish it now.\n\nWe will be using RandomForest classifier and f1 score as a performance improvement metric while keeping an eye on recall, I'm seeking a balanced classification performance with high recall without significant loss in precession rather than a skewed one as in the previous attempt where the classifier generated large amount of false positive predictions.","metadata":{}},{"cell_type":"code","source":"# Model building and evaluation\nclass Results:\n    def __init__(self, model, pred_p, pred_c, pr_precision, pr_recall, pr_thresh, pr_f1, pr_auc, fpr, tpr, roc_thresh, roc_auc):\n        self.model = model\n        self.pred_p = pred_p\n        self.pred_c = pred_c\n        self.pr_precision = pr_precision\n        self.pr_recall = pr_recall\n        self.pr_thresh = pr_thresh\n        self.pr_f1 = pr_f1\n        self.pr_auc = pr_auc\n        self.fpr = fpr\n        self.tpr = tpr\n        self.roc_thresh = roc_thresh\n        self.roc_auc = roc_auc\n        \ndef train_eval(model, features_train, labels_train, features_test, labels_test):\n\n    ''' \n    Train model and display several evaluation metrics including:\n    \n    1 - Area Under the Curve - Precision Recall Curve (PR)\n    2 - Area Under the Curve - Receiver Operating Characteristics (ROC)\n    3 - F measure\n    4 - Thresholds for both curves\n    5 - Precision\n    6 - Recall\n    7 - True Positive Rate\n    8 - False Positive Rate \n    \n    Both raw predictions and probabilities are also calculated\n    '''\n\n    t0 = time()\n    model.fit(features_train, labels_train)\n    print(\"training time:\", round((time()-t0) / 60, 2), \"m\", '\\n', '-' * 40)\n\n    # Predict \n\n    t0 = time()\n    pred_p = model.predict_proba(features_test)\n    pred_c = model.predict(features_test)\n    print(\"prediction time\", round((time()-t0) / 60, 3), \"m\", '\\n', '-' * 40)\n\n    # Hold predictions of +ve class\n    pred_p = pred_p[:,1]\n\n    # Display results\n\n    pr_precision, pr_recall, pr_thresh = precision_recall_curve(labels_test, pred_p)\n    pr_f1, pr_auc = f1_score(labels_test, pred_c), auc(pr_recall, pr_precision)\n\n    fpr, tpr, roc_thresh = roc_curve(labels_test, pred_p)\n    roc_auc = auc(fpr, tpr)\n\n    print('PR_AUC: ', pr_auc.round(4), '\\n', '-' * 40, '\\n', 'F1 score: ', pr_f1.round(4), '\\n', '-' * 40, '\\n', 'ROC_AUC: ', roc_auc.round(4), '\\n', '-' * 40, '\\n',\n          classification_report(labels_test, pred_c, target_names=['show', 'no_show']), '-'*40, '\\n', confusion_matrix(labels_test,  pred_c))\n\n    return Results(model, pred_p, pred_c, pr_precision, pr_recall, pr_thresh, pr_f1, pr_auc, fpr, tpr, roc_thresh, roc_auc)\n\n# Plotting feature importance\ndef rf_plot(model, features):\n    \n    ''' Plot feature importance of RandomForest Classifier'''\n    \n    fig  = plt.subplots(figsize=(10, 10))\n    plt.tight_layout()\n\n    feature_importance = model.feature_importances_\n    sorted_idx = np.argsort(feature_importance)\n    pos = np.arange(sorted_idx.shape[0]) + 0.5\n    plt.barh(pos, feature_importance[sorted_idx], align=\"center\")\n    plt.yticks(pos, np.array(features.columns)[sorted_idx])\n    plt.title(\"Feature Importance RF - Medical appointments\")\n    plt.gca().spines[\"top\"].set_alpha(0.0); plt.gca().spines[\"right\"].set_alpha(0.0);","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:04:19.825061Z","iopub.execute_input":"2022-05-19T18:04:19.825363Z","iopub.status.idle":"2022-05-19T18:04:19.842634Z","shell.execute_reply.started":"2022-05-19T18:04:19.825332Z","shell.execute_reply":"2022-05-19T18:04:19.841498Z"},"trusted":true},"execution_count":546,"outputs":[]},{"cell_type":"code","source":"# Establishing a baseline\n\n# convert non numeric columns to string\ndf_hist.scheduling_day, df_hist.appointment_day = df_hist.scheduling_day.astype('str'), df_hist.appointment_day.astype('str')\n\n# separate categorical and numeric columns\ncat_cols = df_hist.select_dtypes('object').columns\nnum_cols = df_hist.columns[~df_hist.columns.isin(cat_cols)]\n\n# unifying numeric dtypes\ndf_hist[num_cols[:-1]] = df_hist[num_cols[:-1]].astype(float, errors='ignore')\n\n# split data\nfeatures = df_hist[num_cols].drop('no_show', axis=1)\nlabels = df_hist[num_cols].no_show\n\nX_train, X_test, y_train, y_test = train_test_split(features, labels, stratify = labels, test_size=.2, random_state=42)\n\n# Basline Model\nrf_results = train_eval(RandomForestClassifier(class_weight='balanced', random_state = 42), X_train, y_train, X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:04:19.845989Z","iopub.execute_input":"2022-05-19T18:04:19.846314Z","iopub.status.idle":"2022-05-19T18:04:32.831598Z","shell.execute_reply.started":"2022-05-19T18:04:19.846284Z","shell.execute_reply":"2022-05-19T18:04:32.830574Z"},"trusted":true},"execution_count":547,"outputs":[]},{"cell_type":"markdown","source":"The classifier did not perform well. We are particularly interested in f1-score and PR_AUC metrics as the dataset is imbalanced, both metrics displays low values as PR_AUC is below 50% and f1-score is almost 20%; One thing to observe that some metrics can be misleading when evaluating model performance on imbalanced datasets such as accuracy and ROC_AUC as both are showing high values 80% and 76% respectively but are shadowed by classification results of the -ve class with larger sample size.  \n\nThe classifier is having poor recall on the +ve class (no_show) as only 355 out of 2928 no_show samples were correctly classified, one reason is that +ve class is underrepresented another is that current features does not provide decent class discrimination as we've discussed earlier. \n\nNote that we've partially addressed class imbalance issue on classifier level by setting class_weight hyperparameter to 'balanced', this will force the classifier pay more attention to +ve class.\n\nNow that we have a baseline to measure against, let's look at feature importance.","metadata":{}},{"cell_type":"code","source":"# plotting feature importance\nrf_plot(rf_results.model, X_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:04:32.834093Z","iopub.execute_input":"2022-05-19T18:04:32.834438Z","iopub.status.idle":"2022-05-19T18:04:33.420831Z","shell.execute_reply.started":"2022-05-19T18:04:32.834405Z","shell.execute_reply":"2022-05-19T18:04:33.419617Z"},"trusted":true},"execution_count":548,"outputs":[]},{"cell_type":"markdown","source":"Some of the features we've just created are at the top of feature importance. key observation from the above plot is that features that capture time of appointments and related variations are among the top contributors to prediction results.\n\nBefore moving on to Categorical feature engineering lets visualize class distribution using all current features","metadata":{}},{"cell_type":"code","source":"def dr_plot(df, df_idx, labels, pca = True, undersample = False, undersample_method = None, plot_only = True):\n\n    '''\n    Reduce dimensionality to visualize data using either PCA or T-SNE, will construct a dataframe of reduced dimensions \n    indexed same as main dataframe for ease of access. Metric used for T-SNE is cosine.\n\n    df: features scaled\n    df_idx: main dataframe to capture index before scaling\n    labels: target/class\n    method: PCA or T-SNE, default PCA\n    undersample_method: fitted undersampler, used to capture index of kept datapoints\n    plot_only: True is for visualization only, else returns both fitted pca and constructed dataframe if pca; and dataframe only for tsne\n\n    '''\n\n    if pca:\n\n        pca = PCA(n_components=2, random_state = 45) # create a PCA object\n        pca.fit(df) # do the math\n        pca_data = pca.transform(df) # get PCA coordinates for scaled_data\n\n        fig, axes = plt.subplots(figsize=(30, 15))   \n\n        if undersample:\n            pca_df = pd.DataFrame(pca_data, columns=['PC1', 'PC2'], index = df_idx.iloc[undersample_method.sample_indices_].index)\n            us_method_name = str(undersample_method).split('(')[0]\n            print(f'Explained Variance Ratio for PC1 and PC2 respectively - {us_method_name}', np.round(pca.explained_variance_ratio_, 2)) \n\n            _ = plt.scatter(pca_df.PC1, pca_df.PC2, c =  labels, marker='x', cmap = 'RdYlBu')\n\n            plt.title(f'Medical Appointments PCA - {us_method_name}')\n\n        else:\n            pca_df = pd.DataFrame(pca_data, columns=['PC1', 'PC2'], index = df_idx.index)\n            print('Explained Variance Ratio for PC1 and PC2 respectively', np.round(pca.explained_variance_ratio_, 2))\n\n            _ = plt.scatter(pca_df.PC1, pca_df.PC2, c =  labels, marker='x', cmap = 'RdYlBu')\n\n            plt.title(f'Medical Appointments PCA')\n\n        plt.legend(handles = _.legend_elements()[0], labels = ['Show', 'No_show'])\n        plt.gca().spines[\"top\"].set_alpha(0.0); plt.gca().spines[\"right\"].set_alpha(0.0);\n\n        if not plot_only:\n            return [pca, pca_df]\n\n    else:\n        tsne = TSNE(n_components = 2, init='pca', learning_rate='auto', metric='cosine', square_distances=True, random_state = 45)\n        tsne_data = tsne.fit_transform(df)\n\n        fig, axes = plt.subplots(figsize=(30, 15))   \n\n        if undersample:\n            tsne_df = pd.DataFrame(tsne_data, columns=['x', 'y'], index = df_idx.iloc[undersample_method.sample_indices_].index)\n            us_method_name = str(undersample_method).split('(')[0]\n\n            _ = plt.scatter(tsne_df.x, tsne.y, c =  labels, marker='x', cmap = 'RdYlBu')\n\n            plt.title(f'Medical Appointments T-SNE - {us_method_name}')\n\n        else:\n            tsne_df = pd.DataFrame(tsne_data, columns=['x', 'y'], index = df_idx.index)\n\n            _ = plt.scatter(tsne_df.x, tsne_df.y, c =  labels, marker='x', cmap = 'RdYlBu')\n\n            plt.title(f'Medical Appointments T-SNE')\n\n        plt.legend(handles = _.legend_elements()[0], labels = ['Show', 'No_show'])\n        plt.gca().spines[\"top\"].set_alpha(0.0); plt.gca().spines[\"right\"].set_alpha(0.0);\n\n        if not plot_only:\n            return tsne_df","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:04:33.422323Z","iopub.execute_input":"2022-05-19T18:04:33.422640Z","iopub.status.idle":"2022-05-19T18:04:33.442139Z","shell.execute_reply.started":"2022-05-19T18:04:33.422607Z","shell.execute_reply":"2022-05-19T18:04:33.440932Z"},"trusted":true},"execution_count":549,"outputs":[]},{"cell_type":"code","source":"# Visualization using PCA for dimensionality reduction\n\ndata = MaxAbsScaler().fit_transform(df_hist[num_cols[:-1]])\n\nmask_pca = dr_plot(data, df_hist, df_hist.no_show, plot_only = False)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:04:33.443792Z","iopub.execute_input":"2022-05-19T18:04:33.444268Z","iopub.status.idle":"2022-05-19T18:04:36.005754Z","shell.execute_reply.started":"2022-05-19T18:04:33.444219Z","shell.execute_reply":"2022-05-19T18:04:36.004621Z"},"trusted":true},"execution_count":550,"outputs":[]},{"cell_type":"markdown","source":"We are looking at all appointments using two Principle Components (PC), PCA is a dimensionality reduction and feature engineering method where we can reduce all features in the dataset down to a selected number of principle components for visualization or even use these new components as a training data instead of original features.\n\nImportant thing to note is that PCA is an unsupervised linear dimensionality reduction technique; the principal components produced are mixture of original features. Each component does explain some of the variation in the data and usually the first two components are the top contributors but this depends on the data and nature of linear relationship among features/variables.\n\nSo what we see on the plot is how close/far each appointment is from one another based on these two principle components (collection of features) which do not provide significant explanation for variation within the data (only 26% explained variance ration). We are using MaxAbs scaler to preserve the sparsity of the data and it is apparent that there are some regions (appointments) having a clear separation of +ve and -ve class while others are not (class overlap); check cords x: 0.6 and y: 1.2.\n\nPCA may not be an idle 2D visualization technique for our data given the low explained variance ratio of the first two components but it is enough to serve the purpose of having a quick overview of the dataset.","metadata":{}},{"cell_type":"code","source":"# visualize the loading scores\n\nplt.matshow(mask_pca[0].components_, cmap='RdYlBu')\nplt.yticks([0, 1], [\"PC1\", \"PC2\"])\nplt.colorbar()\nplt.grid(None)\nplt.xticks(range(len(df_hist[num_cols[:-1]].columns)), df_hist[num_cols[:-1]].columns, rotation=90, ha='center')\nplt.xlabel(\"Feature\")\nplt.ylabel(\"Principal components\");","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:04:36.007453Z","iopub.execute_input":"2022-05-19T18:04:36.007823Z","iopub.status.idle":"2022-05-19T18:04:36.774924Z","shell.execute_reply.started":"2022-05-19T18:04:36.007786Z","shell.execute_reply":"2022-05-19T18:04:36.773854Z"},"trusted":true},"execution_count":551,"outputs":[]},{"cell_type":"markdown","source":"One way to understand PCA plot is to visualize the contribution of each feature to the components being analysed, PC1 and PC2. Contribution is denoted by how correlated each feature to the component, for example appointments having higher values of PC2 axis (y-axis) represents appointments of male patients that did not receive sms or attend/have a previous appointments while having high absence score and being scheduled in the same day with narrow scheduling window.\n\nWhile these features combined could provide good class discrimination among some of the appointments, this does not hold true for the majority of appointments as we noted earlier inspecting the historical records of some patients; what works for some patients doesn't work for others.","metadata":{}},{"cell_type":"code","source":"# Zoom in a dense no_show area\npca_df = mask_pca[1]\nsubset = pca_df[(pca_df.PC1 > .6) & (pca_df.PC2 > 1.2)]\nlabels_ = df_hist[df_hist.index.isin(subset.index)].no_show\n\n# plotly for indexing and inspecting specific points\nfig = subplots.make_subplots(rows=1, cols=1)\n\nt = go.Scatter(x=subset.PC1, y=subset.PC2, mode='markers', showlegend=False, marker=dict(size=10, color=labels_, \n                 colorscale='RdYlBu', line=dict(color='black', width=1)),\n                 hovertemplate = '<br>X</b>: %{x}' + '<br>Y</b>: %{y}<br>'+ '<br>Index: %{text}', text=subset.index)\n                  \nfig.append_trace(t, 1, 1)\nfig.update_layout(autosize=False, width=1200, height=700)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:04:36.776602Z","iopub.execute_input":"2022-05-19T18:04:36.777318Z","iopub.status.idle":"2022-05-19T18:04:36.840714Z","shell.execute_reply.started":"2022-05-19T18:04:36.777270Z","shell.execute_reply":"2022-05-19T18:04:36.839748Z"},"trusted":true},"execution_count":552,"outputs":[]},{"cell_type":"markdown","source":"We are now looking at a subset from the PCA plot having dense 'no show' appointments, we can observe three clusters of appointments each having its own characteristics depending on where it lies on x and y axis, We will select two of the adjacent appointments (x: 0.85, y: 1.44) and closely inspect full appointment details trying to figure out ways to better separate them but first let’s look and the characteristics of this subset.","metadata":{}},{"cell_type":"code","source":"# full appointment details of a selected subset\ndetailed_subset = df_hist[df_hist.index.isin(subset.index)]\n\ndetailed_subset.describe()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:04:36.842353Z","iopub.execute_input":"2022-05-19T18:04:36.843026Z","iopub.status.idle":"2022-05-19T18:04:36.973677Z","shell.execute_reply.started":"2022-05-19T18:04:36.842977Z","shell.execute_reply":"2022-05-19T18:04:36.972553Z"},"trusted":true},"execution_count":553,"outputs":[]},{"cell_type":"markdown","source":"The descriptive summary corroborates with our earlier interpretation when we visualized feature contribution to each principal component. 'No show' mean for these appointments is 60% and they are mainly related to male patients (76%) that did not receive sms or attend/have a previous appointments while having high absence frequency (68%), additionally most appointments are being scheduled on the same day with narrow scheduling window (couple of seconds between each scheduling time) and are scheduled to take place on days having multiple appointments (several appointments on same day).","metadata":{}},{"cell_type":"code","source":"detailed_subset.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:04:36.975050Z","iopub.execute_input":"2022-05-19T18:04:36.975372Z","iopub.status.idle":"2022-05-19T18:04:37.034929Z","shell.execute_reply.started":"2022-05-19T18:04:36.975340Z","shell.execute_reply":"2022-05-19T18:04:37.033897Z"},"trusted":true},"execution_count":554,"outputs":[]},{"cell_type":"code","source":"# full records of patients having appointments in the selected subset\ndf_hist[df_hist.patient_id.str.contains('11474894783.0|118375575136787.0')]","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-19T18:04:37.038010Z","iopub.execute_input":"2022-05-19T18:04:37.038331Z","iopub.status.idle":"2022-05-19T18:04:37.169254Z","shell.execute_reply.started":"2022-05-19T18:04:37.038299Z","shell.execute_reply":"2022-05-19T18:04:37.168101Z"},"trusted":true},"execution_count":555,"outputs":[]},{"cell_type":"markdown","source":"100% 'no show', almost all appointments for these two patients are being scheduled on the same day with narrow scheduling window (split seconds between each scheduling time) and are multiple appointments on same appointment day.","metadata":{}},{"cell_type":"code","source":"# full appointment details of selected adjacent points having different class labels (x: 0.85, y: 1.44)\ndetailed_subset.loc[[57346, 29863]]","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:04:37.170623Z","iopub.execute_input":"2022-05-19T18:04:37.170935Z","iopub.status.idle":"2022-05-19T18:04:37.223223Z","shell.execute_reply.started":"2022-05-19T18:04:37.170893Z","shell.execute_reply":"2022-05-19T18:04:37.221975Z"},"trusted":true},"execution_count":556,"outputs":[]},{"cell_type":"markdown","source":"We can note that most of numeric features are similar, it is the time, neighborhood and age related features that make clear distinction between the two.","metadata":{}},{"cell_type":"markdown","source":"<li><a href=\"#toc\">Table of contents</a></li>","metadata":{}},{"cell_type":"markdown","source":"<a id='fengc'></a>","metadata":{}},{"cell_type":"markdown","source":"#### Categorical Features\n\nPreviously I've discarded neighborhood completely, now will try to make use of it and check classification performance. From now on we will be creating new features then evaluate model performance right after and compare results to the baseline model.","metadata":{}},{"cell_type":"code","source":"# check cardinality of neighborhood feature\n\nprint(f'Cardinality of Neighbourhood feature is: {df_hist.neighbourhood.nunique()}')","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:04:37.224636Z","iopub.execute_input":"2022-05-19T18:04:37.224981Z","iopub.status.idle":"2022-05-19T18:04:37.245651Z","shell.execute_reply.started":"2022-05-19T18:04:37.224948Z","shell.execute_reply":"2022-05-19T18:04:37.244662Z"},"trusted":true},"execution_count":557,"outputs":[]},{"cell_type":"code","source":"# least represented neighborhood\ndf_hist.groupby('neighbourhood').no_show.mean().sort_values().head(1), df_hist.groupby('neighbourhood').no_show.count().sort_values().head(1)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:04:37.246910Z","iopub.execute_input":"2022-05-19T18:04:37.247192Z","iopub.status.idle":"2022-05-19T18:04:37.274739Z","shell.execute_reply.started":"2022-05-19T18:04:37.247164Z","shell.execute_reply":"2022-05-19T18:04:37.273922Z"},"trusted":true},"execution_count":558,"outputs":[]},{"cell_type":"markdown","source":"Cardinality refers to the number of unique records (levels) in a given categorical feature; neighborhood feature does have 79 unique levels.\n\nThere are several ways to handle categorical (cat) features; all have the same basic principle that is seeking a unique numeric representation for each category. Below are some of the popular ways for handling such features:\n\n- A) Dropping: same as I did in first attempt. This approach is good if we are sure that cat features do not bring any value to the classification results, like the gender feature in this dataset.\n- B) One-hot Encoding (ohe): will produce a sparse matrix for each unique category. For neighborhood feature we would generate 79 additional columns each representing a single level (neighborhood).\n- C) Label Encoding: substituting each level with a unique value (number). For neighborhood feature we would represent each neighborhood by a unique number ranging from 0-78, no additional columns are going to be generated as the numbers substitutes the string values in the same columns.\n- D) Target Encoding: very powerful yet very dangerous encoding technique as it can lead to serious data leakage and misleading optimistic performance. Each category (level) is substituted by the frequency of occurrence of the +ve class in that level, the frequency can take any form such as mean, count, etc..\n    - Example: we calculate frequency of 'no show' appointments per each level neighborhood feature. I'll be using the mean as an indication of frequency which is simply the +ve class samples / all samples per level.\n \nWhy it can lead to misleading results?\n\n- In the example above, neighborhood 'AEROPORTO' have a mean frequency of 0 calculated based on 2 samples only. If we encode this neighborhood as it is then we would be replacing it with the target variable we are trying to predict as the 2 appointments were both attended (0 in target value); another important thing is that we are not confident enough in the calculated statistic(mean) as it is based on 2 samples only.\n\nHow to deal with these issues?\n\n- One approach is to introduce noise, this can take many forms but I'll be using the global mean of 'no show' as a substitute for the calculated mean per each level.","metadata":{}},{"cell_type":"code","source":"# likelihood of absence per neighborhood\ndf_hist.groupby('neighbourhood').no_show.mean().sort_values().head(50)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:04:37.275689Z","iopub.execute_input":"2022-05-19T18:04:37.276002Z","iopub.status.idle":"2022-05-19T18:04:37.296334Z","shell.execute_reply.started":"2022-05-19T18:04:37.275971Z","shell.execute_reply":"2022-05-19T18:04:37.294983Z"},"trusted":true},"execution_count":559,"outputs":[]},{"cell_type":"code","source":"# checking distribution of no_show across neighborhoods\ndf_hist.groupby('neighbourhood').no_show.mean().plot(kind='hist', bins =20);","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:04:37.298027Z","iopub.execute_input":"2022-05-19T18:04:37.298497Z","iopub.status.idle":"2022-05-19T18:04:37.523707Z","shell.execute_reply.started":"2022-05-19T18:04:37.298448Z","shell.execute_reply":"2022-05-19T18:04:37.522952Z"},"trusted":true},"execution_count":560,"outputs":[]},{"cell_type":"code","source":"# sample of appointments per selected neighborhoods\ndf_hist[(df_hist.neighbourhood == 'REDENÇÃO')].no_show.count(), df_hist[(df_hist.neighbourhood == 'JARDIM CAMBURI')].no_show.count()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:04:37.524732Z","iopub.execute_input":"2022-05-19T18:04:37.525148Z","iopub.status.idle":"2022-05-19T18:04:37.559719Z","shell.execute_reply.started":"2022-05-19T18:04:37.525118Z","shell.execute_reply":"2022-05-19T18:04:37.558927Z"},"trusted":true},"execution_count":561,"outputs":[]},{"cell_type":"markdown","source":"One way to target encode cat feature is to compute the probability of 'no show' at each level (per each neighborhood) and use it as a 'score' indicating 'no show' per neighborhood.\n\nFew problems need to be addressed if we are to follow this approach:\n\nA) With few 'no show' samples and imbalanced nature of the dataset, some neighborhoods are going to have 'no show' probability of zero (example: AEROPORTO) making the distribution of the new feature skewed towards zero. Luckily we do not have many examples of such problem in this feature.\n\nB) Calculated probability could be the same for neighborhoods regardless samples available. For example, 'no show' probability for the two neighborhoods 'REDENÇÃO' and 'JARDIM CAMBURI' is approximately 19%, however, it is calculated based on a sample of 1k and 5k appointments respectively for each neighborhood. Therefore, we are more confident in the 'no show' likelihood statistic for 'JARDIM CAMBURI' than the other neighborhood.\n\nTo solve these issues, we're going to combine a priori of 'no show' with the calculated 'no show' probability per neighborhood. The idea is that when there’s not much information (neighborhoods with few samples/appointments) we want to encode these neighborhoods as closely as possible to the mean probability of 'no show' across all neighborhoods (global mean). On the other hand, when we have enough information (more samples/appointments) we want to consider the 'no show' probability per that neighborhood as it is. In order to achieve this, we are going to assign a weight to each calculated probability so that it scales with the available samples from which the statistic is calculated, this weight will pull encoded statistic towards global one in case of few samples and vise versa.","metadata":{}},{"cell_type":"code","source":"# Target encoding cat variables\n\ndef target_enc_scoring(df, feat, label, log_trans = False, with_weight = True):\n    ''' Calculate scores for each level of a categorical feature and adjust the score by sample weight'''\n    \n    # incase single column is passed to encode instead of several ones\n    if isinstance(feat, str):\n        feat = [feat]\n    else:\n        feat = feat\n    \n    # Compute the global mean\n    priori = df[label].mean()\n\n    # total samples\n    tot_samples = df.shape[0]\n    \n    for f in feat:\n\n        # Compute frequency, mean and weight of each level\n        agg = df.groupby(f)[label].agg(['count', 'mean'])\n        \n        # no show likelihood\n        means = agg['mean']\n        \n        # sample count per each level\n        if log_trans:\n            counts = np.log(agg['count'])\n        else:\n            counts = agg['count']\n        \n        weight = counts / tot_samples\n            \n        # scale weights to reduce influence of the priori\n        # if we do not scale, the priori will always have higher influence on the calculated statistic\n        weight = MinMaxScaler().fit_transform(weight.values.reshape(-1,1))\n\n        # Compute the scoring function\n        scoring = (weight.ravel() * means) + ((1-weight.ravel()) * priori)\n\n        # add calculated score to dataframe\n        if with_weight:    \n            df[f'{f}_score'] = df[f].map(scoring)\n        \n        # frequency without weight - if you are feeling adventurous\n        else:\n            df[f'{f}_score'] = df.groupby(f)[label].transform('mean')","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:04:37.565636Z","iopub.execute_input":"2022-05-19T18:04:37.565990Z","iopub.status.idle":"2022-05-19T18:04:37.576053Z","shell.execute_reply.started":"2022-05-19T18:04:37.565958Z","shell.execute_reply":"2022-05-19T18:04:37.574827Z"},"trusted":true},"execution_count":562,"outputs":[]},{"cell_type":"code","source":"# Target encode\ntarget_enc_scoring(df_hist, 'neighbourhood', 'no_show')\n\ndf_hist.head(1)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:04:37.579788Z","iopub.execute_input":"2022-05-19T18:04:37.580463Z","iopub.status.idle":"2022-05-19T18:04:37.655802Z","shell.execute_reply.started":"2022-05-19T18:04:37.580406Z","shell.execute_reply":"2022-05-19T18:04:37.654629Z"},"trusted":true},"execution_count":563,"outputs":[]},{"cell_type":"code","source":"# global no_show mean\nround(df_hist.no_show.mean(), 2)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:04:37.657580Z","iopub.execute_input":"2022-05-19T18:04:37.658084Z","iopub.status.idle":"2022-05-19T18:04:37.665662Z","shell.execute_reply.started":"2022-05-19T18:04:37.658020Z","shell.execute_reply":"2022-05-19T18:04:37.664385Z"},"trusted":true},"execution_count":564,"outputs":[]},{"cell_type":"code","source":"df_hist[(df_hist.neighbourhood == 'REDENÇÃO') |  (df_hist.neighbourhood == 'JARDIM CAMBURI')].neighbourhood_score.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:04:37.667215Z","iopub.execute_input":"2022-05-19T18:04:37.667798Z","iopub.status.idle":"2022-05-19T18:04:37.721905Z","shell.execute_reply.started":"2022-05-19T18:04:37.667764Z","shell.execute_reply":"2022-05-19T18:04:37.720751Z"},"trusted":true},"execution_count":565,"outputs":[]},{"cell_type":"markdown","source":"Initially both neighborhoods had same 'no show' probability, after adjusting weights the probability of 'JARDIM CAMBURI' stayed the same while that of 'REDENÇÃO' moved closer to the priori (global one). It makes sense as we would be more confident in the calculated 'no show' probability for 'Jardim' than that of 'Red' given the sample size.","metadata":{}},{"cell_type":"code","source":"# Better results already?\n\n# updated train and test sets with new features\nX_train = pd.concat([X_train, df_hist[df_hist.index.isin(X_train.index)][['neighbourhood_score']]], axis = 1, \n                    join= 'inner')\nX_test = pd.concat([X_test, df_hist[df_hist.index.isin(X_test.index)][['neighbourhood_score']]], axis = 1, \n                    join= 'inner')\n\nrf_results = train_eval(RandomForestClassifier(class_weight='balanced', random_state = 42), X_train, y_train, X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:04:37.723023Z","iopub.execute_input":"2022-05-19T18:04:37.723343Z","iopub.status.idle":"2022-05-19T18:04:50.255023Z","shell.execute_reply.started":"2022-05-19T18:04:37.723312Z","shell.execute_reply":"2022-05-19T18:04:50.253917Z"},"trusted":true},"execution_count":566,"outputs":[]},{"cell_type":"markdown","source":"Better precision but no change in recall, not what we are looking for.","metadata":{}},{"cell_type":"code","source":"# Extracting additional time features\ndf_hist['time_stamp_sch'] = pd.to_datetime(df_hist.scheduling_day).dt.time.astype('str')\n\n# Remove seconds from time stamp\ndf_hist['sch_time'] = df_hist.time_stamp_sch.apply(lambda x: str(x)[:-3]).astype('str')\n\n# # Update cat cols\n# cat_cols = df_hist.select_dtypes('object').columns\n# df_hist[cat_cols] = df_hist[cat_cols].astype('str')\n\n# Target encode\ntarget_enc_scoring(df_hist, ['time_stamp_sch', 'sch_time'], 'no_show')\n\ndf_hist.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:04:50.256500Z","iopub.execute_input":"2022-05-19T18:04:50.256795Z","iopub.status.idle":"2022-05-19T18:04:50.622964Z","shell.execute_reply.started":"2022-05-19T18:04:50.256765Z","shell.execute_reply":"2022-05-19T18:04:50.621904Z"},"trusted":true},"execution_count":567,"outputs":[]},{"cell_type":"code","source":"# updated train and test sets with new features\nX_train = pd.concat([X_train, df_hist[df_hist.index.isin(X_train.index)][['time_stamp_sch_score', 'sch_time_score']]], axis = 1, \n                    join= 'inner')\nX_test = pd.concat([X_test, df_hist[df_hist.index.isin(X_test.index)][['time_stamp_sch_score', 'sch_time_score']]], axis = 1, \n                    join= 'inner')\n\nrf_results = train_eval(RandomForestClassifier(class_weight='balanced', random_state = 42), X_train, y_train, X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:04:50.624178Z","iopub.execute_input":"2022-05-19T18:04:50.624465Z","iopub.status.idle":"2022-05-19T18:05:02.176499Z","shell.execute_reply.started":"2022-05-19T18:04:50.624437Z","shell.execute_reply":"2022-05-19T18:05:02.175472Z"},"trusted":true},"execution_count":568,"outputs":[]},{"cell_type":"markdown","source":"Big leap in recall while maintaining balanced performance overall.","metadata":{}},{"cell_type":"code","source":"# combine neighborhood with time features\ndf_hist[\"nbr_sch_stamp\"] = (df_hist.neighbourhood + \"_\" + df_hist.time_stamp_sch.astype('str'))\n\ndf_hist[\"nbr_sch_time\"] = (df_hist.neighbourhood + \"_\" + df_hist.sch_time.astype('str'))\n\n# Target encode\ntarget_enc_scoring(df_hist, ['nbr_sch_stamp','nbr_sch_time'], 'no_show')\n\ndf_hist.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:05:02.177919Z","iopub.execute_input":"2022-05-19T18:05:02.178216Z","iopub.status.idle":"2022-05-19T18:05:02.588570Z","shell.execute_reply.started":"2022-05-19T18:05:02.178188Z","shell.execute_reply":"2022-05-19T18:05:02.587472Z"},"trusted":true},"execution_count":569,"outputs":[]},{"cell_type":"code","source":"# updated train and test sets with new features\nX_train = pd.concat([X_train, df_hist[df_hist.index.isin(X_train.index)][['nbr_sch_stamp_score', 'nbr_sch_time_score']]], axis = 1, \n                    join= 'inner')\nX_test = pd.concat([X_test, df_hist[df_hist.index.isin(X_test.index)][['nbr_sch_stamp_score', 'nbr_sch_time_score']]], axis = 1, \n                    join= 'inner')\n\nrf_results = train_eval(RandomForestClassifier(class_weight='balanced', random_state = 42), X_train, y_train, X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:05:02.590103Z","iopub.execute_input":"2022-05-19T18:05:02.590418Z","iopub.status.idle":"2022-05-19T18:05:13.891883Z","shell.execute_reply.started":"2022-05-19T18:05:02.590387Z","shell.execute_reply":"2022-05-19T18:05:13.890595Z"},"trusted":true},"execution_count":570,"outputs":[]},{"cell_type":"markdown","source":"Another big leap in recall combined with balanced performance overall, but hold your horses because we have leakage!\n\nSo far we've been using a fixed train/test sets that is split before target encoding, then we update the train and test set with the new encoded features that are calculated from the main dataframe and thus the calculated statistic (no_show mean per level of categorical feature) takes into account all samples (train and test samples before split) which means that the train set 'knows' about target values of test set in an indirect way, that’s why any preprocessing that includes a statistic calculation (e.g.: standardization/normalization, target encoding, etc..) needs to be performed on training data first after splitting then transformed to the test set; otherwise data leaks. \n\nTo summarize, treat test set as a completely independent set that is transformed based on calculations made on the training set.\n\nWhat is strange though is that we did not achieve perfect scores on test set even with leakage! Is this actually a leakage issue or the new features are just so powerful at discriminating ‘no show’ appointments? Could it be the effect of the regularization we applied when target encoding using sample weight?\n\nLet’s first look at feature importance, maybe we find out the reasons for the big leaps in classification results or maybe it reveals towards the end of the notebook.","metadata":{}},{"cell_type":"markdown","source":"<a id='pm'></a>","metadata":{}},{"cell_type":"code","source":"# Feature Importance\nrf_plot(rf_results.model, X_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:05:13.893674Z","iopub.execute_input":"2022-05-19T18:05:13.894166Z","iopub.status.idle":"2022-05-19T18:05:14.524248Z","shell.execute_reply.started":"2022-05-19T18:05:13.894118Z","shell.execute_reply":"2022-05-19T18:05:14.522986Z"},"trusted":true},"execution_count":571,"outputs":[]},{"cell_type":"markdown","source":"Usually this is when we should start to panic! seeing one feature that is target encoded dominates the feature importance meter is alarming, however, some of the other features that are encoded in the same manner are not that influential additionally even though we have leaked data into training set classification performance did not improve dramatically, well to some extent!","metadata":{}},{"cell_type":"markdown","source":"<li><a href=\"#toc\">Table of contents</a></li>","metadata":{}},{"cell_type":"code","source":"# Investigate time stamp feature\ndf_hist.groupby('time_stamp_sch').no_show.value_counts().sort_values(ascending=False).head(10)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-19T18:05:14.526085Z","iopub.execute_input":"2022-05-19T18:05:14.526535Z","iopub.status.idle":"2022-05-19T18:05:14.633788Z","shell.execute_reply.started":"2022-05-19T18:05:14.526484Z","shell.execute_reply":"2022-05-19T18:05:14.632854Z"},"trusted":true},"execution_count":572,"outputs":[]},{"cell_type":"code","source":"# zoom in a specific date\nprint(round(df_hist[df_hist.time_stamp_sch == '17:17:46'].no_show.mean(),2))\ndf_hist[df_hist.time_stamp_sch == '17:17:46'].sort_values(['patient_id', 'neighbourhood'])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-19T18:05:14.635116Z","iopub.execute_input":"2022-05-19T18:05:14.635401Z","iopub.status.idle":"2022-05-19T18:05:14.812923Z","shell.execute_reply.started":"2022-05-19T18:05:14.635372Z","shell.execute_reply":"2022-05-19T18:05:14.811901Z"},"trusted":true},"execution_count":573,"outputs":[]},{"cell_type":"markdown","source":"The discriminatory power of this feature makes some sense now, it appears that specific scheduling hours experience significantly higher no_show rate than others; but why?\n\nWell, given the consistent occurrence of no_show (91%) at this specific timestamp across a range of patients from different neighborhoods and age groups it must be reflecting an underlying criteria contributing to the no_show behavior, maybe all relate to specific medical provider/nature of visit/medical specialization/physician or maybe the fact that all appointments are on Monday (start of the week) combined with another reason mentioned earlier.\n\nSo, prediction results are reflecting realistic performance? No, because we still have leakage, but we now know that these new features are good at separating class labels and that our target encoding method is properly regularized, is this true?","metadata":{}},{"cell_type":"code","source":"# check record of a selected patient from the above time frame\ndf_hist[df_hist.patient_id == '12143623887377.0'].sort_values(['appointment_day', 'scheduling_day'])","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:05:14.814252Z","iopub.execute_input":"2022-05-19T18:05:14.814547Z","iopub.status.idle":"2022-05-19T18:05:14.900149Z","shell.execute_reply.started":"2022-05-19T18:05:14.814518Z","shell.execute_reply":"2022-05-19T18:05:14.899083Z"},"trusted":true},"execution_count":574,"outputs":[]},{"cell_type":"markdown","source":"By examining the detailed records of one of the patients from the above scheduling window, we note that appointments are awkwardly scheduled (scheduled on the same day with split seconds difference), and the patient does appear to be missing all such appointments. This is actually true for all 'no show' appointments in that specific window, check the 'tight schedule' feature. We observed this behavior several times earlier among different patients who miss some/all appointments that are scheduled in a very narrow time span, we can't actually tell if these are erroneous records or they are actual appointments and why such practice is taking place.  \n\nIf I was working on this project in a real life setup I'd stop here and investigate these specific dates further and incorporate more data about each appointment which should be readily available with whoever is responsible for administering the process of scheduling appointments; but we will continue just for the sake of satisfying my curious soul and also to explore the possibility of building a predictive model out of this dataset.\n\nNow that we have examined our suspicious feature, back to the leakage problem; before handling this problem we need to address another problem that popped up during our investigation of time stamps above, which is absence of uniform distribution categorical levels (time stamps have few frequent levels and many rare levels). We know that this will be problematic if we are to continue target encoding categorical features as we discussed earlier even with the introduction of noise; so we are going to group all these unique samples into a single level.\n\nLet's first examine how our target encoding function works.","metadata":{}},{"cell_type":"code","source":"# how does the encoding function works?\n\npriori = df_hist.no_show.mean()\ntot_samples = df_hist.shape[0]\n\n# Compute frequency, mean and weight of each level\nagg = df_hist.groupby('time_stamp_sch')['no_show'].agg(['count', 'mean'])\n# sample count per each level\ncounts = agg['count']\n# no show likelihood\nmeans = agg['mean']\n# proportion of transactions per level to all the transactions\n# how confident should we be in calculated statistic?\nweight = counts / tot_samples\n\n# scale weights to reduce influence of the priori\n# if we do not scale, the priori will always have higher influence on the calculated statistic\nweight = MinMaxScaler().fit_transform(weight.values.reshape(-1,1))\n\n# Compute the scoring function\nscoring = (weight.ravel() * means) + ((1-weight.ravel()) * priori)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:05:14.901582Z","iopub.execute_input":"2022-05-19T18:05:14.901895Z","iopub.status.idle":"2022-05-19T18:05:14.982217Z","shell.execute_reply.started":"2022-05-19T18:05:14.901864Z","shell.execute_reply":"2022-05-19T18:05:14.981169Z"},"trusted":true},"execution_count":575,"outputs":[]},{"cell_type":"code","source":"# check results\ntemp_df = scoring.to_frame().rename(columns={'mean':'scoring'})\ntemp_df = pd.concat([agg,temp_df], axis =1).reset_index()\ntemp_df['weight'] = weight\n\ntemp_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:05:14.983391Z","iopub.execute_input":"2022-05-19T18:05:14.983670Z","iopub.status.idle":"2022-05-19T18:05:15.015583Z","shell.execute_reply.started":"2022-05-19T18:05:14.983642Z","shell.execute_reply":"2022-05-19T18:05:15.014568Z"},"trusted":true},"execution_count":576,"outputs":[]},{"cell_type":"code","source":"# frequency of levels\ntemp_df['count'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:05:15.017039Z","iopub.execute_input":"2022-05-19T18:05:15.017347Z","iopub.status.idle":"2022-05-19T18:05:15.025700Z","shell.execute_reply.started":"2022-05-19T18:05:15.017315Z","shell.execute_reply":"2022-05-19T18:05:15.024318Z"},"trusted":true},"execution_count":577,"outputs":[]},{"cell_type":"markdown","source":"few frequent levels but many rare levels","metadata":{}},{"cell_type":"code","source":"# sample of frequent levels\ntemp_df[temp_df['count'] > 20]","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:05:15.027294Z","iopub.execute_input":"2022-05-19T18:05:15.028018Z","iopub.status.idle":"2022-05-19T18:05:15.050613Z","shell.execute_reply.started":"2022-05-19T18:05:15.027963Z","shell.execute_reply":"2022-05-19T18:05:15.049582Z"},"trusted":true},"execution_count":578,"outputs":[]},{"cell_type":"code","source":"# sample of rare levels\ntemp_df[temp_df['count'] == 1].head()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:05:15.051829Z","iopub.execute_input":"2022-05-19T18:05:15.052143Z","iopub.status.idle":"2022-05-19T18:05:15.071232Z","shell.execute_reply.started":"2022-05-19T18:05:15.052113Z","shell.execute_reply":"2022-05-19T18:05:15.069983Z"},"trusted":true},"execution_count":579,"outputs":[]},{"cell_type":"code","source":"# weights distribution\ntemp_df.weight.plot(kind='hist', bins =20);","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:05:15.072705Z","iopub.execute_input":"2022-05-19T18:05:15.073190Z","iopub.status.idle":"2022-05-19T18:05:15.267363Z","shell.execute_reply.started":"2022-05-19T18:05:15.073142Z","shell.execute_reply":"2022-05-19T18:05:15.266548Z"},"trusted":true},"execution_count":580,"outputs":[]},{"cell_type":"code","source":"# scores distribution\ntemp_df.scoring.plot(kind='hist', bins =20);","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:05:15.268513Z","iopub.execute_input":"2022-05-19T18:05:15.268975Z","iopub.status.idle":"2022-05-19T18:05:15.503238Z","shell.execute_reply.started":"2022-05-19T18:05:15.268928Z","shell.execute_reply":"2022-05-19T18:05:15.502453Z"},"trusted":true},"execution_count":581,"outputs":[]},{"cell_type":"markdown","source":"We can see how the scoring (target encoding) function works, when we have many samples the scores(mean encoding) are pulled towards the true target probability(mean) per level unlike when samples are rare that’s when scores tend to be closer to the global target probability.\n\nThis address the problems associated with lack of samples to some extent and we can see that encoded scores are normally distributed despite the skew in weights. However, a new problem emerges being absence of uniform distribution of categorical levels; there are too many rare levels compared to frequent ones this means that:\n\n- Majority of scores do not capture the trend we intend it to capture (fluctuations in 'no show' among levels) because simply it is dominated by the global mean representing these rare levels and we can see how 'narrow' is the distribution of encoded scores.\n\n- Training set will not be representative and the model will overfit, simply because there will inevitably be some levels that appear in testing set and are not seen by the model during training, also the model will overfit the training data being trained on a single sample for each level.\n\nSo we are going to group these rare levels into a single level.","metadata":{}},{"cell_type":"code","source":"# Combining best features with target\ndf_hist['time_stamp_sch_target'] = (df_hist.time_stamp_sch.astype(str) + '_' + df_hist.no_show.astype(str))\ndf_hist['nbr_sch_time_target'] = (df_hist.nbr_sch_time.astype(str) + '_' + df_hist.no_show.astype(str))\n\n# number of samples per each combined category\ndf_hist['time_stamp_sch_target_sample_count'] = df_hist.groupby('time_stamp_sch_target').no_show.transform('count')\ndf_hist['nbr_sch_time_target_sample_count'] = df_hist.groupby('nbr_sch_time_target').no_show.transform('count')\n\ndf_hist.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-19T18:05:15.504390Z","iopub.execute_input":"2022-05-19T18:05:15.504836Z","iopub.status.idle":"2022-05-19T18:05:15.981934Z","shell.execute_reply.started":"2022-05-19T18:05:15.504790Z","shell.execute_reply":"2022-05-19T18:05:15.981071Z"},"trusted":true},"execution_count":582,"outputs":[]},{"cell_type":"code","source":"# statistics of cat features\nfeat = ['time_stamp_sch', 'nbr_sch_time']\n\nfor f in feat:\n    sample_size = sorted(df_hist[f'{f}_target_sample_count'].unique())\n    for s in sample_size:\n        # statistics per sample size\n        print(f'feature \\'{f}_target\\': ', f'\\'no_show\\' mean of levels having {s} sample(s) :', round(df_hist[df_hist[f'{f}_target_sample_count'] == s].no_show.mean(),2),\\\n              '---- total sample size per level :', df_hist[df_hist[f'{f}_target_sample_count'] == s].no_show.count())","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-19T18:05:15.983235Z","iopub.execute_input":"2022-05-19T18:05:15.983729Z","iopub.status.idle":"2022-05-19T18:05:16.390167Z","shell.execute_reply.started":"2022-05-19T18:05:15.983679Z","shell.execute_reply":"2022-05-19T18:05:16.388884Z"},"trusted":true},"execution_count":583,"outputs":[]},{"cell_type":"markdown","source":"What’s interesting to note is that levels having single samples are abundant and also have high 'no show' rate compared to other levels. For the feature 'time_stamp_sch' the +ve examples of levels having single samples are around 24k while that of the scheduling window we inspected earlier are only 21 samples, this will affect how these levels are going to be represented in terms of average 'no show' as now, after grouping rare levels into a single group, the encoding function will treat the new group as the dominate one and others as being 'rare' having 'no show' mean closer to the global average.\n\nShould we care? I'd refrain from using such approach as it solves one problem but creates another; however, the fact that these new levels will have high 'no show' rate than other, and thus be more distinguishable, is interesting to explore further so we are going to group levels having single samples only and leave the rest as is.","metadata":{}},{"cell_type":"code","source":"# grouping levels having single samples\n\nfor l in feat:                                    \n    # statistics of unique samples (single samples)\n    print(f'\\'{l}_target\\' unique samples \\'no_show\\' mean:', round(df_hist[df_hist[f'{l}_target_sample_count'] == 1].no_show.mean(),2),\\\n          '---- sample count:', df_hist[df_hist[f'{l}_target_sample_count'] == 1].no_show.count())\n    \n    # grouping unique samples under one category\n    df_hist.loc[df_hist[f'{l}_target_sample_count'] == 1, f'{l}_target'] = 'rare__'\n    \n    # update count\n    df_hist[f'{l}_target_sample_count'] = df_hist.groupby(df_hist[f'{l}_target'])['no_show'].transform('count')\n    \n    # Remove association with target so the whole level is uniquely encoded regardless of target association\n    df_hist[f'{l}_target'] = df_hist[f'{l}_target'].apply(lambda x: x[:-2])\n    \n# target encode\ntarget_enc_scoring(df_hist, ['time_stamp_sch_target','nbr_sch_time_target'], 'no_show')\n\ndf_hist.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-19T18:05:16.391752Z","iopub.execute_input":"2022-05-19T18:05:16.392206Z","iopub.status.idle":"2022-05-19T18:05:16.806429Z","shell.execute_reply.started":"2022-05-19T18:05:16.392159Z","shell.execute_reply":"2022-05-19T18:05:16.804465Z"},"trusted":true},"execution_count":584,"outputs":[]},{"cell_type":"code","source":"# closer look at what just happened\n\ndf_hist[['time_stamp_sch', 'nbr_sch_time', 'time_stamp_sch_score', 'nbr_sch_time_score', \n         'time_stamp_sch_target', 'nbr_sch_time_target', 'time_stamp_sch_target_score', 'nbr_sch_time_target_score']].head(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:05:16.807916Z","iopub.execute_input":"2022-05-19T18:05:16.808259Z","iopub.status.idle":"2022-05-19T18:05:16.849529Z","shell.execute_reply.started":"2022-05-19T18:05:16.808221Z","shell.execute_reply":"2022-05-19T18:05:16.848350Z"},"trusted":true},"execution_count":585,"outputs":[]},{"cell_type":"code","source":"# number of samples below the selected rare sample threshold\nprint(temp_df[temp_df['count'] == 1].shape[0], df_hist.time_stamp_sch.nunique())\n\n# earlier scores and weights\nt = '08:45:03|11:13:45'\ntemp_df[temp_df.time_stamp_sch.str.contains(t)]","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:05:16.851349Z","iopub.execute_input":"2022-05-19T18:05:16.851758Z","iopub.status.idle":"2022-05-19T18:05:16.936499Z","shell.execute_reply.started":"2022-05-19T18:05:16.851713Z","shell.execute_reply":"2022-05-19T18:05:16.935639Z"},"trusted":true},"execution_count":586,"outputs":[]},{"cell_type":"markdown","source":"Since we are going to use target encoding for cat features we need to ensure that encoded scores are representative of underlying pattern and at the same time avoid the problems we discussed earlier, so we did the following for each level of a categorical feature:\n\n- If there are enough samples (>1) per level, then pass.\n- Else group all these unique samples under a unified level.\n\nChanging the threshold for considering a level to be rare is going to affect final scores calculated and in turn prediction results, we've select a threshold of 1 samples per level given the large number of samples below that threshold (13K out of 32k for 'time_stamp_sch'). However, this introduced another problem for levels having samples above that threshold for example the levels '08:45:03' and '11:13:45' both have 0 'no show' appointments and their earlier encoding reflected such fact by having an average target encoded score below the global average but after grouping unique samples their scores are now same as the global mean hiding the fact that these two levels experienced a lower 'no show' rate. To overcome this we will apply log transformation to counts before calculating weights to control the effect of sample size of the new level on weights calculation and encoded score of each level.\n\nLet's examine scores after log transformation.","metadata":{}},{"cell_type":"code","source":"# Final look at weights and encoded scores after grouping and log transformation\ndf = df_hist\n\npriori = df.no_show.mean()\ntot_samples = df.shape[0]\n\n# Compute frequency, mean and weight of each level\nagg = df.groupby('time_stamp_sch_target')['no_show'].agg(['count', 'mean'])\n# sample count per each level\ncounts = np.log(agg['count'])\n\n# no show likelihood\nmeans = agg['mean']\n# proportion of transactions per level to all the transactions\n# how confident should we be in calculated statistic?\nweight = counts / tot_samples\n\n# scale weights to reduce influence of the priori\n# if we do not scale, the priori will always have higher influence on the calculated statistic\nweight = MinMaxScaler().fit_transform(weight.values.reshape(-1,1))\n\n# Compute the scoring function\nscoring = (weight.ravel() * means) + ((1-weight.ravel()) * priori)\n\n# check results\ntemp_df_n = scoring.to_frame().rename(columns={'mean':'scoring'})\ntemp_df_n = pd.concat([agg,temp_df_n], axis =1).reset_index()\ntemp_df_n['weight'] = weight","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:05:16.937553Z","iopub.execute_input":"2022-05-19T18:05:16.938023Z","iopub.status.idle":"2022-05-19T18:05:16.998195Z","shell.execute_reply.started":"2022-05-19T18:05:16.937990Z","shell.execute_reply":"2022-05-19T18:05:16.996984Z"},"trusted":true},"execution_count":587,"outputs":[]},{"cell_type":"code","source":"# current scores and weights\ntemp_df_n[temp_df_n.time_stamp_sch_target.str.contains(t)]","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:05:16.999527Z","iopub.execute_input":"2022-05-19T18:05:16.999863Z","iopub.status.idle":"2022-05-19T18:05:17.037713Z","shell.execute_reply.started":"2022-05-19T18:05:16.999810Z","shell.execute_reply":"2022-05-19T18:05:17.036455Z"},"trusted":true},"execution_count":588,"outputs":[]},{"cell_type":"markdown","source":"If we compare to previous scores and weights before and after log transformation, we now have:\n- Reduced weights\n- Scoring closer to the 'no show' statistic per level and at the same time reflective of underlying sample size","metadata":{}},{"cell_type":"code","source":"# scores distribution\nfig, axes = plt.subplots(ncols=2, figsize=(10, 5), constrained_layout=True)\nax1, ax2 = axes\n\ntemp_df.scoring.plot(kind='hist', bins =20, ax = ax1, title = 'Before grouping')\ntemp_df_n.scoring.plot(kind='hist', bins =20, ax = ax2, title = 'After grouping');","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:05:17.039513Z","iopub.execute_input":"2022-05-19T18:05:17.039978Z","iopub.status.idle":"2022-05-19T18:05:17.650811Z","shell.execute_reply.started":"2022-05-19T18:05:17.039928Z","shell.execute_reply":"2022-05-19T18:05:17.649749Z"},"trusted":true},"execution_count":589,"outputs":[]},{"cell_type":"markdown","source":"Scores are still centered on global mean but more spread.","metadata":{}},{"cell_type":"markdown","source":"<li><a href=\"#toc\">Table of contents</a></li>","metadata":{}},{"cell_type":"markdown","source":"<a id='modeling'></a>","metadata":{}},{"cell_type":"markdown","source":"## Modeling\n\nWe now do a fresh start considering the notes from analysing previous classification results and related issues.","metadata":{}},{"cell_type":"code","source":"# Starting Fresh - Group rare levels, train/test split then apply target encoding\n\ndf_split = df_hist[df_hist.columns[:-15]].copy()\n\n# Extracting additional time features\ndf_split['time_stamp_sch'] = pd.to_datetime(df_split.scheduling_day).dt.time.astype('str')\n\n# Remove seconds from time stamp\ndf_split['sch_time'] = df_split.time_stamp_sch.apply(lambda x: str(x)[:-3]).astype('str')\n\n# combine neighbourhood with time features\ndf_split[\"nbr_sch_stamp\"] = (df_split.neighbourhood + \"_\" + df_split.time_stamp_sch.astype('str'))\ndf_split[\"nbr_sch_time\"] = (df_split.neighbourhood + \"_\" + df_split.sch_time.astype('str'))\n\n# number of samples per rare levels\ndf_split['time_stamp_sch_sample_count'] = df_split.groupby('time_stamp_sch').no_show.transform('count')\ndf_split['sch_time_sample_count'] = df_split.groupby('sch_time').no_show.transform('count')\ndf_split['nbr_sch_stamp_sample_count'] = df_split.groupby('nbr_sch_stamp').no_show.transform('count')\ndf_split['nbr_sch_time_sample_count'] = df_split.groupby('nbr_sch_time').no_show.transform('count')\n\n# statistics of cat features\nfeat = ['time_stamp_sch', 'sch_time', 'nbr_sch_stamp', 'nbr_sch_time']\n\nfor f in feat:\n    # Combining best features with target\n    df_split[f'{f}_target'] = (df_split[f'{f}'].astype(str) + '_' + df_split.no_show.astype(str))\n    \n    # number of samples per each combined category\n    df_split[f'{f}_target_sample_count'] = df_split.groupby(f'{f}_target').no_show.transform('count')\n    \n    # statistics of unique samples (single samples)\n    print(f'\\'{f}\\' unique samples \\'no_show\\' mean:', round(df_split[df_split[f'{f}_target_sample_count'] == 1].no_show.mean(),2),\\\n          '---- sample count:', df_split[df_split[f'{f}_target_sample_count'] == 1].no_show.count())\n    \n    # grouping unique samples under one category\n    df_split.loc[df_split[f'{f}_target_sample_count'] == 1, f'{f}'] = 'rare'\n    \n    # update count\n    df_split[f'{f}_target_sample_count'] = df_split.groupby(df_split[f'{f}'])['no_show'].transform('count')\n    \n    # drop temp column\n    df_split.drop(f'{f}_target', axis = 1, inplace = True)\n\n# Sort column\nno_show_idx = df_split.columns.get_loc(\"no_show\")\n\ncolumns_list = df_split.columns.tolist()\n\ncolumns_sort = columns_list[0:no_show_idx] + columns_list[no_show_idx+1:] + columns_list[no_show_idx : no_show_idx+1]\n\ndf_split = df_split[columns_sort]\n\ndf_split.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:05:17.660115Z","iopub.execute_input":"2022-05-19T18:05:17.660433Z","iopub.status.idle":"2022-05-19T18:05:19.575171Z","shell.execute_reply.started":"2022-05-19T18:05:17.660404Z","shell.execute_reply":"2022-05-19T18:05:19.574114Z"},"trusted":true},"execution_count":591,"outputs":[]},{"cell_type":"code","source":"# Train/Test split\n\n# separate categorical and numeric columns\ncat_cols = df_split.select_dtypes('object').columns\nnum_cols = df_split.columns[~df_split.columns.isin(cat_cols)]\n\n# unifying numeric dtypes\ndf_split[num_cols[:-1]] = df_split[num_cols[:-1]].astype(float, errors='ignore')\n\n# split data\nfeatures = df_split.drop(df_split.filter(regex='count|no_show').columns, axis=1)\n# features = df_split.drop(['no_show'], axis=1)\nlabels = df_split.no_show\n\nX_train, X_test, y_train, y_test = train_test_split(features, labels, stratify = labels, test_size=.2, random_state=42)\n\nX_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:05:19.576786Z","iopub.execute_input":"2022-05-19T18:05:19.577112Z","iopub.status.idle":"2022-05-19T18:05:19.814596Z","shell.execute_reply.started":"2022-05-19T18:05:19.577080Z","shell.execute_reply":"2022-05-19T18:05:19.813598Z"},"trusted":true},"execution_count":592,"outputs":[]},{"cell_type":"code","source":"# Target encode and transform test set\n\ndef target_encode_feat_mapping(key, X_train, X_test, y_train, enc_func = target_enc_scoring, with_weight = True, \n                               log_trans = False, fillna = True):\n    \n    '''apply target encoding to training set then transform test set'''\n    \n    # incase several columns are passed at one\n    if isinstance(key, str):\n        key = [key]\n    else:\n        key = key\n\n    X_train['target'] = y_train\n    \n    # Last column in train set, to mark start of newly created columns (0 indexing)\n    # -1 for no_show column to be dropped latter\n    start_col = X_train.shape[1] - 1\n\n    # Target encode\n    enc_func(X_train, key, 'target', with_weight = with_weight, log_trans = log_trans)\n\n    # drop\n    X_train.drop(['target'], axis=1, inplace=True)\n    \n    # newly created target encoded columns of train set\n    value = X_train.iloc[:,start_col:].columns\n    \n    # X_test transformation   \n    # applying transformation and filling nan (unseen data) with average of training data columns\n    for k, v in zip(key, value):\n        # map keys(label encoded cat feats) to their target encoded values in the train set\n        mapr = dict(X_train[[k, v]].values)\n        \n        if fillna:\n            # filling nan with calculated scores mean\n            X_test[v] = X_test[k].map(mapr).fillna(X_train[v].mean())\n        else:\n            X_test[v] = X_test[k].map(mapr)\n\n            \n# Copies to modify\nX_train_mod = X_train.copy()\nX_test_mod = X_test.copy()\n\nkey = ['time_stamp_sch', 'sch_time', 'neighbourhood', 'nbr_sch_stamp','nbr_sch_time']\n\ntarget_encode_feat_mapping(key, X_train_mod, X_test_mod, y_train, log_trans = True)\n\n# Remove cat features\nX_train_mod = X_train_mod.drop(cat_cols, axis=1).copy()\nX_test_mod = X_test_mod.drop(cat_cols, axis=1).copy()\n\nX_test_mod.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:05:19.816242Z","iopub.execute_input":"2022-05-19T18:05:19.816673Z","iopub.status.idle":"2022-05-19T18:05:20.720095Z","shell.execute_reply.started":"2022-05-19T18:05:19.816626Z","shell.execute_reply":"2022-05-19T18:05:20.718929Z"},"trusted":true},"execution_count":593,"outputs":[]},{"cell_type":"code","source":"# leakage busted?\nrf_results = train_eval(RandomForestClassifier(class_weight='balanced', random_state = 42), \n                        X_train_mod, y_train, X_test_mod, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:05:20.721740Z","iopub.execute_input":"2022-05-19T18:05:20.722197Z","iopub.status.idle":"2022-05-19T18:05:30.324159Z","shell.execute_reply.started":"2022-05-19T18:05:20.722151Z","shell.execute_reply":"2022-05-19T18:05:30.323191Z"},"trusted":true},"execution_count":594,"outputs":[]},{"cell_type":"markdown","source":"Precision and recall are now 64 and 62 respectively, down from 76 and 64 before we deal with leakage issue. The former scores also incorporate the effect of grouping unique samples per each cat feature.","metadata":{}},{"cell_type":"code","source":"# Feature Importance\nrf_plot(rf_results.model, X_test_mod)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:05:30.325384Z","iopub.execute_input":"2022-05-19T18:05:30.325653Z","iopub.status.idle":"2022-05-19T18:05:30.951735Z","shell.execute_reply.started":"2022-05-19T18:05:30.325626Z","shell.execute_reply":"2022-05-19T18:05:30.950649Z"},"trusted":true},"execution_count":595,"outputs":[]},{"cell_type":"markdown","source":"Now we examine the effect of incorporating more categorical features to the model combining neighborhood and time features.","metadata":{}},{"cell_type":"code","source":"# Assessing combining neighborhood and elapsed_days_sch feature\ndf_split.groupby(['neighbourhood', 'elapsed_days_sch']).no_show.agg(['count', 'mean']).sort_values('mean', ascending=False).head(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:05:30.953143Z","iopub.execute_input":"2022-05-19T18:05:30.953463Z","iopub.status.idle":"2022-05-19T18:05:30.987472Z","shell.execute_reply.started":"2022-05-19T18:05:30.953434Z","shell.execute_reply":"2022-05-19T18:05:30.986437Z"},"trusted":true},"execution_count":596,"outputs":[]},{"cell_type":"code","source":"# detailed records of combined features\ndf_split[(df_split.neighbourhood == 'HORTO') & (df_split.elapsed_days_sch == 10)]","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:05:30.988819Z","iopub.execute_input":"2022-05-19T18:05:30.989236Z","iopub.status.idle":"2022-05-19T18:05:31.066813Z","shell.execute_reply.started":"2022-05-19T18:05:30.989202Z","shell.execute_reply":"2022-05-19T18:05:31.065734Z"},"trusted":true},"execution_count":597,"outputs":[]},{"cell_type":"markdown","source":"Combining both features seems beneficial as some categories will have pure 'no show' appointments. In the example above, all appointments in neighborhood 'HORTO' having 10 elapsed days between scheduling and appointment dates were 'no show' and these are appointments of different patients so this behavior is not confined to a single patient. \n\nExact reason of 'no show' is still vague as most of appointment details are somewhat similar; however, there is consistency among scheduling and appointment dates as all appointments were scheduled on the same day to take place on the same date so maybe they relate to specific medical provider or share some common criteria.","metadata":{}},{"cell_type":"code","source":"# combine neighborhood with elapsed days features\ndf_split['nbr_elapsed_days'] = (df_split.neighbourhood + \"_\" + df_split.elapsed_days_sch.astype('str'))\n\n# combine with target for grouping\ndf_split['nbr_elapsed_days_target'] = (df_split.nbr_elapsed_days.astype(str) + '_' + df_split.no_show.astype(str))\n\n# number of samples per each combined category\ndf_split['nbr_elapsed_days_target_sample_count'] = df_split.groupby('nbr_elapsed_days_target').no_show.transform('count')\n\n# statistics of unique samples (single samples)\nprint('\\'nbr_elapsed_days\\' unique samples \\'no_show\\' mean:', round(df_split[df_split['nbr_elapsed_days_target_sample_count'] == 1].no_show.mean(),2),\\\n      '---- sample count:', df_split[df_split['nbr_elapsed_days_target_sample_count'] == 1].no_show.count())\n\n# grouping unique samples into one category\ndf_split.loc[df_split.nbr_elapsed_days_target_sample_count == 1, \"nbr_elapsed_days\"] = 'rare'\n\n# update count\ndf_split['nbr_elapsed_days_target_sample_count'] = df_split.groupby('nbr_elapsed_days').no_show.transform('count')\n\n# drop temp column\ndf_split.drop('nbr_elapsed_days_target', axis = 1, inplace = True)\n\n# update categorical and numeric columns\ncat_cols = df_split.select_dtypes('object').columns\nnum_cols = df_split.columns[~df_split.columns.isin(cat_cols)]\n\n# Sort column\nno_show_idx = df_split.columns.get_loc(\"no_show\")\n\ncolumns_list = df_split.columns.tolist()\n\ncolumns_sort = columns_list[0:no_show_idx] + columns_list[no_show_idx+1:] + columns_list[no_show_idx : no_show_idx+1]\n\ndf_split = df_split[columns_sort]\n\ndf_split.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:05:31.068608Z","iopub.execute_input":"2022-05-19T18:05:31.069089Z","iopub.status.idle":"2022-05-19T18:05:31.493914Z","shell.execute_reply.started":"2022-05-19T18:05:31.069040Z","shell.execute_reply":"2022-05-19T18:05:31.492895Z"},"trusted":true},"execution_count":598,"outputs":[]},{"cell_type":"code","source":"# Add new feature to train and test set then target encode\nX_train_mod = pd.concat([X_train_mod, df_split[df_split.index.isin(X_train_mod.index)][['nbr_elapsed_days']]], axis = 1, \n                    join= 'inner')\n\nX_test_mod = pd.concat([X_test_mod, df_split[df_split.index.isin(X_test_mod.index)][['nbr_elapsed_days']]], axis = 1, \n                    join= 'inner')\n\n# Target encode and transform test set\nkey = ['nbr_elapsed_days']\n\ntarget_encode_feat_mapping(key, X_train_mod, X_test_mod, y_train, log_trans = True)\n\n# Remove cat features, random forest works with only with numeric features\nX_train_mod = X_train_mod.drop('nbr_elapsed_days', axis=1).copy()\nX_test_mod = X_test_mod.drop('nbr_elapsed_days', axis=1).copy()\n\nX_test_mod.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:05:31.495251Z","iopub.execute_input":"2022-05-19T18:05:31.495533Z","iopub.status.idle":"2022-05-19T18:05:31.814578Z","shell.execute_reply.started":"2022-05-19T18:05:31.495505Z","shell.execute_reply":"2022-05-19T18:05:31.813591Z"},"trusted":true},"execution_count":599,"outputs":[]},{"cell_type":"code","source":"# worth it?\n\nrf_results = train_eval(RandomForestClassifier(class_weight='balanced', random_state = 42), \n                        X_train_mod, y_train, X_test_mod, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:05:31.815669Z","iopub.execute_input":"2022-05-19T18:05:31.816054Z","iopub.status.idle":"2022-05-19T18:05:41.703227Z","shell.execute_reply.started":"2022-05-19T18:05:31.816021Z","shell.execute_reply":"2022-05-19T18:05:41.702159Z"},"trusted":true},"execution_count":600,"outputs":[]},{"cell_type":"markdown","source":"Not a worthy addition as we are interested in higher recall over precision.","metadata":{}},{"cell_type":"code","source":"rf_plot(rf_results.model, X_test_mod)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:05:41.704769Z","iopub.execute_input":"2022-05-19T18:05:41.705197Z","iopub.status.idle":"2022-05-19T18:05:42.463588Z","shell.execute_reply.started":"2022-05-19T18:05:41.705155Z","shell.execute_reply":"2022-05-19T18:05:42.462761Z"},"trusted":true},"execution_count":601,"outputs":[]},{"cell_type":"markdown","source":"<li><a href=\"#toc\">Table of contents</a></li>","metadata":{}},{"cell_type":"markdown","source":"<a id='fs'></a>","metadata":{}},{"cell_type":"markdown","source":"#### Feature selection\n\ncheck [this notebook](https://www.kaggle.com/prashant111/comprehensive-guide-on-feature-selection) for various methods of feature selection with examples, we will be using univariate feature selection for simplicity.","metadata":{}},{"cell_type":"code","source":"# Feature selection\n# code source--> https://github.com/abhishekkrthakur/approachingalmost\n\nclass UnivariateFeatureSelction:\n    def __init__(self, n_features, problem_type, scoring, return_cols=True):\n        \"\"\"\n        Custom univariate feature selection wrapper on\n        different univariate feature selection models from\n        scikit-learn.\n        :param n_features: SelectPercentile if float else SelectKBest\n        :param problem_type: classification or regression\n        :param scoring: scoring function, string\n        \"\"\"\n        self.n_features = n_features\n        \n        if problem_type == \"classification\":\n            valid_scoring = {\n                \"f_classif\": f_classif,\n                \"chi2\": chi2,\n                \"mutual_info_classif\": mutual_info_classif\n            }\n        else:\n            valid_scoring = {\n                \"f_regression\": f_regression,\n                \"mutual_info_regression\": mutual_info_regression\n            }\n        if scoring not in valid_scoring:\n            raise Exception(\"Invalid scoring function\")\n            \n        if isinstance(n_features, int):\n            self.selection = SelectKBest(\n                valid_scoring[scoring],\n                k=n_features\n            )\n        elif isinstance(n_features, float):\n            self.selection = SelectPercentile(\n                valid_scoring[scoring],\n                percentile=int(n_features * 100)\n            )\n        else:\n            raise Exception(\"Invalid type of feature\")\n    \n    def fit(self, X, y):\n        return self.selection.fit(X, y)\n    \n    def transform(self, X):\n        return self.selection.transform(X)\n    \n    def fit_transform(self, X, y):\n        return self.selection.fit_transform(X, y)\n    \n    def return_cols(self, X):\n        if isinstance(self.n_features, int):\n            mask = SelectKBest.get_support(self.selection)\n            selected_features = []\n            features = list(X.columns)\n            for bool, feature in zip(mask, features):\n                if bool:\n                    selected_features.append(feature)\n                    \n        elif isinstance(self.n_features, float):\n            mask = SelectPercentile.get_support(self.selection)\n            selected_features = []\n            features = list(X.columns)\n            for bool, feature in zip(mask, features):\n                if bool:\n                    selected_features.append(feature)\n        else:\n            raise Exception(\"Invalid type of feature\")\n        \n        return selected_features\n\n\nufs = UnivariateFeatureSelction(n_features = 20, problem_type = \"classification\", scoring = \"mutual_info_classif\")\n\nfeatures = X_train_mod.columns\n\nufs.fit(X_train_mod[features], y_train.values.ravel())\n\nselected_features = ufs.return_cols(X_train_mod[features])\n\nselected_features","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:05:42.464693Z","iopub.execute_input":"2022-05-19T18:05:42.465123Z","iopub.status.idle":"2022-05-19T18:06:10.551125Z","shell.execute_reply.started":"2022-05-19T18:05:42.465092Z","shell.execute_reply":"2022-05-19T18:06:10.549773Z"},"trusted":true},"execution_count":602,"outputs":[]},{"cell_type":"code","source":"# train on selected features only\nX_train_mod_sf = X_train_mod[selected_features]\nX_test_mod_sf = X_test_mod[selected_features]\n\nrf_results = train_eval(RandomForestClassifier(max_depth=17, class_weight='balanced', random_state = 42), X_train_mod_sf, y_train, X_test_mod_sf, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:06:10.552908Z","iopub.execute_input":"2022-05-19T18:06:10.553366Z","iopub.status.idle":"2022-05-19T18:06:17.496772Z","shell.execute_reply.started":"2022-05-19T18:06:10.553319Z","shell.execute_reply":"2022-05-19T18:06:17.495700Z"},"trusted":true},"execution_count":603,"outputs":[]},{"cell_type":"markdown","source":"<li><a href=\"#toc\">Table of contents</a></li>","metadata":{}},{"cell_type":"markdown","source":"<a id='rts'></a>","metadata":{}},{"cell_type":"markdown","source":"#### Resampling training set\n\nClassification results are affected by the unbalanced nature of the dataset, first thing we're going to do is to introduce more samples to both class labels using the data of patients having single appointment; next we're going to explore threshold moving and assesse total misclassification cost for each threshold.","metadata":{}},{"cell_type":"code","source":"# Resampling the training set by adding all sample of single patient appointments\n# first we apply same preprocessing to single appointment dataframe\n\n# avg elapsed days per neighborhood\ndf_first['avg_elaps_day_neighbourhood'] = df_first.groupby('neighbourhood').elapsed_days_sch.transform('mean').round()\n\n# difference between elapsed day and average per neighborhood\ndf_first['elday_navg_diff'] = df_first.elapsed_days_sch - df_first.avg_elaps_day_neighbourhood\n\n# Extracting additional timing features\ndf_first['sch_day'] =  pd.to_datetime(df_first.scheduling_day).dt.day\ndf_first['sch_month'] =  pd.to_datetime(df_first.scheduling_day).dt.month\ndf_first['sch_hour'] =  pd.to_datetime(df_first.scheduling_day).dt.hour\ndf_first['app_month'] =  pd.to_datetime(df_first.appointment_day).dt.month\ndf_first['day_of_year_sch'] =  pd.to_datetime(df_first.scheduling_day).dt.dayofyear\ndf_first['time_stamp_sch'] = pd.to_datetime(df_first.scheduling_day).dt.time.astype('str')\n\n# Remove seconds from time stamp\ndf_first['sch_time'] = df_first.time_stamp_sch.apply(lambda x: str(x)[:-3]).astype('str')\n\n# filler columns for merging, calculations will be updated later\ndf_first['msd_app_sch_slot_nb'] = 0\ndf_first['msd_app_sch_slot'] = 0\n\n# Mark all samples to easily identify later\ndf_first['n'] = 1\n\ndf_first.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:06:17.498307Z","iopub.execute_input":"2022-05-19T18:06:17.498635Z","iopub.status.idle":"2022-05-19T18:06:17.737244Z","shell.execute_reply.started":"2022-05-19T18:06:17.498603Z","shell.execute_reply":"2022-05-19T18:06:17.736200Z"},"trusted":true},"execution_count":604,"outputs":[]},{"cell_type":"code","source":"# Preprocessing function\n\ndef combine_group(df, features, target, label_encode = False):\n    ''' Combine unique samples into unique levels and apply label encoding '''\n    \n    # incase several features are passed at once\n    if isinstance(features, list):\n        features = features\n    else:\n        features = [features]\n    \n    for f in features:\n        \n        # Combining with target to check sample count\n        df[f'{f}_target'] = (df[f].astype(str) + '_' + df[target].astype(str))\n\n        # combined feature sample count\n        df[f'{f}_sample_count'] = df.groupby(df[f'{f}_target'])[target].transform('count')\n        \n        # grouping unique samples under one category\n        if df[df[f'{f}_sample_count'] == 1][f'{f}_sample_count'].sum() > 0:\n            \n            df.loc[df[f'{f}_sample_count'] == 1, f\"{f}\"] = 'rare'\n            \n            # statistics of unique samples (single samples)\n            print(f'\\'{f}\\' unique samples \\'no_show\\' mean:', round(df[df[f'{f}_sample_count'] == 1].no_show.mean(),2),\\\n                  '---- sample count:', df[df[f'{f}_sample_count'] == 1].no_show.count())\n        \n            # update count\n            df[f'{f}_sample_count'] = df.groupby(df[f'{f}'])[target].transform('count')\n            \n            # drop temp column\n            df.drop(f'{f}_target', axis = 1, inplace = True)\n        \n        if label_encode:    \n            encoder = LabelEncoder()\n            df[f'{f}_'] = encoder.fit_transform(df[f'{f}'])","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:06:17.738667Z","iopub.execute_input":"2022-05-19T18:06:17.738990Z","iopub.status.idle":"2022-05-19T18:06:17.750103Z","shell.execute_reply.started":"2022-05-19T18:06:17.738958Z","shell.execute_reply":"2022-05-19T18:06:17.748915Z"},"trusted":true},"execution_count":605,"outputs":[]},{"cell_type":"code","source":"# Starting Fresh - Resampling then apply target encoding\n\n# match columns of the two dataframes as single appointment patients do not have all features of those with multiple appointments\ndf_split_re = df_split[df_first.columns[:-1]].copy()\n\n# Mark old samples\ndf_split_re['n'] = 0\n\n# Combine the two dataframes\ndf_split_re = df_split_re.append(df_first, ignore_index=True)\n\n# Update numeric features by new samples\ndf_split_re['msd_app_sch_slot_nb'] = df_split_re.sort_values(['appointment_day','scheduling_day']).\\\ngroupby(['neighbourhood', 'sch_day', 'sch_month', 'sch_hour']).no_show.shift().fillna(0)\n\ndf_split_re['msd_app_sch_slot_nb'] = df_split_re.sort_values(['appointment_day','scheduling_day']).\\\ngroupby(['neighbourhood', 'sch_day', 'sch_month', 'sch_hour']).msd_app_sch_slot_nb.cumsum().fillna(0)\n\ndf_split_re['msd_app_sch_slot'] = df_split_re.sort_values(['appointment_day', 'scheduling_day']).groupby(['sch_day', 'sch_month', 'sch_hour'])\\\n.no_show.shift().fillna(0)\n\ndf_split_re['msd_app_sch_slot'] = df_split_re.sort_values(['appointment_day', 'scheduling_day']).groupby(['sch_day', 'sch_month', 'sch_hour'])\\\n.msd_app_sch_slot.cumsum().fillna(0)\n\n# Remove previous grouping\ndf_split_re['time_stamp_sch'] = pd.to_datetime(df_split_re.scheduling_day).dt.time.astype('str')\ndf_split_re['sch_time'] = df_split_re.time_stamp_sch.apply(lambda x: str(x)[:-3]).astype('str')\ndf_split_re[\"nbr_sch_time\"] = (df_split_re.neighbourhood + \"_\" + df_split_re.sch_time.astype(str))\ndf_split_re[\"nbr_elapsed_days\"] = (df_split_re.neighbourhood + \"_\" + df_split_re.elapsed_days_sch.astype('str'))\n\n# Group unique samples\nfeatures = ['time_stamp_sch', 'sch_time', 'neighbourhood', 'nbr_sch_time', 'nbr_elapsed_days']\n\ntarget = 'no_show'\n\ncombine_group(df_split_re, features, target, label_encode = True)\n\n# Sort column\nno_show_idx = df_split_re.columns.get_loc('no_show')\n\ncolumns_list = df_split_re.columns.tolist()\n\ncolumns_sort = columns_list[0:no_show_idx] + columns_list[no_show_idx+1:] + columns_list[no_show_idx : no_show_idx+1]\n\ndf_split_re = df_split_re[columns_sort]\n\ndf_split_re.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:06:17.751462Z","iopub.execute_input":"2022-05-19T18:06:17.751771Z","iopub.status.idle":"2022-05-19T18:06:24.634645Z","shell.execute_reply.started":"2022-05-19T18:06:17.751743Z","shell.execute_reply":"2022-05-19T18:06:24.633261Z"},"trusted":true},"execution_count":606,"outputs":[]},{"cell_type":"code","source":"# Train/Test split, we have different feature setup than previous split as now we incorporated patients with single appointments\n# they miss some of the features created for the ones having multiple appointments\n\n# separate categorical and numeric columns\ncat_cols_re = df_split_re.select_dtypes('object').columns\nnum_cols_re = df_split_re.columns[~df_split_re.columns.isin(cat_cols_re)]\n\n# remove unnecessary cat cols\nmask = df_split_re.drop(cat_cols_re[0:5], axis=1)\n\n# unifying numeric dtypes\nmask[num_cols_re[:-1]] = mask[num_cols_re[:-1]].astype('float', errors='ignore')\n\n# split old samples, add all new samples to train set later\nmask_old = mask[mask.n == 0].copy()\n\n# split data\nfeatures_re = mask_old.drop(['time_stamp_sch_sample_count', 'sch_time_sample_count', \n                             'neighbourhood_sample_count', 'nbr_sch_time_sample_count', \n                             'nbr_elapsed_days_sample_count', 'n', 'no_show'], axis=1)\nlabels_re = mask_old.no_show\n\nX_train_re, X_test_re, y_train_re, y_test_re = train_test_split(features_re, labels_re, stratify = labels_re, \n                                                                test_size=.2, random_state=42)\n\nX_train_re.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:06:24.641164Z","iopub.execute_input":"2022-05-19T18:06:24.641575Z","iopub.status.idle":"2022-05-19T18:06:24.898332Z","shell.execute_reply.started":"2022-05-19T18:06:24.641482Z","shell.execute_reply":"2022-05-19T18:06:24.897229Z"},"trusted":true},"execution_count":607,"outputs":[]},{"cell_type":"code","source":"# no_show statistics before resampling\ny_train.value_counts(), y_train.value_counts(normalize=True).round(2)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:06:24.899978Z","iopub.execute_input":"2022-05-19T18:06:24.900269Z","iopub.status.idle":"2022-05-19T18:06:24.912457Z","shell.execute_reply.started":"2022-05-19T18:06:24.900240Z","shell.execute_reply":"2022-05-19T18:06:24.911248Z"},"trusted":true},"execution_count":608,"outputs":[]},{"cell_type":"code","source":"# Resample train set\n\n# Copies to modify\nX_train_mod = X_train_re.copy()\nX_test_mod = X_test_re.copy()\n\n# using both +ve and -ve examples\nX_train_mod = X_train_mod.append(df_split_re[(df_split_re.n == 1)][features_re.columns])\n\n# update labels\ny_train_re = y_train_re.append(df_split_re[(df_split_re.n == 1)]['no_show'])\n\ny_train_re.value_counts(), y_train_re.value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:06:24.914317Z","iopub.execute_input":"2022-05-19T18:06:24.914933Z","iopub.status.idle":"2022-05-19T18:06:25.008793Z","shell.execute_reply.started":"2022-05-19T18:06:24.914882Z","shell.execute_reply":"2022-05-19T18:06:25.007680Z"},"trusted":true},"execution_count":609,"outputs":[]},{"cell_type":"code","source":"# Target Encode\n\n# cols to encode\nkey = ['time_stamp_sch', 'sch_time', 'neighbourhood', 'nbr_sch_time', 'nbr_elapsed_days']\n\ntarget_encode_feat_mapping(key, X_train_mod, X_test_mod, y_train_re, log_trans = True)\n\n# Remove cat features features\nX_train_mod_re = X_train_mod.drop(cat_cols_re[5:], axis=1).copy()\nX_test_mod_re = X_test_mod.drop(cat_cols_re[5:], axis=1).copy()\n\nX_test_mod_re.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:06:25.009802Z","iopub.execute_input":"2022-05-19T18:06:25.010102Z","iopub.status.idle":"2022-05-19T18:06:26.391205Z","shell.execute_reply.started":"2022-05-19T18:06:25.010071Z","shell.execute_reply":"2022-05-19T18:06:26.390082Z"},"trusted":true},"execution_count":610,"outputs":[]},{"cell_type":"markdown","source":"<a id='prevr'></a>","metadata":{}},{"cell_type":"code","source":"# worth it?\n\nrf_results_re = train_eval(RandomForestClassifier(class_weight='balanced', random_state = 42), X_train_mod_re, y_train_re, \n                                                                                               X_test_mod_re, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:06:26.392718Z","iopub.execute_input":"2022-05-19T18:06:26.393102Z","iopub.status.idle":"2022-05-19T18:06:45.276281Z","shell.execute_reply.started":"2022-05-19T18:06:26.393066Z","shell.execute_reply":"2022-05-19T18:06:45.275035Z"},"trusted":true},"execution_count":611,"outputs":[]},{"cell_type":"markdown","source":"Resampling the training set resulted in better classification performance compared to previous results before resampling. \n\nNote that we are using both label and target encoded cat features and removed 'nbr_sch_stamp' feature completely; however, performance is still improved after adding new samples even without these modifications.","metadata":{}},{"cell_type":"code","source":"rf_plot(rf_results_re.model, X_test_mod_re)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:06:45.278084Z","iopub.execute_input":"2022-05-19T18:06:45.278418Z","iopub.status.idle":"2022-05-19T18:06:45.831459Z","shell.execute_reply.started":"2022-05-19T18:06:45.278381Z","shell.execute_reply":"2022-05-19T18:06:45.830393Z"},"trusted":true},"execution_count":612,"outputs":[]},{"cell_type":"markdown","source":"<li><a href=\"#impr\">Fast forward to improved results</a></li>","metadata":{}},{"cell_type":"code","source":"# Feature selection\n\nufs = UnivariateFeatureSelction(n_features = 20, problem_type = \"classification\", scoring = \"mutual_info_classif\")\n\nfeatures = X_train_mod_re.columns\n\nufs.fit(X_train_mod_re[features], y_train_re.values.ravel())\n\nselected_features = ufs.return_cols(X_train_mod_re[features])\n\nselected_features","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:06:45.832702Z","iopub.execute_input":"2022-05-19T18:06:45.833234Z","iopub.status.idle":"2022-05-19T18:07:19.547702Z","shell.execute_reply.started":"2022-05-19T18:06:45.833178Z","shell.execute_reply":"2022-05-19T18:07:19.546739Z"},"trusted":true},"execution_count":613,"outputs":[]},{"cell_type":"markdown","source":"<a id='prevr2'></a>","metadata":{}},{"cell_type":"code","source":"# Train on selected features only\nX_train_mod_re_sf = X_train_mod_re[selected_features]\nX_test_mod_re_sf = X_test_mod_re[selected_features]\n\nrf_results_re_sf = train_eval(RandomForestClassifier(min_samples_leaf = 5, class_weight='balanced', random_state = 42), \n                              X_train_mod_re_sf, y_train_re, X_test_mod_re_sf, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:07:19.549242Z","iopub.execute_input":"2022-05-19T18:07:19.549555Z","iopub.status.idle":"2022-05-19T18:07:36.138208Z","shell.execute_reply.started":"2022-05-19T18:07:19.549525Z","shell.execute_reply":"2022-05-19T18:07:36.137060Z"},"trusted":true},"execution_count":614,"outputs":[]},{"cell_type":"markdown","source":"Same consistent improved performance compared to previous predictions on selected features but before resampling the training set. As a reminder, I'm seeking a higher recall on +ve class without significant loss in precision as there will always be a tradeoff between these two metrics and predicting a false 'no show' (false positive) is better than a false show (false negative) hypothetically; we will discuss this in more details latter when selecting a threshold for classification.\n\nNote that we've been addressing class imbalance issue so far on the classifier level by setting 'class_weight' hyperparameter to 'balanced', this will force the classifier to pay equal attention to both classes; one way to set the class weight parameter is to inverse the current distribution of class labels (‘class_weight’ = {0:.8, 1:.2}) or play around with some other percentages till you achieve desired results.\n\nAdditionally we've increase the required number of samples to consider a node to be a leaf node (no further splitting) from 1 (default) to 5 and this is to prune trees and reduce overfitting.\n\nPruning? Overfitting? .....","metadata":{}},{"cell_type":"markdown","source":"<li><a href=\"#impr2\">Fast forward to improved results</a></li>","metadata":{}},{"cell_type":"code","source":"# Sample of a decision Tree\n\n# grow a tree\nmed_tree = tree.DecisionTreeClassifier(max_depth = 3)\nmed_tree.fit(X_train_mod_re_sf, y_train_re)\n\n# Plot\nfig  = plt.subplots(figsize=(11, 11), dpi=800)\nplt.tight_layout()\n\ntree.plot_tree(med_tree, filled=True, rounded=True, feature_names =  X_train_mod_re_sf.columns, fontsize=9);","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:07:36.140012Z","iopub.execute_input":"2022-05-19T18:07:36.140408Z","iopub.status.idle":"2022-05-19T18:07:43.193485Z","shell.execute_reply.started":"2022-05-19T18:07:36.140371Z","shell.execute_reply":"2022-05-19T18:07:43.192574Z"},"trusted":true},"execution_count":615,"outputs":[]},{"cell_type":"markdown","source":"Tree classifiers cast a series of yes/no questions accordingly classification routes are determined, the depth of tree indicates the number of decisions that have to be made down the longest path through the tree. Important features for decision making are used as a splitting points early on (or near the top of the tree), that’s why 'time_stamp_sch_' is being used as a first decision split point, we know from random forest feature importance plots that this feature is the one with highest importance score.\n\nBoxes are called nodes; there are two types of nodes: 1) Split nodes that pose yes/no question and leads to further splitting; the very first split node is called root node. 2) Terminal nodes (leaf nodes) is where classifications are made. In the plot above, the terminal nodes(leaf nodes) are the nodes at the bottom of the figure that have no branches or further decision nodes below them(no further splitting). \n\nEach arrow represents an answer to the posed question, arrows to the left hand side (right side of the root node pointing to 'time_stamp_sch_score') are the 'Yes' answer to the condition within the box; opposite arrows are the 'No' answer.\n\nNode colors indicate class membership, where brownish colors represent -ve class and bluish colors represents +ve class. The intensity of coloring follows class concentration within each node, the higher the number of samples of a given class the darker the color.\n\nText within each node holds several information about the structure of the node, such as:\n\n- Split criteria, which is the basis for making a split decision\n- Measure of split quality based on Gini and Entropy criteria. The choice does not make a lot of difference in performance.\n- Total sample size\n- Class distribution per node, root node holds all samples and same class distribution as seen the training set since no decision is being made.\n\nHow Predictions are made?\n\nSplit point is first determined, and then each sample in the dataset is passed to a non-terminal node where further splitting takes place until tree structure is achieved. \n\nNow let’s follow a decision route from our sample tree, the one at the right hand side leading to the blue leaf node:\n- time_stamp_sch_ <=0.21, No, nbr_sch_time_score <= 0.18, No, nbr_elapsed_days_score <= 0.18, No, then all samples within this criteria gets classified as a +ve class.\n\nThe structure of our sample tree is of depth 3 and minimum sample per leaf is 1, this means that the tree is only allowed a total of 3 splits before reaching to a classification decision, and each split will only take place if the resulting leaf nod holds at least 1 sample. Such structure may not be sufficient depending on the problem at hand, sometimes deep trees are needed and best thing to do is to test multiple ranges of values and their impact on classification results.\n\nPruning is controlling the tree growth in a way that maintains a balance between the amount of information needed to produce accurate predictions and at the same time control overfitting.","metadata":{}},{"cell_type":"markdown","source":"<li><a href=\"#toc\">Table of contents</a></li>","metadata":{}},{"cell_type":"markdown","source":"<a id='ovrft'></a>","metadata":{}},{"cell_type":"markdown","source":"#### Overfitting\n\nSimply put, overfitting is when the classifier learns the training data by heart and generates excellent predictions results for that data but can't reproduce such performance on the test/validation data; this is usually referred to as poor generalization performance.\n \nWhy does this happen?\n- Unbalanced and unrepresentative training data\n- Too many features to learn from\n- Un-tuned classifier\n- Bad features that does not represent the underlying problem\n\nWhy is it problematic?\n- Degraded performance on unseen data\n- Wasted efforts, the time and resources spent on building a model is not fully utilized because of poor generalization performance.\n\nHow to handle overfitting?\n- First be conscious that the model is overfitting\n- Cross-Validate\n- Hyperparameter tuning, pruning and regularization.\n- Revisit approach\n- Feature selection/ limiting number of features available for training\n- Maintain a balanced and representative training set\n\nThe above are not exhaustive lists of all reasons of overfitting or counteracts because it really depends on the problem at hand. Should we be worried that the model overfits? Yes, but in the context of how well it performs on unseen data. Generalization is key measure of model performance, we should be aiming to produce a model with as much generalizability as possible and be conscious about when the model overfits and try to reduce its impact; trying to avoid over-fitting completely might lead to under-fitting, where we regularize too much and fail to learn relevant information. \n\nWe are not going to care much about overfitting issue in this project, but it had to be mentioned as a sub-topic in modeling because of its importance; next we going to explore the effect of overfitting on both training and test results and how some of the earlier mentioned counteracts can help control overfitting specifically pruning and maintaining a representative training set.","metadata":{}},{"cell_type":"code","source":"# analysing overfitting effect based on tree depth\nmax_d = [5,10,14,18]\n\nfor d in max_d:\n    # model\n    rf = RandomForestClassifier(max_depth = d, class_weight='balanced', random_state = 42)\n    # fit \n    rf.fit(X_train_mod_re_sf, y_train_re)\n    # PR_AUC of train-set\n    train_pred = rf.predict_proba(X_train_mod_re_sf)    \n    pr_precision, pr_recall, pr_thresh = precision_recall_curve(y_train_re, train_pred[:,1])\n    pr_auc_train = auc(pr_recall, pr_precision)\n    # PR_AUC of test-set\n    y_pred = rf.predict_proba(X_test_mod_re_sf)\n    pr_precision, pr_recall, pr_thresh = precision_recall_curve(y_test_re, y_pred[:,1])\n    pr_auc_test = auc(pr_recall, pr_precision)\n    print(f'Precision Recall AUC is {pr_auc_train:.0%} and {pr_auc_test:.0%} for train and test set respectively at max_depth of {d}')","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:07:43.194626Z","iopub.execute_input":"2022-05-19T18:07:43.195096Z","iopub.status.idle":"2022-05-19T18:08:40.496904Z","shell.execute_reply.started":"2022-05-19T18:07:43.195062Z","shell.execute_reply":"2022-05-19T18:08:40.495458Z"},"trusted":true},"execution_count":616,"outputs":[]},{"cell_type":"markdown","source":"We are computing the PR AUC score for predictions of training and testing set separately at different pruning level, setting the max depth of decision tree is a form of pruning which aims at controlling overfitting.\n\nFirst thing to notice is the gap between AUC score for both training and testing sets the less trees the lower the gap, another thing to note is that increasing the number of trees does in fact increase AUC scores for both sets but up to a certain limit; there is no benefit of increasing the max depth beyond 10 as the training score improves while testing score stalls and at the same time if max depth was set to 5 we would be missing on some improvement in prediction results gained by these additional trees despite the fact that it overfits.\n\nJust remember that *'If it fits, it overfits’!*","metadata":{}},{"cell_type":"markdown","source":"<li><a href=\"#toc\">Table of contents</a></li>","metadata":{}},{"cell_type":"markdown","source":"<a id='thrm'></a>","metadata":{}},{"cell_type":"markdown","source":"#### Threshold Moving\n\nOne way to address class imbalance issue even before attempting to rebalance/resample the training set is Threshold Moving. Some classifiers, including decision trees, are capable of providing two types of prediction results being class label [0,1] and/or the probability of class membership. \n\nWhen predicting class membership probability, classification results are displayed in the form of normalized probabilities for each class label, so let’s assume that the predicted probabilities for an unseen sample are [0.3,0.7] this means that there is 30% chance that this sample belong to 1st class, and 70& chance it belongs to the other class being 0 and 1 respectively; for simplicity we reduce this representation to focus only on the +ve class. Going back to our example we would be interested in the [0.7] part of prediction results.\n\nTo convert theses probabilities to class labels [0,1] there need to be some sort of guidance for mapping prediction probabilities to class labels, this is known as classification threshold with a default value of 0.5. This means that any sample in the test set that has predicted probability equal to or greater than 0.5 will be classified as a member of the class to which the prediction probability refer to, this may not be optimal and we can control this by threshold moving. \n\nThreshold moving is simply changing the default classification threshold to whatever works best for problem at hand, going back to our example let assume that the predicted probabilities are [0.6,0.4] which means that this sample belongs to the -ve class since the probability of being a member of +ve class is only 0.4 and the default class assignment threshold is 0.5; by setting the classification threshold to 0.4 this sample will be classified as a member of the +ve class.\n\nBut what threshold to use and why?","metadata":{}},{"cell_type":"code","source":"# Threshold moving - Calculating and plotting Area under the cure for Precision recall and ROC curve\n\n# Precision Recall Curve\nfig  = plt.subplots(figsize=(10, 4))\nplt.tight_layout()\n\nplt.subplot(1, 2, 1) # 2 rows , 2 col , index 1\n\n# calculate f1_score for each threshold\nfscore_rf = (2 * rf_results_re_sf.pr_precision * rf_results_re_sf.pr_recall) / (rf_results_re_sf.pr_precision + rf_results_re_sf.pr_recall)\n\n# optimal(largest) f1_score threshold location on plot\nix_f_rf = np.argmax(fscore_rf)\n\n# plot the precision-recall curves\nno_skill = len(y_test[y_test==1]) / len(y_test)\nplt.plot([0, 1], [no_skill, no_skill], 'r--')\nplt.plot(rf_results_re_sf.pr_recall, rf_results_re_sf.pr_precision, 'b', label = 'AUC = %0.2f' % rf_results_re_sf.pr_auc, linewidth=2)\nplt.scatter(rf_results_re_sf.pr_recall[ix_f_rf], rf_results_re_sf.pr_precision[ix_f_rf], marker='o', color='red', label='Best Threshold')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title(\"PR_AUC RF - Medical Appointments\")\nplt.legend(loc = 6)\nplt.gca().spines[\"top\"].set_alpha(0.0); plt.gca().spines[\"right\"].set_alpha(0.0);\n\nplt.subplot(1, 2, 2)\n\n# ROC Curve\n# calculate the g-mean for each threshold\nj_rf = rf_results_re_sf.tpr - rf_results_re_sf.fpr\n\n# optimal(largest) g-mean threshold location on plot\nix_j_rf = np.argmax(j_rf)\n\nplt.plot(rf_results_re_sf.fpr, rf_results_re_sf.tpr, 'b', label = 'AUC = %0.2f' % rf_results_re_sf.roc_auc, linewidth=2)\nplt.plot([0,1], [0,1], 'r--')\nplt.scatter(rf_results_re_sf.fpr[ix_j_rf], rf_results_re_sf.tpr[ix_j_rf], marker='o', color='red', label='Best Threshold')\nplt.legend(loc='lower right')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title(\"ROC_AUC RF - Medical Appointments\")\nplt.gca().spines[\"top\"].set_alpha(0.0); plt.gca().spines[\"right\"].set_alpha(0.0);","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:08:40.498469Z","iopub.execute_input":"2022-05-19T18:08:40.499026Z","iopub.status.idle":"2022-05-19T18:08:41.077551Z","shell.execute_reply.started":"2022-05-19T18:08:40.498981Z","shell.execute_reply":"2022-05-19T18:08:41.076345Z"},"trusted":true},"execution_count":617,"outputs":[]},{"cell_type":"markdown","source":"First step in threshold moving is to select a threshold, we may have already a set of known thresholds that we would like to test or we can select a threshold that optimize a given performance measure. The above two plots represents AUC for PR and ROC curves calculated at different classification thresholds of random forest trained on selected features, we note that AUC is 71 and 91 for PR and ROC curve respectively and these are the results of earlier predictions based on 0.5 classification threshold.\n\nThe red dots correspond to location of optimal classification threshold that maximizes a given performance measure, F-measure (f1 score) is used as the performance measure optimization for PR Curve while Geometric mean is chosen for ROC Curve. The Geometric Mean or G-Mean is a metric for imbalanced classification that, if optimized, will seek a balance between the true positive rate (tpr) and the false positive rate (fpr). G-Mean = sqrt(tpr * (1-fpr)) or (tpr - fpr) for faster calculation according to [Youden’s J statistic](https://en.wikipedia.org/wiki/Youden%27s_J_statistic)\n\nWe are particularly interested in PR Curve because unlike the ROC Curve, it focuses on the performance of a classifier on the positive (minority class) only and we can observe this in the differences between each curve's AUC scores being very high for ROC curve and lower for PR curve, reason being imbalanced nature of dataset where ROC metrics (tpr and fpr) are shadowed by the high number of samples for the -ve examples and the classifiers' performance on those examples compared to +ve ones.\n\nIf we are interested in a threshold that results in the best balance of precision and recall, then this is the same as optimizing the F-measure that summarizes the harmonic mean of both measures. Selecting threshold that maximize f1 score is not always the best choice cost wise, also that best balance between precision and recall sometimes is not required as the cost of low recall outweigh that of low precision.\n\nEach point on these curves corresponds to a different classification threshold and a separate confusion matrix, knowing associated costs of misclassification errors can help us make guided decision on which threshold to choose; we just have to choose the confusion matrix that would imply the cost matrix we are looking. We might also want to test more thresholds apart from those on the curves.","metadata":{}},{"cell_type":"code","source":"# Threshold moving - Calculating costs of classification errors at different decision threshold \n\n# choosing a cut-off threshold\n# here we are selecting the earlier calculated optimum threshold that maximize F-measure and some others lower/higher\n# thresholds to compare results\ncutoff = [rf_results_re_sf.pr_thresh[ix_f_rf - 1000], rf_results_re_sf.pr_thresh[ix_f_rf - 800], rf_results_re_sf.pr_thresh[ix_f_rf - 600], \n          rf_results_re_sf.pr_thresh[ix_f_rf - 400], rf_results_re_sf.pr_thresh[ix_f_rf], rf_results_re_sf.pr_thresh[ix_f_rf + 400], \n          rf_results_re_sf.pr_thresh[ix_f_rf + 600], rf_results_re_sf.pr_thresh[ix_f_rf + 800], rf_results_re_sf.pr_thresh[ix_f_rf + 1000]]\n\n# Instantiate empty list to hold performance metrics of different classification threshold\nmisclss_err = []\n\nfor c in cutoff:\n    \n    # Converting probabilities to class labels using new threshold\n    y_pred_classes = np.zeros_like(rf_results_re_sf.pred_p)             # initialize a matrix full with zeros\n    y_pred_classes[np.array(rf_results_re_sf.pred_p) > c] = 1           # add a 1 if the cutoff was breached\n    \n    # Calculate performance metrics for the a given threshold\n    pr_precision, pr_recall, _ = precision_recall_curve(y_test, y_pred_classes)\n    pr_f1, pr_auc = f1_score(y_test, y_pred_classes), auc(pr_recall, pr_precision)\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred_classes)\n    roc_auc = auc(fpr, tpr)\n    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_classes).ravel()\n    err_rate = (fp+fn) / len(y_pred_classes)\n    \n    # update list\n    misclss_err.append((tn, fp, fn, tp, err_rate.round(4), pr_precision[1].round(3), pr_recall[1].round(3), pr_f1.round(3), pr_auc.round(3), \n                        roc_auc.round(3), c.round(5) , y_pred_classes))\n\n# Display results of each classification threshold\nmisclss_err_df = pd.DataFrame([i[0:11] for i in misclss_err], columns=['tn', 'fp', 'fn', 'tp', 'err_rate', 'precision', 'recall', 'f1-score', 'pr_auc', 'roc_auc',\n                                                                       'threshold'])    \n\n# assuming different cost values for tp / fn\nmisclss_err_df['fp_cost'], misclss_err_df['fn_cost'] = misclss_err_df.fp * 10, misclss_err_df.fn * 100\nmisclss_err_df['total_misclss_cost'] = misclss_err_df.fp_cost + misclss_err_df.fn_cost\n\nmisclss_err_df","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:08:41.079208Z","iopub.execute_input":"2022-05-19T18:08:41.079559Z","iopub.status.idle":"2022-05-19T18:08:41.824360Z","shell.execute_reply.started":"2022-05-19T18:08:41.079526Z","shell.execute_reply":"2022-05-19T18:08:41.823291Z"},"trusted":true},"execution_count":618,"outputs":[]},{"cell_type":"code","source":"# Threshold that maximize f-score\nrf_results_re_sf.pr_thresh[ix_f_rf]","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:08:41.825581Z","iopub.execute_input":"2022-05-19T18:08:41.825885Z","iopub.status.idle":"2022-05-19T18:08:41.832815Z","shell.execute_reply.started":"2022-05-19T18:08:41.825853Z","shell.execute_reply":"2022-05-19T18:08:41.831584Z"},"trusted":true},"execution_count":619,"outputs":[]},{"cell_type":"markdown","source":"Second step is to assess classification performance at the selected thresholds; here we've made such assessment using different classification thresholds, including the one that maximize our selected performance measure (F-measure).\n\nThe above table displays performance metrics at different classification thresholds, along with associated hypothetical cost of total misclassification errors that is equal to false positive and false negative predictions multiplied by corresponding monetary value. We've established the monetary value of false negative predictions (predictions = show, actual = ‘no show’) to be 10 times more than those of false positive ones (predictions = ‘no show’, actual = show) reason being that usually the costs associated with false negative predictions in imbalanced classification problems outweigh that of false positive ones.\n\nClassification thresholds are sorted in an ascending order with first row (index 0) displaying results of very low threshold, a conservative approach where we classify everything above 27% as being a ‘no show’; this result in a higher recall of +ve class and the lowest misclassification error cost among the selected thresholds. The 5th row (index 4) displays results of the classification threshold that maximize F-measure (f1 score). \n\nWe can note that misclassification error hypothetical costs tends to be higher at higher classification thresholds due to the fact that recall of +ve class is lower and vice versa; similarly is error rate its decreasing due to difference in sample size of each class and the imbalanced nature of the dataset but collectively the decline have an inverse effect on total misclassification cost given the significance of one class (+ve) over the other (-ve).\n\nSo which threshold to choose? It really depends on what we want to achieve with our classification task, costs associated with misclassification errors can be a starting point to compare different thresholds. Regardless of the threshold we will choose and for this approach to work properly, we’ll need to get a proper estimate of the costs associated with each type of misclassification error and we’ll need to make sure that the percentages of positive cases and negative cases match those the model will see at deployment.\n\nSummary:\n\n- Associated costs of different confusion matrix need to be accounted for when assessing model performance\n- Sometimes having high recall on +ve class outweigh any other performance measure.\n- There are many techniques that may be used to address an imbalanced classification problem; perhaps the simplest approach is to change the decision threshold.","metadata":{}},{"cell_type":"markdown","source":"<li><a href=\"#toc\">Table of contents</a></li>","metadata":{}},{"cell_type":"markdown","source":"<a id='kfcv'></a>","metadata":{}},{"cell_type":"markdown","source":"#### K-fold cross validation\n\nSo far we've been testing the model using fixed training/testing sets while addressing class imbalance issue on both classifier level (class_weight hyperparameter) and data level (collecting more sample from both classes).\n\nNext we explore k-fold cross validation where the model is tested using different train/test sets.","metadata":{}},{"cell_type":"code","source":"# Cross validation - Instantiating folds\n\n# Copies to modify\nX_train_mod_re_cv = X_train_mod_re.copy()\nX_test_mod_re_cv = X_test_mod_re.copy()\n\n# Reattach target for CV split\nX_train_mod_re_cv['target'] = y_train_re\n\n# k-fold splits\nX_train_mod_re_cv[\"kfold\"] = -1\n\nX_train_mod_re_cv = X_train_mod_re_cv.sample(frac=1).reset_index(drop=True)\n\ny_train_re_cv = X_train_mod_re_cv.target\n\nkf = StratifiedKFold()\n\nfor f, (t_, v_) in enumerate(kf.split(X = X_train_mod_re_cv, y = y_train_re_cv)):\n    X_train_mod_re_cv.loc[v_, 'kfold'] = f\n    \nX_train_mod_re_cv.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:08:41.834333Z","iopub.execute_input":"2022-05-19T18:08:41.834800Z","iopub.status.idle":"2022-05-19T18:08:41.942175Z","shell.execute_reply.started":"2022-05-19T18:08:41.834750Z","shell.execute_reply":"2022-05-19T18:08:41.941158Z"},"trusted":true},"execution_count":620,"outputs":[]},{"cell_type":"code","source":"# Class distribution per each fold\nX_train_mod_re_cv.groupby('kfold').target.value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:08:41.945092Z","iopub.execute_input":"2022-05-19T18:08:41.945463Z","iopub.status.idle":"2022-05-19T18:08:41.961956Z","shell.execute_reply.started":"2022-05-19T18:08:41.945428Z","shell.execute_reply":"2022-05-19T18:08:41.960958Z"},"trusted":true},"execution_count":621,"outputs":[]},{"cell_type":"markdown","source":"Now we test model based on cross validation folds. For fold 0 the model is trained on folds 1-4 and tested on fold 0, the process is repeated in the same manner across the 5 folds/iterations (fold 1 train model on fold 0 and 2-4 then test on fold 1, and so on).\n\nWe using stratified k-fold as the data set is imbalanced and we want to have equal representation of both classes in each fold.","metadata":{}},{"cell_type":"code","source":"# Cross validation - Train and evaluate\n\ndef train_eval_cv(fold, df, model, eval_func):\n    # training data is where kfold is not equal to provided fold\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    \n    # validation data is where kfold is equal to provided fold\n    df_test = df[df.kfold == fold].reset_index(drop=True)\n    \n    # instantiate train/test features/labels\n    x_train = df_train.drop(['target', 'kfold'], axis=1)\n    y_train = df_train.target\n    \n    x_test = df_test.drop(['target', 'kfold'], axis=1)\n    y_test = df_test.target\n    \n    # evaluate model\n    eval_func(model, x_train, y_train, x_test, y_test)\n\n# Model evaluation CV\nfold = [0,1,2,3,4]\n\nmodel = RandomForestClassifier(class_weight='balanced', random_state = 42)\n\nfor f in fold:\n    print('-' * 40, '\\n', f'Fold {f + 1} results:', '\\n', '-' * 40)\n    train_eval_cv(f, X_train_mod_re_cv, model, train_eval)\n    print('\\n', '-' * 40, '\\n')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-19T18:08:41.963921Z","iopub.execute_input":"2022-05-19T18:08:41.964576Z","iopub.status.idle":"2022-05-19T18:09:56.277889Z","shell.execute_reply.started":"2022-05-19T18:08:41.964525Z","shell.execute_reply":"2022-05-19T18:09:56.276672Z"},"trusted":true},"execution_count":622,"outputs":[]},{"cell_type":"markdown","source":"How come CV results are better than fixed train/test set? Because we have leakage!\n\nWe've instantiated folds after target encoding so train sets within folds 'knows' about target labels of test sets, let’s fix this!","metadata":{}},{"cell_type":"code","source":"# Target encoding within Cross validation folds\n\ndef train_eval_cv(fold, df, model, key, target_encode_func, eval_func, target = None,\n                  custom_encoding = True, log_trans = False, with_weight = True, drop_cat_feats = False, cat_cols = None):\n  \n    # training data is where kfold is not equal to provided fold\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    \n    # validation data is where kfold is equal to provided fold\n    df_test = df[df.kfold == fold].reset_index(drop=True)\n    \n    # instantiate train/test features/labels\n    x_train = df_train.drop([target, 'kfold'], axis=1)\n    y_train = df_train[target]\n    \n    x_test = df_test.drop([target, 'kfold'], axis=1)\n    y_test = df_test[target]\n    \n    # Target Encode\n    if custom_encoding:\n        target_encode_func(key, x_train, x_test, y_train, log_trans = log_trans, with_weight = with_weight)\n    else:\n        target_encode_func.fit(x_train, y_train)\n        x_train = target_encode_func.transform(x_train)\n        x_test = target_encode_func.transform(x_test)\n        \n    if drop_cat_feats:\n        # Remove cat features\n        x_train = x_train.drop(cat_cols, axis = 1).copy()\n        x_test = x_test.drop(cat_cols, axis = 1).copy()\n\n    # evaluate model\n    eval_func(model, x_train, y_train, x_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:17:28.033927Z","iopub.execute_input":"2022-05-19T18:17:28.034380Z","iopub.status.idle":"2022-05-19T18:17:28.046684Z","shell.execute_reply.started":"2022-05-19T18:17:28.034346Z","shell.execute_reply":"2022-05-19T18:17:28.044951Z"},"trusted":true},"execution_count":623,"outputs":[]},{"cell_type":"code","source":"# Instantiate folds\n\n# copies to modify\ndf_split_re_cv = df_split_re.copy()\n\n# remove unnecessary cols\ndf_split_re_cv = df_split_re_cv.drop(['patient_id', 'appointment_id', 'gender', 'scheduling_day', 'appointment_day', \n                                      'time_stamp_sch_sample_count', \n                                      'sch_time_sample_count', 'neighbourhood_sample_count', \n                                      'nbr_sch_time_sample_count', 'nbr_elapsed_days_sample_count', \n                                      'n'], axis=1)\n\n# k-fold splits\ndf_split_re_cv[\"kfold\"] = -1\n\ndf_split_re_cv = df_split_re_cv.sample(frac=1).reset_index(drop=True)\n\ny_train_cv = df_split_re_cv.no_show\n\nkf = StratifiedKFold()\n\nfor f, (t_, v_) in enumerate(kf.split(X = df_split_re_cv, y = y_train_cv)):\n    df_split_re_cv.loc[v_, 'kfold'] = f\n    \ndf_split_re_cv.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:17:39.144206Z","iopub.execute_input":"2022-05-19T18:17:39.144933Z","iopub.status.idle":"2022-05-19T18:17:39.474889Z","shell.execute_reply.started":"2022-05-19T18:17:39.144894Z","shell.execute_reply":"2022-05-19T18:17:39.474073Z"},"trusted":true},"execution_count":624,"outputs":[]},{"cell_type":"code","source":"# Class distribution per fold\ndf_split_re_cv.groupby('kfold').no_show.value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:17:57.517799Z","iopub.execute_input":"2022-05-19T18:17:57.518590Z","iopub.status.idle":"2022-05-19T18:17:57.536866Z","shell.execute_reply.started":"2022-05-19T18:17:57.518535Z","shell.execute_reply":"2022-05-19T18:17:57.535968Z"},"trusted":true},"execution_count":625,"outputs":[]},{"cell_type":"code","source":"# Model evaluation k-fold cross validation\nfold = [0,1,2,3,4]\n\nencoder = target_encode_feat_mapping\n\n# # play around with different encoders\n# encoder = ce.TargetEncoder(min_samples_leaf = 10)\n\nmodel = RandomForestClassifier(random_state = 42)\n\nkey = ['time_stamp_sch', 'sch_time', 'neighbourhood', 'nbr_sch_time', 'nbr_elapsed_days']\n\ncat_cols = cat_cols_re[5:10]\n\nfor f in fold:\n    print('-' * 40, '\\n', f'Fold {f + 1} results:', '\\n', '-' * 40)\n    train_eval_cv(f, df_split_re_cv, model, key, encoder, train_eval, target = 'no_show',\n                  custom_encoding = True, log_trans = True, drop_cat_feats = True, cat_cols = cat_cols)\n    print('\\n', '-' * 40, '\\n')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-19T18:18:43.574327Z","iopub.execute_input":"2022-05-19T18:18:43.574692Z","iopub.status.idle":"2022-05-19T18:20:16.684403Z","shell.execute_reply.started":"2022-05-19T18:18:43.574661Z","shell.execute_reply":"2022-05-19T18:20:16.683187Z"},"trusted":true},"execution_count":628,"outputs":[]},{"cell_type":"markdown","source":"Target encoding at each folds now portrays less optimistic performance than when target encoding was done wrongfully before splits. Overall, K-fold cross validation using full data displays consistent performance across folds similar to what we've seen earlier on fixed train/test split using all features.\n\nNote that we are applying target encoding at each fold but following the same principles we used when applying at fixed train/test split; this is not to be confused with K-fold target encoding which is a different method of target encoding aiming at reducing overfitting where target scores of each fold are derived from the other folds.\n\nUsually it's better to use k-fold cross validation than a fixed train/test set to gain some comfort on the model's generalized performance, another way to assess generalization performance is to visually examine classification decision boundaries.\n\nBut what is a Decision Boundary?","metadata":{}},{"cell_type":"markdown","source":"<li><a href=\"#toc\">Table of contents</a></li>","metadata":{}},{"cell_type":"markdown","source":"<a id='vdb'></a>","metadata":{}},{"cell_type":"markdown","source":"#### Visualizing Decision Boundary","metadata":{}},{"cell_type":"code","source":"# plotting samples of train data points\n\n# full data\nX = X_train_mod_re_sf.append(X_test_mod_re_sf)\ny = y_train_re.append(y_test_re)\n\n# Scale data for PCA\n# scaler = StandardScaler()\n# scaler = RobustScaler()\nscaler = PowerTransformer()\n\nX_scaled = pd.DataFrame(scaler.fit_transform(X), index = X.index, columns = X.columns)\n\n# PCA for 2D plotting\npca = PCA(n_components=2, random_state = 45)\nXreduced = pca.fit_transform(X_scaled)\n\nprint('Explained Variance Ratio for PC1 and PC2 respectively', np.round(pca.explained_variance_ratio_, 2))\n\npca_df_vis = pd.DataFrame(np.column_stack((Xreduced, y)), columns=['PC1', 'PC2', 'no_show'], index = X_scaled.index)\n\ndef make_meshgrid(x, y, h=.02):\n    x_min, x_max = x.min() - 1, x.max() + 1\n    y_min, y_max = y.min() - 1, y.max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    return xx, yy\n\ndef plot_contours(ax, clf, xx, yy, model = '', **params):\n    if model == 'svm':\n        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    else:\n        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n    Z = Z.reshape(xx.shape)\n    out = ax.contourf(xx, yy, Z, **params)\n    return out\n\n# Set-up grid for plotting.\nX0, X1 = Xreduced[:, 0], Xreduced[:, 1]\nxx, yy = make_meshgrid(X0, X1)\n\n# Plot\nfig, ax1= plt.subplots(figsize=(20, 10))\n\n# RandomForest\nmodel = RandomForestClassifier(max_depth = 15, class_weight='balanced', random_state = 42)\n\n# train model using same data used in latest model as we are visualising the boundaries of that model \nclf = model.fit(pca_df_vis[pca_df_vis.index.isin(X_train_mod_re_sf.index)][['PC1', 'PC2']].values, \n                y[y.index.isin(y_train_re.index)])\n\n# plot mesh grid\nplot_contours(ax1, clf, xx, yy, cmap='RdYlBu', alpha=0.8)\n\n# plot sample of train data\ntrain_sample = pca_df_vis[pca_df_vis.index.isin(X_train_mod_re_sf.index)].sample(1000, random_state = 42).sort_index()\nlabels_tr_sample = y[y.index.isin(train_sample.index)].sort_index()\n\n_ = ax1.scatter(train_sample.PC1, train_sample.PC2, c =  labels_tr_sample , s = 80,  edgecolors = 'k', cmap = 'RdYlBu')\n\n# # plot +ve samples only\n# train_sample = pca_df_vis[(pca_df_vis.index.isin(X_train_mod_re_sf.index)) & \n#                           (pca_df_vis.no_show == 1)].sort_index()\n# labels_tr_sample = y[y.index.isin(train_sample.index)].sort_index()\n\n# _ = ax1.scatter(train_sample.PC1, train_sample.PC2, c =  'blue' , s = 80,  edgecolors = 'k')\n\n# Decoration\nax1.set_ylabel('PC2'); ax1.set_xlabel('PC1')\nax1.set_title('Random Forest - Decision boundary', fontsize = 15)\n\nplt.legend(handles = _.legend_elements()[0], labels = ['Show', 'No_show'], loc=4, fontsize = 'large')\nplt.tight_layout();","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:20:16.686549Z","iopub.execute_input":"2022-05-19T18:20:16.687026Z","iopub.status.idle":"2022-05-19T18:20:40.968773Z","shell.execute_reply.started":"2022-05-19T18:20:16.686976Z","shell.execute_reply":"2022-05-19T18:20:40.964058Z"},"trusted":true},"execution_count":629,"outputs":[]},{"cell_type":"markdown","source":"What is a decision boundary/surface?\n\nIt's the assignment of class labels along the feature space, color shades represent predicted probabilities of class membership for a given sample where reddish regions mark -ve class labels and bluish regions mark the +ve ones. Color intensity signals the classifier's confidence in class membership predictions for given sample; the darker the higher probability of being a member of the class for which the color represents.\n\nIn order to create this plot we draw a meshgrid (background) of prediction probabilities then we plot the desired train/test data points to know where each points fits and how the classifier 'sees' them; in the above plot we are plotting only a sample of training data points to have a less crowded plot and we are using PCA for visualizing the feature space in a 2D plot. Note that these 2 PCs represent only 35% of the variance but we can conclude the following:\n\n- We still have overlapping issues, despite the fact that 'no show' appointments tends to be more at PC1 > 0. It should be apparent that because of this overlap, any decision boundary will necessarily misclassify some observations fairly often and this is what we actually saw in classification results having fairly large amount of false positive/negative classifications.\n\n- Decision boundary is fairly complicated with several signs of overfitting; we are using trees of depth 15 and can observe carved decision boundaries around some data point. The color shades of decision boundaries follow the respective class label intensity in any given region; however, complicated boundaries usually do not generalize well on unseen data resulting in degraded performance. Complexity of the model depends on the number of features used and the level of regularization/pruning applied, there is no ‘best’ measurement of complexity as it varies from one problem to another but in general we should aim for a less complicated model that generalize well on unseen data\n\n\nWe'll next look at the decision boundaries of samples from test data and examine the characteristics of adjacent and disjoint data points to understand what distinguish these appointments from one another and what should be our next steps.","metadata":{}},{"cell_type":"code","source":"# Plotly interactive visualization in case we want to inspect individual data points\n# we now plot samples of test data points\n\n# meshgrid\nh = .02  # step size in the mesh\nX = pca_df_vis.values # full data\n\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\ny_ = np.arange(y_min, y_max, h)\n\n# contour RF\nfig = subplots.make_subplots(rows=1, cols=1)\n\npred = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]\npred = pred.reshape(xx.shape)\n\nt1 = go.Heatmap(x = xx[0], y = y_, z = pred, colorscale='RdYlBu', showscale=False)\n\n# plot sample of test data\ntest_sample = pca_df_vis[pca_df_vis.index.isin(X_test_mod_re_sf.index)].sample(1000, random_state = 42).sort_index()\nlabels_ts_sample = y[y.index.isin(test_sample.index)].sort_index()\n\nt2 = go.Scatter(x = test_sample.PC1,\n                y = test_sample.PC2,\n                mode = 'markers', showlegend = False, marker = dict(size=10, color = labels_ts_sample, colorscale = 'RdYlBu', \n                                                                    line = dict(color = 'black', width = 1)),\n                hovertemplate = '<br>X</b>: %{x}' + '<br>Y</b>: %{y}<br>'+ '<br>Index: %{text}', text = test_sample.index)\n                   \n                \nfig.append_trace(t1, 1, 1)\nfig.append_trace(t2, 1, 1)\nfig.update_layout(autosize=False, width=1200, height=700)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:20:40.970696Z","iopub.execute_input":"2022-05-19T18:20:40.971023Z","iopub.status.idle":"2022-05-19T18:20:49.479179Z","shell.execute_reply.started":"2022-05-19T18:20:40.970992Z","shell.execute_reply":"2022-05-19T18:20:49.477833Z"},"trusted":true},"execution_count":630,"outputs":[]},{"cell_type":"code","source":"# full features, co-ords (x:-2.4, y:2.9 and x:-2.1, y=3.3) for first two samples and (x:-4.2, y:4.7) for the last one\ndf_split_re.loc[[39621, 3562, 43673]]","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:33:16.771520Z","iopub.execute_input":"2022-05-19T18:33:16.771991Z","iopub.status.idle":"2022-05-19T18:33:16.817498Z","shell.execute_reply.started":"2022-05-19T18:33:16.771944Z","shell.execute_reply":"2022-05-19T18:33:16.816670Z"},"trusted":true},"execution_count":633,"outputs":[]},{"cell_type":"code","source":"# features used to train the model\nX_test_mod_re_sf.loc[[39621, 3562, 43673]]","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:36:32.155389Z","iopub.execute_input":"2022-05-19T18:36:32.155940Z","iopub.status.idle":"2022-05-19T18:36:32.188102Z","shell.execute_reply.started":"2022-05-19T18:36:32.155903Z","shell.execute_reply":"2022-05-19T18:36:32.186967Z"},"trusted":true},"execution_count":634,"outputs":[]},{"cell_type":"code","source":"# prediction results\npredictions_df.loc[[39621, 3562, 43673]][['no_show', 'pred_p', 'pred_c']]","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:39:17.648802Z","iopub.execute_input":"2022-05-19T18:39:17.649290Z","iopub.status.idle":"2022-05-19T18:39:17.665469Z","shell.execute_reply.started":"2022-05-19T18:39:17.649254Z","shell.execute_reply":"2022-05-19T18:39:17.664232Z"},"trusted":true},"execution_count":635,"outputs":[]},{"cell_type":"markdown","source":"First two samples are adjacent and we can observe that most features used in training are somewhat similar except for the target scores which are also similar for the highly discriminating features used by the model for making predictions ('time_stamp_sch'). The last sample is clearly distinct from the first two in most of the features and this justifies its location in the plot from these points, as for predictions the +ve class sample of the first two examples is wrongly classified and this is caused by mainly the following:\n\n- Similarity of features for both examples\n- Class imbalance as this region of the plot is dominated by -ve samples, imbalance issue is also reflected in the levels per categorical features.\n- Scores of highly discriminating features do not portray the fact that they belong to different criteria contrary to what they were designed to do, we are using the exact time stamps captured in 'time_stamp_sch' feature as a mean of highlighting a common criteria belonging to these specific timing (nature of appointment, venue, physician, etc..) that we do not know about but we believe that it has a direct cause of 'no show'; the way we are grouping unique samples currently does not serve this purpose well.\n\nGiven the above, a lot of things that can be done come to one’s mind such as:\n- Redesign training set instead of random splits\n- Under/oversample training set\n- Use only +ve class samples of single appointment patients when resampling the training set\n- Remove some of the -ve class samples from each categorical feature level\n- Change categories grouping scheme\n- Change weight calculation in custom target encoding\n- Other method of feature selection\n- Grid search hyperparameter tuning\n- Calibrate model\n- Test other classifiers\n- Create more features!\n\nWe've seen previously that data points were not well separated, even after engineering some numeric features and cleaning overlapping areas, it is inevitable that there will still be several adjacent points with no clear explanation because we are simply dealing with uninformative data thus classification is influenced by individual behavior which is highly unpredictable.\n\nWhat about adding more features and using different classifier?","metadata":{}},{"cell_type":"markdown","source":"<li><a href=\"#toc\">Table of contents</a></li>","metadata":{}},{"cell_type":"markdown","source":"<a id='mfnc'></a>","metadata":{}},{"cell_type":"markdown","source":"#### More features and a new classifier\n\nRandomForest and [CatBoost](https://catboost.ai/) classifiers are [Ensembles Learners](https://en.wikipedia.org/wiki/Ensemble_learning) each using different method to generate predictions. RandomForest is a [Bootstrap aggregation - Bagging](https://en.wikipedia.org/wiki/Ensemble_learning#Bootstrap_aggregating_(bagging) classifier while CatBoost is, well, a [Boosting](https://en.wikipedia.org/wiki/Ensemble_learning#Boosting) one!\n\n###### Ensemble Learning:\n\nEnsemble methods employ a hierarchy of two algorithms; the low-level algorithm is called a base learner. The base learner is a single machine learning algorithm that gets used for all of the models that will get aggregated into an ensemble. The upper-level algorithm manipulates the inputs to the base learners so that the models they generate are somewhat independent.\n\n###### RandomForest:\n\nRandom forests generate its sequence of models by training them on subsets of the data using decision trees as base learners. The subsets are drawn at random from the full training set, the first model is the first tree, the second model is the average of the first two trees, and the third model is the average of the first three trees and so on.\n\nOne way in which a subset is selected is to randomly sample rows with replacement (bootstrap) so an individual sample may be represented twice in that subset, another random element is that subsets don’t incorporate all the features rather a random subset of the features is also selected. So each tree will be trained on different training subset and using different combination of features used in previous trees and the conclusion reached by the most trees in the ensemble is considered to be the ensemble's overall assessment (aggregation).\n\n###### CatBoost: \n\nCatBoost is a gradient boosting (GB) algorithm which develops an ensemble of tree based models, just like RandomForest. However, each new tree in the ensemble is trained on prediction errors (residuals) of earlier trees rather than training on different subsets of features and samples. We can think of GB as an optimization of the initial prediction by learning from previous errors hence the name Boosting; at the same time prediction errors(residuals) are slowly being reduced at each iteration taking small steps towards the correct predictions hence the name Gradient.\n\nEach tree's contribution to the model is controlled by a value called Learning rate/step size, it scales a given tree's predictions by a small factor(value) that result in a small step in the right direction of minimizing prediction errors. If the steps are too large (using high value of learning rate) the optimization can diverge instead of converging (converge means that as the iterations proceed, the output gets closer and closer to optimal solution). If the step size is too small the process can take too many iterations, thus making the algorithm computationally expensive.\n\nCatBoost is a nice off-the-shelf GB algorithm with default hyperparameter that produce good results, including automatically setting the learning rate. What is unique about CatBoost is the ability to handle categorical features without preprocessing, so you just mark the columns of these features and the algorithm will do the preprocessing steps which include target encoding but encoding is done [differently](https://github.com/catboost/catboost/blob/master/catboost/tutorials/categorical_features/categorical_features_parameters.ipynb) than how we did it earlier.\n\nAnother popular algorithms that handle categorical features directly is [LightGBM](https://lightgbm.readthedocs.io/), but we going to stick to CatBoost for now.","metadata":{}},{"cell_type":"code","source":"# Resampling the dataset into one hour interval\n\n# convert time stamps cols back to datetime and extract scheduling year\ndf_split_re.scheduling_day = pd.to_datetime(df_split_re.scheduling_day)\ndf_split_re.appointment_day = pd.to_datetime(df_split_re.appointment_day)\ndf_split_re['sch_year'] = df_split_re.scheduling_day.dt.year\n\n# build time series and aggregate statistics\nser = df_split_re.groupby(['scheduling_day']).no_show.sum().resample('h').sum().to_frame()\n\nser['total_apps'] = df_split_re.groupby(['scheduling_day']).no_show.count().resample('h').sum()\n\nser_ = df_split_re[['scheduling_day', 'patient_id']].copy()\nser_['rounded_hour'] = ser_.scheduling_day.dt.floor('h')\n\nser = pd.concat([ser, ser_.groupby('rounded_hour').patient_id.nunique()], axis=1).fillna(0)\\\n.rename(columns = {'patient_id':'patients_count'})\n\nser['absence_frequency'] = (ser.no_show / ser.total_apps).fillna(0)\n\nser.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-19T18:40:49.274881Z","iopub.execute_input":"2022-05-19T18:40:49.275293Z","iopub.status.idle":"2022-05-19T18:40:49.640492Z","shell.execute_reply.started":"2022-05-19T18:40:49.275259Z","shell.execute_reply":"2022-05-19T18:40:49.639433Z"},"trusted":true},"execution_count":636,"outputs":[]},{"cell_type":"code","source":"# Filter on high absence frequency time slots, excluding the ones having few scheduled appointments \nser[ser.total_apps > 2].sort_values(['absence_frequency', 'total_apps'], ascending=False).head(20)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-19T18:41:09.764662Z","iopub.execute_input":"2022-05-19T18:41:09.765072Z","iopub.status.idle":"2022-05-19T18:41:09.786815Z","shell.execute_reply.started":"2022-05-19T18:41:09.765035Z","shell.execute_reply":"2022-05-19T18:41:09.785607Z"},"trusted":true},"execution_count":637,"outputs":[]},{"cell_type":"code","source":"# check neighborhood for a selected time slot\ndf_split_re[(df_split_re.sch_day == 25) & (df_split_re.sch_month == 2) & (df_split_re.sch_hour == 17) &\n            (df_split_re.sch_year == 2016)].neighbourhood.unique()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:41:24.817138Z","iopub.execute_input":"2022-05-19T18:41:24.817509Z","iopub.status.idle":"2022-05-19T18:41:24.830184Z","shell.execute_reply.started":"2022-05-19T18:41:24.817477Z","shell.execute_reply":"2022-05-19T18:41:24.829098Z"},"trusted":true},"execution_count":638,"outputs":[]},{"cell_type":"code","source":"# Neighborhood of another selected time slot\ndf_split_re[(df_split_re.sch_day == 25) & (df_split_re.sch_month == 4) & (df_split_re.sch_hour == 17) &\n            (df_split_re.sch_year == 2016)].neighbourhood.unique()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:41:31.103882Z","iopub.execute_input":"2022-05-19T18:41:31.104287Z","iopub.status.idle":"2022-05-19T18:41:31.117512Z","shell.execute_reply.started":"2022-05-19T18:41:31.104252Z","shell.execute_reply":"2022-05-19T18:41:31.116419Z"},"trusted":true},"execution_count":639,"outputs":[]},{"cell_type":"markdown","source":"Apparently scheduling time and day combined does impact 'no show' somehow specially that it is repeated among several different neighborhoods/patients and not an individual behavior, we've already seen similar pattern in timestamps earlier and maybe we can improve predictions creating some additional features within that context.","metadata":{}},{"cell_type":"markdown","source":"<li><a href=\"#toc\">Table of contents</a></li>","metadata":{}},{"cell_type":"markdown","source":"<a id='upr'></a>","metadata":{}},{"cell_type":"markdown","source":"#### Updated prediction results: RandomForest and CatBoost\n\nThis section contains the final dataset of all features used in assessing classification performance for both RandomForest and CatBoost classifiers.","metadata":{}},{"cell_type":"code","source":"# Copies to modify\ndata = df_split_re.copy()\n\ndata.scheduling_day = pd.to_datetime(data.scheduling_day)\ndata.appointment_day = pd.to_datetime(data.appointment_day)\n\n# More Features\ndata['sch_date'] = data.scheduling_day.dt.date.astype('str')\ndata['app_date'] = data.appointment_day.dt.date.astype('str')\ndata['sch_day_hour'] = data.scheduling_day.dt.floor('h').astype('str')\n\n# Combine\ndata['nbr_sch_day'] = (data.neighbourhood + \"_\" + data.sch_date)\ndata['nbr_app_day'] = (data.neighbourhood + \"_\" + data.app_date)\ndata['nbr_sch_day_hour'] = (data.neighbourhood + \"_\" + data.sch_day_hour)\ndata['nbr_sms'] = (data.neighbourhood + \"_\" + data.sms_received.astype('str'))\ndata['nbr_age'] = (data.neighbourhood + \"_\" + data.age.astype('str'))\n\n# To drop cat features laters\ndata.scheduling_day = data.scheduling_day.astype('str')\ndata.appointment_day = data.appointment_day.astype('str')\n\n# Group unique samples\nfeatures = ['sch_date', 'sch_day_hour', 'app_date', 'nbr_sch_day', 'nbr_app_day', 'nbr_sch_day_hour', 'nbr_sms', 'nbr_age']\n\ntarget = 'no_show'\n\ncombine_group(data, features, target, label_encode = True)\n\n# Sort column\nno_show_idx = data.columns.get_loc(\"no_show\")\n\ncolumns_list = data.columns.tolist()\n\ncolumns_sort = columns_list[0:no_show_idx] + columns_list[no_show_idx+1:] + columns_list[no_show_idx : no_show_idx+1]\n\ndata = data[columns_sort]\n\ndata.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:42:12.551642Z","iopub.execute_input":"2022-05-19T18:42:12.552210Z","iopub.status.idle":"2022-05-19T18:42:18.353229Z","shell.execute_reply.started":"2022-05-19T18:42:12.552169Z","shell.execute_reply":"2022-05-19T18:42:18.352235Z"},"trusted":true},"execution_count":640,"outputs":[]},{"cell_type":"code","source":"# update train/test sets with new features\n\n# Copies to modify\nX_train_mod_ = X_train_mod.copy()\nX_test_mod_ = X_test_mod.copy()\n\nnew_feats = ['sch_date', 'sch_day_hour', 'app_date', 'nbr_sch_day', 'nbr_app_day', 'nbr_sch_day_hour', 'nbr_sms', 'nbr_age',\n             'sch_date_', 'sch_day_hour_', 'app_date_', 'nbr_sch_day_', 'nbr_app_day_', 'nbr_sch_day_hour_', 'nbr_sms_', 'nbr_age_']\n\n# update\nX_train_mod_ = pd.concat([X_train_mod_, data[data.index.isin(X_train_mod_.index)][new_feats]], axis = 1, join= 'inner')\nX_test_mod_ = pd.concat([X_test_mod_, data[data.index.isin(X_test_mod_.index)][new_feats]], axis = 1, join= 'inner')\n\nX_train_mod_.head(1)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:46:22.068067Z","iopub.execute_input":"2022-05-19T18:46:22.068645Z","iopub.status.idle":"2022-05-19T18:46:22.374981Z","shell.execute_reply.started":"2022-05-19T18:46:22.068611Z","shell.execute_reply":"2022-05-19T18:46:22.373950Z"},"trusted":true},"execution_count":641,"outputs":[]},{"cell_type":"code","source":"# Target Encode\n\n# Copies to modify\nX_train_mod_rf = X_train_mod_.copy()\nX_test_mod_rf = X_test_mod_.copy()\n\n# cols to encode\nkey = ['sch_date', 'sch_day_hour', 'app_date', 'nbr_sch_day', 'nbr_app_day', 'nbr_sch_day_hour', 'nbr_sms', 'nbr_age']\n\ntarget_encode_feat_mapping(key, X_train_mod_rf, X_test_mod_rf, y_train_re, log_trans = True)\n\n# Remove cat features features\nX_train_mod_rf = X_train_mod_rf.drop(X_train_mod_rf.select_dtypes('object').columns, axis=1).copy()\nX_test_mod_rf = X_test_mod_rf.drop(X_test_mod_rf.select_dtypes('object').columns, axis=1).copy()\n\nX_test_mod_rf.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-19T18:46:40.919786Z","iopub.execute_input":"2022-05-19T18:46:40.920193Z","iopub.status.idle":"2022-05-19T18:46:43.270314Z","shell.execute_reply.started":"2022-05-19T18:46:40.920156Z","shell.execute_reply":"2022-05-19T18:46:43.268993Z"},"trusted":true},"execution_count":642,"outputs":[]},{"cell_type":"markdown","source":"<a id='impr'></a>","metadata":{}},{"cell_type":"code","source":"rf_results_ = train_eval(RandomForestClassifier(class_weight='balanced', random_state = 42), X_train_mod_rf, y_train_re, \n                                                                                             X_test_mod_rf, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:47:21.124031Z","iopub.execute_input":"2022-05-19T18:47:21.124406Z","iopub.status.idle":"2022-05-19T18:47:47.596584Z","shell.execute_reply.started":"2022-05-19T18:47:21.124371Z","shell.execute_reply":"2022-05-19T18:47:47.595545Z"},"trusted":true},"execution_count":643,"outputs":[]},{"cell_type":"code","source":"rf_plot(rf_results_.model, X_test_mod_rf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Indeed the new features did improve classification performance and we can see couple of them at the top of the chart 'nbr_sch_day_hour_score' and 'nbr_age_score', us now at 75% and 68% for precision and recall compared to 68% and 64% previously.","metadata":{}},{"cell_type":"markdown","source":"<li><a href=\"#prevr\">Earlier Results</a></li>","metadata":{}},{"cell_type":"code","source":"# Feature selection\nufs = UnivariateFeatureSelction(n_features = 20, problem_type = \"classification\", scoring = \"mutual_info_classif\")\n\nfeatures = X_train_mod_rf.columns\n\nufs.fit(X_train_mod_rf[features], y_train_re.values.ravel())\n\nselected_features = ufs.return_cols(X_train_mod_rf[features])\n\nselected_features","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:49:32.288982Z","iopub.execute_input":"2022-05-19T18:49:32.289459Z","iopub.status.idle":"2022-05-19T18:50:24.462708Z","shell.execute_reply.started":"2022-05-19T18:49:32.289420Z","shell.execute_reply":"2022-05-19T18:50:24.461713Z"},"trusted":true},"execution_count":644,"outputs":[]},{"cell_type":"markdown","source":"<a id='impr2'></a>","metadata":{}},{"cell_type":"code","source":"# Train on selected features only\nX_train_mod_sf = X_train_mod_rf[selected_features]\nX_test_mod_sf = X_test_mod_rf[selected_features]\n\nrf_results_sf = train_eval(RandomForestClassifier(max_depth = 10, min_samples_leaf = 5, class_weight='balanced', random_state = 42), \n                           X_train_mod_sf, y_train_re, X_test_mod_sf, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:50:24.464202Z","iopub.execute_input":"2022-05-19T18:50:24.464517Z","iopub.status.idle":"2022-05-19T18:50:40.518872Z","shell.execute_reply.started":"2022-05-19T18:50:24.464485Z","shell.execute_reply":"2022-05-19T18:50:40.517677Z"},"trusted":true},"execution_count":645,"outputs":[]},{"cell_type":"markdown","source":"Another improvement compared to earlier results, now we at 64% and 82% for precision and recall compared to 61% and 79%.\n\nNote that the training data for RandomForest includes both label encoded and target encoded cat features, we also restricted the max depth to only 10 as we noted earlier that beyond that no improvement in results were noted.\n\nNow we prepare the datasets for CatBoost classifier using only numeric and categorical features without any encodings.","metadata":{}},{"cell_type":"markdown","source":"<li><a href=\"#prevr2\">Earlier Results</a></li>","metadata":{}},{"cell_type":"code","source":"# copies to modify\nX_train_cat = X_train_mod_.copy()\nX_test_cat = X_test_mod_.copy()\n\n# remove target/label encoded cat feats\ndrop_cols = ['time_stamp_sch_', 'sch_time_', 'neighbourhood_', 'nbr_sch_time_', 'nbr_elapsed_days_', 'sch_date_', \n             'sch_day_hour_', 'app_date_', 'nbr_sch_day_', 'nbr_app_day_', 'nbr_sch_day_hour_', 'nbr_sms_', 'nbr_age_', \n             'time_stamp_sch_score', 'sch_time_score', 'neighbourhood_score', 'nbr_sch_time_score', 'nbr_elapsed_days_score',\n             ]\n\nX_train_cat.drop(drop_cols, inplace=True, axis=1)\nX_test_cat.drop(drop_cols, inplace=True, axis=1)\n\n# Mark cat columns\ncat_cols_cat_boost = X_train_cat.select_dtypes('object').columns\n\nX_train_cat.head(1)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:54:10.928166Z","iopub.execute_input":"2022-05-19T18:54:10.928636Z","iopub.status.idle":"2022-05-19T18:54:11.169124Z","shell.execute_reply.started":"2022-05-19T18:54:10.928601Z","shell.execute_reply":"2022-05-19T18:54:11.168064Z"},"trusted":true},"execution_count":646,"outputs":[]},{"cell_type":"code","source":"# Check Cardinality of all cat features in both sets\nfor f in cat_cols_cat_boost:\n    print(f'Cardinality of {f} in train set: {X_train_cat[f].nunique()}', '------', \n          f'and in test set: {X_test_cat[f].nunique()}')","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:54:40.798496Z","iopub.execute_input":"2022-05-19T18:54:40.798906Z","iopub.status.idle":"2022-05-19T18:54:41.316893Z","shell.execute_reply.started":"2022-05-19T18:54:40.798867Z","shell.execute_reply":"2022-05-19T18:54:41.315655Z"},"trusted":true},"execution_count":647,"outputs":[]},{"cell_type":"code","source":"# CatBoost\n\n# creating Pools\ntrain_set = cb.Pool(data = X_train_cat, label= y_train_re, cat_features = cat_cols_cat_boost.values)\neval_set = cb.Pool(data = X_test_cat, label= y_test, cat_features = cat_cols_cat_boost.values)\n\n# Train\nmodel = cb.CatBoostClassifier(random_seed = 42, early_stopping_rounds=5, one_hot_max_size = 24000, \n                              custom_metric = ['F1', 'Recall'])\n\nmodel.fit(train_set, eval_set = eval_set, verbose = 50)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-19T18:55:53.938870Z","iopub.execute_input":"2022-05-19T18:55:53.939434Z","iopub.status.idle":"2022-05-19T18:56:14.654899Z","shell.execute_reply.started":"2022-05-19T18:55:53.939384Z","shell.execute_reply":"2022-05-19T18:56:14.653672Z"},"trusted":true},"execution_count":649,"outputs":[]},{"cell_type":"code","source":"# Compare best scores for both sets \nmodel.get_best_score()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:57:06.303862Z","iopub.execute_input":"2022-05-19T18:57:06.304587Z","iopub.status.idle":"2022-05-19T18:57:06.312462Z","shell.execute_reply.started":"2022-05-19T18:57:06.304535Z","shell.execute_reply":"2022-05-19T18:57:06.311529Z"},"trusted":true},"execution_count":650,"outputs":[]},{"cell_type":"code","source":"# Report full results - CatBoost default parameters using all features and OneHot encoding only\n\ndef scoring(model, features_test, labels_test, eval_set):\n    \n    # Hold predictions of +ve class\n    pred_c = model.predict(features_test)\n    pred_p = model.predict_proba(features_test)[:,1]\n\n    # Display results\n    pr_precision, pr_recall, pr_thresh = precision_recall_curve(labels_test, pred_p)\n    pr_f1, pr_auc = f1_score(labels_test, pred_c), auc(pr_recall, pr_precision)\n\n    curve = utils.get_roc_curve(model, eval_set)\n    fpr, tpr, roc_thresh = curve\n    roc_auc = auc(fpr, tpr)\n\n    print('\\n', '-' * 40, '\\n', 'PR_AUC: ', pr_auc.round(4), '\\n', '-' * 40, '\\n', 'F1 score: ', pr_f1.round(4), '\\n', '-' * 40, '\\n',\n          'ROC_AUC: ', roc_auc.round(4), '\\n', '-' * 40, '\\n', classification_report(labels_test, pred_c, \n                                                                                     target_names=['show', 'no_show']), \n          '-'*40, '\\n', utils.get_confusion_matrix(model, eval_set))\n\nscoring(model = model, features_test = X_test_cat, labels_test = y_test, eval_set = eval_set)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:57:20.389127Z","iopub.execute_input":"2022-05-19T18:57:20.389512Z","iopub.status.idle":"2022-05-19T18:57:20.714315Z","shell.execute_reply.started":"2022-05-19T18:57:20.389480Z","shell.execute_reply":"2022-05-19T18:57:20.713154Z"},"trusted":true},"execution_count":651,"outputs":[]},{"cell_type":"markdown","source":"We've trained the model with original features only, the feature set include un-edited numeric and categorical features (no target encoding involved).\n\nWhat’s unique about CatBoost is how it handles categorical features in raw format; no pre transformation to numeric values is needed as it is done internally upon fitting. First thing that CatBoost do is OneHot encoding cat features that include levels < the predefined threshold, no other encoding is applied to features that are OneHot encoded; Features that are not OneHot encoded are subject to series of target encoding and further combinations.\n\nOur first attempt is to make CatBoost use one-hot encoding only for all our categorical features by setting the 'one_hot_max_size' hyperparameter to 24k as a threshold for applying OneHot encoding (the max categorical feature cardinality in our dataset is 23k). The documentation says, *Use one-hot encoding for all categorical features with a number of different values less than or equal to the given parameter value. Ctrs are not calculated for such features*. There is no clear definition of what does *Ctrs* stand for but let’s assume that it is 'Categorical to Ratios' acronym, as the provided equations in the documentation are a form of target encoding and our understanding for that parameter can also be confirmed [here](https://github.com/catboost/catboost/blob/master/catboost/tutorials/categorical_features/categorical_features_parameters.ipynb)\n\nWe've specified 'early_stopping_rounds', which is an overfitting detector hyperparameter, to be 5 this means that iterations stops when error rate of the test sets stalls for max 5 iterations while that of the train set keeps on decreasing (overfitting starts).\n\nSetting 'custom_metric' to include f1 score and recall is for display purpose only, as no optimization for these two metrics are actually done during training the only metric optimize is the 'logloss' and that is what's displayed in each iteration results.\n\nThe takeaway from this attempt is that:\n- OneHot encoding only did not provide good results compared to earlier ones\n- Even when the model started overfitting early during training, we still got improvement in test results up to a specific number of trees (iterations) beyond which no improvement could be obtained.","metadata":{}},{"cell_type":"code","source":"# CatBoost Feature Importance\nmodel.get_feature_importance(prettified=True)[:10]","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-19T18:57:54.266066Z","iopub.execute_input":"2022-05-19T18:57:54.266436Z","iopub.status.idle":"2022-05-19T18:57:54.283153Z","shell.execute_reply.started":"2022-05-19T18:57:54.266405Z","shell.execute_reply":"2022-05-19T18:57:54.282032Z"},"trusted":true},"execution_count":652,"outputs":[]},{"cell_type":"markdown","source":"Looking at top 10 features in terms of importance, there seems to be agreement between both CatBoost and RandomForest classifiers.","metadata":{}},{"cell_type":"markdown","source":"<a id='pm2'></a>","metadata":{}},{"cell_type":"code","source":"# Disable OneHot Encoding\nmodel = cb.CatBoostClassifier(random_seed = 42, early_stopping_rounds=5, one_hot_max_size = 0,\n                              custom_loss=['F1', 'Recall'])\n\nmodel.fit(train_set, eval_set = eval_set, verbose = 50)\n\n# Compare best socres for both sets\nprint('\\n', '-' * 40, '\\n', model.get_best_score())\n\nscoring(model = model, features_test = X_test_cat, labels_test = y_test, eval_set = eval_set)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:58:09.872070Z","iopub.execute_input":"2022-05-19T18:58:09.872468Z","iopub.status.idle":"2022-05-19T18:59:01.472318Z","shell.execute_reply.started":"2022-05-19T18:58:09.872432Z","shell.execute_reply":"2022-05-19T18:59:01.470876Z"},"trusted":true},"execution_count":653,"outputs":[]},{"cell_type":"markdown","source":"Results are just too good to be true! I'm usually skeptical when test results are too perfect, especially that we know that our model overfits.\n\nThe way train loss is higher than test loss suggests that we should revisit our train test split approach and/or train set design in general; remember that we resampled the training set with all samples of single appointment patients and also we are using a random 80:20 split.\n\nAnother thing to note is that CatBoost does combine categorical features that are not OneHotEncoded, our feature set already include some combinations of categorical features so this may also be contributing to the perfect scores we are getting.\n\nSo first thing to do is to reduce tree depth to 3 down from 6 the default parameter then we cancel further combinations of categorical features, will this affect our perfect scores?","metadata":{}},{"cell_type":"markdown","source":"<li><a href=\"#toc\">Table of contents</a></li>","metadata":{}},{"cell_type":"code","source":"# adjusting tree depth\nmodel = cb.CatBoostClassifier(random_seed = 42, early_stopping_rounds=5, one_hot_max_size = 0, depth = 3,\n                              custom_loss=['F1', 'Recall'])\n\nmodel.fit(train_set, eval_set = eval_set, verbose = 50)\n\n# Compare best socres for both sets \nprint('\\n', '-' * 40, '\\n', model.get_best_score())\n\nscoring(model = model, features_test = X_test_cat, labels_test = y_test, eval_set = eval_set)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T18:59:01.474381Z","iopub.execute_input":"2022-05-19T18:59:01.474859Z","iopub.status.idle":"2022-05-19T18:59:48.870267Z","shell.execute_reply.started":"2022-05-19T18:59:01.474791Z","shell.execute_reply":"2022-05-19T18:59:48.869158Z"},"trusted":true},"execution_count":654,"outputs":[]},{"cell_type":"code","source":"# growing small trees using all features and disabling further combinations\n\nmodel = cb.CatBoostClassifier(random_seed = 42, one_hot_max_size = 0, depth = 3,\n                              max_ctr_complexity = 0, custom_loss=['F1', 'Recall'])\n\nmodel.fit(train_set, eval_set = eval_set, verbose = 50)\n\n# Compare best scores for both sets \nprint('\\n', '-' * 40, '\\n', model.get_best_score())\n\nscoring(model = model, features_test = X_test_cat, labels_test = y_test, eval_set = eval_set)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-19T18:59:48.872119Z","iopub.execute_input":"2022-05-19T18:59:48.872436Z","iopub.status.idle":"2022-05-19T19:00:29.090641Z","shell.execute_reply.started":"2022-05-19T18:59:48.872403Z","shell.execute_reply":"2022-05-19T19:00:29.089558Z"},"trusted":true},"execution_count":655,"outputs":[]},{"cell_type":"code","source":"# CatBoost Feature Importance\nmodel.get_feature_importance(prettified=True)[:10]","metadata":{"execution":{"iopub.status.busy":"2022-05-19T19:00:29.092551Z","iopub.execute_input":"2022-05-19T19:00:29.092889Z","iopub.status.idle":"2022-05-19T19:00:29.109339Z","shell.execute_reply.started":"2022-05-19T19:00:29.092858Z","shell.execute_reply":"2022-05-19T19:00:29.107988Z"},"trusted":true},"execution_count":656,"outputs":[]},{"cell_type":"markdown","source":"So we pruned and restricted further categorical combinations, but results still seems to be too good especially that training set loss is higher than test set! How come we are good at predicting the unknown better than what we do at what we have learned?\n\nIt's because we still have leakage :D\n\nRemember how these 'rare' levels are designed, we first combined each level with the corresponding target value then grouped combined levels having single sample into one category. Our aim was to create a more representative level per categorical feature in light of absence of sufficient +ve samples, there were two problems in such application:\n\n- We combined and grouped features *before* train/test splits, so target values of latter split test set is leaked into training data. The classifier now knows that 'rare' levels have relatively high 'no show' rates which is ok if that was actually the case after doing the splits and transforming the test set not before, even after introducing noise (unseen categories assigned as being also 'rare').\n\n- We are doing *random* train/test split so there is no guarantee that each level is equally represented in both sets for this approach to work correctly. This means that training set is more diverse thus its somewhat 'harder' to predict than the simpler test set version with few categories specially for the features with high importance, additionally, difference in sample size between both sets does affect loss calculation when it comes to gradient boosting.\n\nOne way to address this is to maintain a balanced and representative training set where categories are equally represented in both sets; this is actually really important and mostly overlooked as we are accustomed to train/test split without considering what actually goes into these sets.\n\nWe now need to pause and rethink our approach in light of what we know about the data so far. The most important thing we need to consider is our train/test split and define a *sampling unit*, we can either define our sampling unit to be a patient or time span.\n\nSetting a sampling unit will dictate how to split the data and what features to use, if we are to use patients as a sampling unit then we would be interested in predicting the last appointment of each patient based on historical records, one problem of this approach is that we do not have sufficient historical records for each patient and there are many patients who have single appointments that will be excluded from our model; but this approach actually make sense in a real world application. We would normally expect patients to show up to their appointments, it’s after accumulating several appointments that we begin to rationalize and be able to predict patients' behavior.\n\nIf we decide to use time span as our sampling unit then we would also be interested in predicting the last appointment per each time slot, however, we know that there will be some gaps as our time series is not complete but bear in mind that we are using time features as a reflection of an underlying cause of 'no show' not the fact that it’s the exact time that is the cause of a 'no show'.\n\nThe choice of using last appoint as a test/validation set of whatever sampling unit we select is more intuitive, it doesn’t make sense to have random splits and include in the training set appointments from future dates to predict earlier appointments!\n\nTrain/test split, and training set design in general, is not always discussed or emphasized in most introductory machine learning guides. However, it’s of key importance when it comes to building and assessing predictive models. Now let's try both approaches.","metadata":{}},{"cell_type":"markdown","source":"<a id='rvapp'></a>","metadata":{}},{"cell_type":"markdown","source":"### Revised Approach\n\nIn this section we combine our knowledge gained from earlier attempts and approach the problem from different perspective, building two different models each capturing specific patterns contributing to the 'no show' problem.","metadata":{}},{"cell_type":"code","source":"# Time span as a sampling unit using full data\n\n# Dropping duplicate/identical appointments\n# Selecting columns to identify duplicates\nmask = ml_df[['patient_id','gender','scheduling_day','appointment_day','age', 'neighbourhood','no_show']].copy() \n\n# Mark all +ve samples\nmask['flag'] = mask.no_show.apply(lambda x: 1 if x ==1 else 0)\n\n# Total +ve samples per group, used to capture identical appointments with different class labels\nmask['flag_sum'] = mask.groupby(['patient_id', 'scheduling_day']).flag.transform('sum')\n\n# -ve samples of duplicated appointments that have different class labels\n# flag_sum >= 1 captures +ve samples per group and flag = 0 captures the -ve samples for which a +ve sample exists per the \n# same group\nidx_1 = mask[(mask.flag_sum >=1) & (mask.flag ==0)].index\n\n# duplicate observations to be dropped\nidx_2 = mask[mask.duplicated()].index\n\n# combined List of indices to be dropped\ncom_idx = idx_1.append(idx_2)\n\n# applying filter\nml_df = ml_df[~ml_df.index.isin(com_idx)]\n\n# reset index\nml_df.reset_index(inplace=True, drop=True)\n\n# extracting additional time features\nml_df['time_stamp_sch'] = pd.to_datetime(ml_df.scheduling_day).dt.time.astype('str')\nml_df['sch_time'] = ml_df.time_stamp_sch.apply(lambda x: str(x)[:-3]).astype('str')\n\n# separate categorical and numeric columns\ncat_cols = ml_df.select_dtypes('object').columns\nnum_cols = ml_df.columns[~ml_df.columns.isin(cat_cols)]\n\n# sort data by appointment date\nml_df = ml_df.sort_values(['appointment_day', 'scheduling_day']).copy()\n\nml_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T19:00:29.111047Z","iopub.execute_input":"2022-05-19T19:00:29.111451Z","iopub.status.idle":"2022-05-19T19:00:30.036977Z","shell.execute_reply.started":"2022-05-19T19:00:29.111406Z","shell.execute_reply":"2022-05-19T19:00:30.036013Z"},"trusted":true},"execution_count":657,"outputs":[]},{"cell_type":"code","source":"# Train and test sets\n# using last appointment of each time span as an input to test set\nml_df['last_app'] = ml_df.groupby(['sch_time', 'no_show']).appointment_id.transform('last')\n\ntest_set = ml_df[ml_df.appointment_id == ml_df.last_app].copy()\ntrain_set = ml_df[~ml_df.index.isin(test_set.index)].copy()\n\n# Group and combine then transform\nfeat = ['time_stamp_sch', 'sch_time']\n\nfor f in feat:\n    # Combining best features with target\n    train_set[f'{f}_target'] = (train_set[f'{f}'].astype(str) + '_' + train_set.no_show.astype(str))\n    # number of samples per each combined category\n    train_set[f'{f}_sample_count'] = train_set.groupby(f'{f}_target').no_show.transform('count')\n    \n    # statistics of unique samples (single samples)\n    print(f'\\'{f}\\' unique train samples\\' \\'no_show\\' mean:', round(train_set[train_set[f'{f}_sample_count'] == 1].no_show.mean(),2),\\\n          '---- sample count:', train_set[train_set[f'{f}_sample_count'] == 1].no_show.count())\n    \n    # grouping unique samples under one category\n    train_set.loc[train_set[f'{f}_sample_count'] == 1, f'{f}_target'] = 'rare__'\n    \n    # update count\n    train_set[f'{f}_sample_count'] = train_set.groupby(train_set[f'{f}_target'])['no_show'].transform('count')\n    \n    # Remove association with target\n    train_set[f'{f}_target'] = train_set[f'{f}_target'].apply(lambda x: x[:-2])\n    \n    # mapping risk criteria to test set    \n    mapr = dict(train_set[[f'{f}', f'{f}_target']].values)\n    test_set[f'{f}'] = test_set[f'{f}'].map(mapr).fillna('rare')\n    \n    # drop temp column\n    train_set[f'{f}'] = train_set[f'{f}_target']\n    train_set.drop(f'{f}_target', axis = 1, inplace = True)\n\n    print(f'\\'{f}\\' unique test samples\\' \\'no_show\\' mean:', round(test_set[test_set[f'{f}'] == 'rare'].no_show.mean(),2),\\\n          '---- sample count:', test_set[test_set[f'{f}'] == 'rare'].no_show.count())\n    \n    # label Encode\n    encoder = LabelEncoder()\n    encoder.fit(train_set[f'{f}'])\n    train_set[f'{f}_'] = encoder.transform(train_set[f'{f}'])\n    test_set[f'{f}_'] = encoder.transform(test_set[f'{f}'])\n    \n# update categorical and numeric columns\ncat_cols = train_set.select_dtypes('object').columns\nnum_cols = train_set.columns[~train_set.columns.isin(cat_cols)].drop(train_set.filter(regex = 'count').columns)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-19T19:00:30.038359Z","iopub.execute_input":"2022-05-19T19:00:30.038676Z","iopub.status.idle":"2022-05-19T19:00:31.652142Z","shell.execute_reply.started":"2022-05-19T19:00:30.038643Z","shell.execute_reply":"2022-05-19T19:00:31.650938Z"},"trusted":true},"execution_count":658,"outputs":[]},{"cell_type":"code","source":"# Instantiate features/labels\n\nfeats = ['time_stamp_sch_', 'elapsed_days_sch']\n\nX_train, y_train = train_set[feats], train_set.no_show\nX_test, y_test = test_set[feats], test_set.no_show\n\n# train and evaluate\nrf_results_full = train_eval(RandomForestClassifier(class_weight='balanced', random_state = 42), X_train, y_train, \n                                                                                                 X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T19:00:31.653685Z","iopub.execute_input":"2022-05-19T19:00:31.654090Z","iopub.status.idle":"2022-05-19T19:00:46.325003Z","shell.execute_reply.started":"2022-05-19T19:00:31.654053Z","shell.execute_reply":"2022-05-19T19:00:46.323919Z"},"trusted":true},"execution_count":659,"outputs":[]},{"cell_type":"markdown","source":"So we've selected scheduling time to be our sampling unit and split the data in a way that ensure we are predicting last appointment of any given scheduling day using a single sample of each outcome (show/no show) per day. We've also created our rare categories but this time using train set as a lead while introducing noise categorizing unseen data as also a 'rare' category.\n\nFinally we've used only two features being the exact scheduling time and day difference between scheduling and appointment dates to make predictions, this is to emphasize our earlier observations that time does influence no show somehow for this particular dataset.\n\nNext we try a more logical and closer to a real world application approach using patient as a sampling unit.","metadata":{}},{"cell_type":"code","source":"# patient as a sampling unit using data of patients who have several records\n\n# Encode user ID\nencoder = LabelEncoder()\ndf_hist['patient_id_'] = encoder.fit_transform(df_hist.patient_id)\n\n# use patients having several records\ndf_hist['app_count'] = df_hist.groupby('patient_id').no_show.transform('count')\ndf_hist = df_hist[df_hist.app_count >= 10].copy()\n\n# switching threshold for marking an appointment as being tightly scheduled\n# different thresholds affect prediction results\ndf_hist['same_day_count'] = df_hist[(df_hist.patient_id.duplicated(keep=False)) & (df_hist.schd_days_same ==1)]\\\n.sort_values(['patient_id', 'scheduling_day']).groupby(['patient_id', 'sch_day', 'sch_hour']).scheduling_day.transform('count')\n\ndf_hist['tight_schedule'] = np.where((df_hist.schd_seconds_diff > 0) & (df_hist.same_day_count > 1) & \n                                     (df_hist.schd_seconds_diff <= 411), 1,0)\n\n# separate categorical and numeric columns\ncat_cols = df_hist.select_dtypes('object').columns\nnum_cols = df_hist.columns[~df_hist.columns.isin(cat_cols)]\n\n# Train and test sets split\n\n# using last appointmentfor each patient as an input to test set\ndf_hist['last_app'] = df_hist.groupby('patient_id').appointment_id.transform('last')\n\ntest_set = df_hist[df_hist.appointment_id ==  df_hist.last_app].copy()\n\ntrain_set = df_hist[~df_hist.index.isin(test_set.index)].copy()\n\n# Mapping features to risk criteria\nfeat = ['patient_id_', 'age', 'neighbourhood', 'nbr_sch_time']\n\nrisk_lvl = ['rare', 'unlikely', 'possible', 'likely', 'certain', 'extreme']\n\nfor f in feat:\n\n    # grouping levels into fewer risk criteria\n    train_set[f'{f}_no_show_prlvl'] = train_set.groupby(f'{f}').no_show.transform('mean').round(3)\n    train_set[f'{f}_risk_lvl'] = pd.cut(train_set[f'{f}_no_show_prlvl'], 6, labels = risk_lvl).astype('str')\n    \n    # statistics per risk criteria\n    unique_labels = train_set[f'{f}_risk_lvl'].unique()\n    \n    for l in unique_labels:\n        print(f'feature \\'{f}\\': ', f'Average \\'no show\\' of \\'{l}\\' risk rating :', round(train_set[train_set[f'{f}_risk_lvl'] == l].no_show.mean(),2),\\\n              '---- Total sample size :', train_set[train_set[f'{f}_risk_lvl'] == l].no_show.count())\n        \n    # mapping risk criteria to test set    \n    mapr = dict(train_set[[f'{f}', f'{f}_risk_lvl']].values)\n    test_set[f'{f}_risk_lvl'] = test_set[f'{f}'].map(mapr).fillna('rare')\n\n    # Label Encode categories\n    encoder = LabelEncoder()\n    encoder.fit(train_set[f'{f}_risk_lvl'])\n    train_set[f'{f}_risk_lvl_'] = encoder.transform(train_set[f'{f}_risk_lvl'])\n    test_set[f'{f}_risk_lvl_'] = encoder.transform(test_set[f'{f}_risk_lvl'])\n    \n# update categorical and numeric columns\ncat_cols = train_set.select_dtypes('object').columns\nnum_cols = train_set.columns[~train_set.columns.isin(cat_cols)].drop(train_set.filter(regex='count|prlvl|no_show').columns)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-19T19:00:46.327138Z","iopub.execute_input":"2022-05-19T19:00:46.327434Z","iopub.status.idle":"2022-05-19T19:00:46.842604Z","shell.execute_reply.started":"2022-05-19T19:00:46.327404Z","shell.execute_reply":"2022-05-19T19:00:46.841439Z"},"trusted":true},"execution_count":660,"outputs":[]},{"cell_type":"code","source":"# Instentiate train/test featuers/labels\n\nfrom random import shuffle\n\nfeats = ['sch_month', 'elapsed_days_sch', 'age_risk_lvl_', 'patient_id', 'M', 'age', 'patient_id__risk_lvl_', 'sms_received', \n         'same_day_app', 'tight_schedule']\n\nX_train, y_train = train_set[feats], train_set.no_show\nX_test, y_test = test_set[feats], test_set.no_show\n\n# Target Encode\nX_train_mod = X_train.copy()\nX_test_mod = X_test.copy()\n\nkey = ['patient_id']\n\ntarget_encode_feat_mapping(key, X_train_mod, X_test_mod, y_train, log_trans = True)\n\nX_train_mod = X_train_mod.drop(key, axis=1).copy()\nX_test_mod = X_test_mod.drop(key, axis=1).copy()\n\n# train and evaluate\nrf_results_patient = train_eval(RandomForestClassifier(class_weight = 'balanced', random_state = 42), X_train_mod, y_train, \n                                                                                                      X_test_mod, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T19:13:09.807515Z","iopub.execute_input":"2022-05-19T19:13:09.808015Z","iopub.status.idle":"2022-05-19T19:13:10.405402Z","shell.execute_reply.started":"2022-05-19T19:13:09.807975Z","shell.execute_reply":"2022-05-19T19:13:10.404265Z"},"trusted":true},"execution_count":663,"outputs":[]},{"cell_type":"markdown","source":"We've created risk profiles for each patient and some of the features that contribute to no show, each portrays an underlying pattern that is captured based on historical no show behavior. We've also used different feature to train our model aiming to capture inconsistent behavior that we've noted earlier among patients.\n\nWe've selected patients having 10 appointments or more and split the train/test data using only last appoint of each patient as our validation set, thus using patient ID as a feature is justifiable as we are predicting appointments of same patients used to train the model.\n\nYou might be wondering why the features are in this exact order. I noticed really interesting phenomena that is the order of features does affect prediction results, so whenever I introduce a new feature I kept shuffling the feature list to obtain best possible combination.\n\nUsing cross validation for any of these approaches doesn't make sense in light of the available data we have.","metadata":{}},{"cell_type":"markdown","source":"<li><a href=\"#toc\">Table of contents</a></li>","metadata":{}},{"cell_type":"markdown","source":"<a id='references'></a>","metadata":{}},{"cell_type":"markdown","source":"## Summary of updates and additional references\n\nWe've split the data into two datasets one for patients having historical(several) records and another for patients having single appointment, then we started analysing historical records of patients searching for 'no show' patterns and/or justifications in case of absence of common patterns.\n\nNext, we proceeded with engineering numeric features based on our earlier analysis, mainly focusing on time details (hour, days, month, etc...) and time difference between appointments for each patient. We also created additional features to identify the nature of appointment (follow-up/substitute) and aggregated historical 'no show' behavior per patient, time slot and time slot/neighborhood combined.\n\nThe classifier did not find much value when trained on the above features as the initial (baseline) classification results were not satisfactory, however, based on the output of the classifier's feature importance and in-depth analysis of patient behavior we noted that time features does impact 'no show' behavior; examples of these features are the ones that capture differences between scheduling and appointment dates as well as differences between scheduling timing. Exact reason for why specific scheduling slots experienced higher 'no show' rates than others is still vague as there are some missing critical details for each appointment combined with the imbalanced nature of the data and lack of representative +ve class samples.\n\nThis led us to think about the 'no show' problem as a time series classification problem, however, this will impose some complications when it comes to segmenting the data and labeling each series for classification due to the reasons mentioned earlier. Alternatively, we've thought of creating categorical features that captures this 'no show' pattern, so we've dissected time stamps into different categorical feature each capturing different aspect of scheduling and appointment time and we also combined the feature 'neighborhood' with others such as 'scheduling hour', 'sms received' and 'age' to capture any significant contribution of these features combined to the 'no show' behavior. This approach follows our believe that there is an underlying reason at these specific dates that contribute to the 'no show' behavior and due to the absence of sufficient information we can’t conclude that specific reason but tried to capture it in the features we've created.\n\nWe've tried to present as much topics as possible that may be encountered going through a binary classification problem but with a twist!\n\n### references:\n\nConcept simplification:\n\n- https://statquest.org/\n\n- https://machinelearningmastery.com/\n\nPublications, researches, thesis:\n\n- [Adaptive Machine Learning for Credit Card Fraud Detection](http://www.ulb.ac.be/di/map/adalpozz/pdf/Dalpozzolo2015PhD.pdf)\n\n- [A Comparison of Data Sampling Techniques for Credit Card Fraud Detection](https://thesai.org/Downloads/Volume11No6/Paper_60-A_Comparison_of_Data_Sampling_Techniques.pdf)\n\n- [Employing transaction aggregation strategy to detect credit card fraud](http://euro.ecom.cmu.edu/resources/elibrary/epay/1-s2.0-S0957417412007166-main.pdf)\n\n- [Overlap versus Imbalance](https://www.researchgate.net/publication/221442044_Overlap_versus_Imbalance)\n\n- [Handling Class Overlap and Imbalance to Detect Prompt Situations in Smart Homes](https://eecs.wsu.edu/~cook/pubs/icdm13.2.pdf)\n\n- [Classification with Class Overlapping: A Systematic Study](https://www.atlantis-press.com/article/2053.pdf)\n\n- [Time Series Classification and its Applications](https://www.researchgate.net/publication/326248402_Time_Series_Classification_and_its_Applications)\n\n- [Segmentation and Classification of Time-Series: Real Case Studies](https://www.researchgate.net/publication/221253019_Segmentation_and_Classification_of_Time-Series_Real_Case_Studies)\n\n- [Calibrating Probability with Undersampling for Unbalanced Classification](https://www3.nd.edu/~dial/publications/dalpozzolo2015calibrating.pdf)\n\nBooks:\n\n- [Machine Learning in Python: Essential Techniques for Predictive Analysis](https://www.wiley.com/en-ie/Machine+Learning+in+Python%3A+Essential+Techniques+for+Predictive+Analysis-p-9781119183600)\n\n- [Imbalanced Learning: Foundations, Algorithms, and Applications](https://www.wiley.com/en-us/Imbalanced+Learning%3A+Foundations%2C+Algorithms%2C+and+Applications-p-9781118074626)\n\n- [Approaching (Almost) Any Machine Learning Problem](https://github.com/abhishekkrthakur/approachingalmost)\n\nwalkthroughs:\n- [Undersampling](https://machinelearningmastery.com/undersampling-algorithms-for-imbalanced-classification/)\n- [Decision bounday visualization](https://www.kaggle.com/arthurtok/decision-boundaries-visualised-via-python-plotly/notebook)\n- [Target Encoding](https://maxhalford.github.io/blog/target-encoding/)\n- [Cuddling the Cats!](https://github.com/catboost/catboost/blob/master/catboost/tutorials/categorical_features/categorical_features_parameters.ipynb)","metadata":{}},{"cell_type":"markdown","source":"<li><a href=\"#toc\">Table of contents</a></li>","metadata":{}}]}